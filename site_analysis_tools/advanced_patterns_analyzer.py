#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ù…Ø­Ù„Ù„ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…ØªÙ‚Ø¯Ù… - ØªØ­Ù„ÙŠÙ„ Ø£Ø¹Ù…Ù‚ Ù„Ù„Ø£Ù†Ù…Ø§Ø· ÙˆØ§Ù„Ø³Ù„ÙˆÙƒÙŠØ§Øª ÙÙŠ Ù…ÙˆÙ‚Ø¹ AKWAM
"""

import re
import json
import urllib.parse
from collections import defaultdict, Counter
from datetime import datetime
import math

class AdvancedPatternsAnalyzer:
    def __init__(self, filename='site_links.txt'):
        self.filename = filename
        self.all_links = []
        self.insights = {}
        
    def load_links(self):
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·"""
        with open(self.filename, 'r', encoding='utf-8') as f:
            content = f.read()
        
        lines = content.strip().split('\n')
        for line in lines:
            line = line.strip()
            if line.startswith('https://ak.sv/'):
                self.all_links.append(line)
                
    def analyze_url_patterns(self):
        """ØªØ­Ù„ÙŠÙ„ Ø£Ù†Ù…Ø§Ø· URLs Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©"""
        url_analysis = {
            'depth_analysis': defaultdict(int),
            'parameter_patterns': defaultdict(int),
            'special_characters': defaultdict(int),
            'encoding_patterns': defaultdict(int)
        }
        
        for link in self.all_links:
            # ØªØ­Ù„ÙŠÙ„ Ø¹Ù…Ù‚ URL
            path = link.replace('https://ak.sv/', '')
            depth = path.count('/')
            url_analysis['depth_analysis'][depth] += 1
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ±Ù…ÙŠØ²
            if '%' in link:
                url_analysis['encoding_patterns']['encoded'] += 1
            else:
                url_analysis['encoding_patterns']['plain'] += 1
                
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø­Ø±Ù Ø§Ù„Ø®Ø§ØµØ©
            if re.search(r'[\u0600-\u06FF]', link):
                url_analysis['special_characters']['arabic'] += 1
            if re.search(r'[A-Za-z]', link):
                url_analysis['special_characters']['english'] += 1
            if re.search(r'\d', link):
                url_analysis['special_characters']['numbers'] += 1
            if re.search(r'[-_]', link):
                url_analysis['special_characters']['separators'] += 1
                
        self.insights['url_patterns'] = url_analysis
        
    def analyze_content_distribution(self):
        """ØªØ­Ù„ÙŠÙ„ ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¬ØºØ±Ø§ÙÙŠ ÙˆØ§Ù„Ù„ØºÙˆÙŠ"""
        content_analysis = {
            'arabic_content': defaultdict(int),
            'international_content': defaultdict(int),
            'mixed_content': defaultdict(int),
            'content_origins': defaultdict(int)
        }
        
        # Ù‚ÙˆØ§Ø¦Ù… Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠØ©
        arabic_keywords = ['Ø¹Ø±Ø¨ÙŠ', 'Ù…ØµØ±ÙŠ', 'Ø³ÙˆØ±ÙŠ', 'Ù„Ø¨Ù†Ø§Ù†ÙŠ', 'Ø®Ù„ÙŠØ¬ÙŠ', 'Ù…ØºØ±Ø¨ÙŠ', 'Ø¬Ø²Ø§Ø¦Ø±ÙŠ']
        turkish_keywords = ['turkish', 'turk', 'istanbul', 'ankara']
        korean_keywords = ['korean', 'seoul', 'kpop', 'kdrama']
        indian_keywords = ['bollywood', 'hindi', 'tamil', 'telugu']
        
        for link in self.all_links:
            link_lower = link.lower()
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù†Ø´Ø£
            if any(keyword in link_lower for keyword in arabic_keywords):
                content_analysis['content_origins']['Ø¹Ø±Ø¨ÙŠ'] += 1
            elif any(keyword in link_lower for keyword in turkish_keywords):
                content_analysis['content_origins']['ØªØ±ÙƒÙŠ'] += 1
            elif any(keyword in link_lower for keyword in korean_keywords):
                content_analysis['content_origins']['ÙƒÙˆØ±ÙŠ'] += 1
            elif any(keyword in link_lower for keyword in indian_keywords):
                content_analysis['content_origins']['Ù‡Ù†Ø¯ÙŠ'] += 1
            else:
                content_analysis['content_origins']['Ø¯ÙˆÙ„ÙŠ'] += 1
                
        self.insights['content_distribution'] = content_analysis
        
    def analyze_popularity_indicators(self):
        """ØªØ­Ù„ÙŠÙ„ Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø´Ø¹Ø¨ÙŠØ© ÙˆØ§Ù„Ø§Ù‡ØªÙ…Ø§Ù…"""
        popularity_analysis = {
            'high_id_content': [],
            'series_with_many_seasons': defaultdict(int),
            'actors_frequency': defaultdict(int),
            'content_clusters': defaultdict(list)
        }
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø¹Ø§Ù„ÙŠ ID (Ù‚Ø¯ ÙŠØ¯Ù„ Ø¹Ù„Ù‰ Ø´Ø¹Ø¨ÙŠØ©)
        for link in self.all_links:
            if '/movie/' in link or '/series/' in link:
                match = re.search(r'/(?:movie|series)/(\d+)/', link)
                if match:
                    content_id = int(match.group(1))
                    if content_id > 8000:  # Ù…Ø­ØªÙˆÙ‰ Ø­Ø¯ÙŠØ« Ø£Ùˆ Ø´Ø¹Ø¨ÙŠ
                        title = link.split('/')[-1]
                        title = urllib.parse.unquote(title)
                        popularity_analysis['high_id_content'].append({
                            'id': content_id,
                            'title': title,
                            'type': 'ÙÙŠÙ„Ù…' if '/movie/' in link else 'Ù…Ø³Ù„Ø³Ù„'
                        })
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ÙˆØ§Ø³Ù… Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©
        series_seasons = defaultdict(set)
        for link in self.all_links:
            if '/series/' in link and 'Ø§Ù„Ù…ÙˆØ³Ù…' in link:
                series_match = re.search(r'/series/\d+/([^/]+)', link)
                season_match = re.search(r'Ø§Ù„Ù…ÙˆØ³Ù…-(\d+)', link)
                if series_match and season_match:
                    series_name = series_match.group(1).split('-Ø§Ù„Ù…ÙˆØ³Ù…')[0]
                    season_num = int(season_match.group(1))
                    series_seasons[series_name].add(season_num)
                    
        for series, seasons in series_seasons.items():
            if len(seasons) > 3:  # Ù…Ø³Ù„Ø³Ù„ Ù„Ù‡ Ø£ÙƒØ«Ø± Ù…Ù† 3 Ù…ÙˆØ§Ø³Ù…
                popularity_analysis['series_with_many_seasons'][series] = len(seasons)
        
        self.insights['popularity_indicators'] = popularity_analysis
        
    def analyze_technical_patterns(self):
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„ØªÙ‚Ù†ÙŠØ©"""
        technical_analysis = {
            'id_gaps': [],
            'content_clustering': defaultdict(list),
            'naming_conventions': defaultdict(int),
            'url_efficiency': {}
        }
        
        # ØªØ­Ù„ÙŠÙ„ ÙØ¬ÙˆØ§Øª IDs
        movie_ids = []
        series_ids = []
        
        for link in self.all_links:
            if '/movie/' in link:
                match = re.search(r'/movie/(\d+)/', link)
                if match:
                    movie_ids.append(int(match.group(1)))
            elif '/series/' in link:
                match = re.search(r'/series/(\d+)/', link)
                if match:
                    series_ids.append(int(match.group(1)))
        
        # ØªØ­Ù„ÙŠÙ„ ÙØ¬ÙˆØ§Øª IDs Ù„Ù„Ø£ÙÙ„Ø§Ù…
        movie_ids.sort()
        gaps = []
        for i in range(len(movie_ids) - 1):
            gap = movie_ids[i + 1] - movie_ids[i]
            if gap > 100:  # ÙØ¬ÙˆØ© ÙƒØ¨ÙŠØ±Ø©
                gaps.append(gap)
        
        technical_analysis['id_gaps'] = {
            'movie_gaps': gaps[:10],  # Ø£ÙƒØ¨Ø± 10 ÙØ¬ÙˆØ§Øª
            'largest_gap': max(gaps) if gaps else 0,
            'average_gap': sum(gaps) / len(gaps) if gaps else 0
        }
        
        # ØªØ­Ù„ÙŠÙ„ Ø§ØµØ·Ù„Ø§Ø­Ø§Øª Ø§Ù„ØªØ³Ù…ÙŠØ©
        for link in self.all_links:
            if '-' in link:
                technical_analysis['naming_conventions']['dash_separated'] += 1
            if '_' in link:
                technical_analysis['naming_conventions']['underscore_separated'] += 1
            if re.search(r'\d+', link):
                technical_analysis['naming_conventions']['contains_numbers'] += 1
                
        self.insights['technical_patterns'] = technical_analysis
        
    def generate_advanced_insights_report(self):
        """Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø§Ø³ØªØ¨ØµØ§Ø±Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©"""
        report = []
        report.append("ğŸ”¬ ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ù„Ù„Ø£Ù†Ù…Ø§Ø· ÙˆØ§Ù„Ø§Ø³ØªØ¨ØµØ§Ø±Ø§Øª")
        report.append("=" * 70)
        report.append(f"ğŸ“… ØªØ§Ø±ÙŠØ® Ø§Ù„ØªØ­Ù„ÙŠÙ„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"ğŸ” Ø¹Ø¯Ø¯ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…Ø­Ù„Ù„Ø©: {len(self.all_links):,}")
        report.append("")
        
        # ØªØ­Ù„ÙŠÙ„ Ø£Ù†Ù…Ø§Ø· URLs
        if 'url_patterns' in self.insights:
            url_patterns = self.insights['url_patterns']
            report.append("ğŸŒ ØªØ­Ù„ÙŠÙ„ Ø£Ù†Ù…Ø§Ø· URLs Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©:")
            report.append("-" * 40)
            
            report.append("â€¢ ØªÙˆØ²ÙŠØ¹ Ø¹Ù…Ù‚ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª:")
            for depth, count in sorted(url_patterns['depth_analysis'].items()):
                percentage = (count / len(self.all_links)) * 100
                report.append(f"  - Ø¹Ù…Ù‚ {depth}: {count:,} ({percentage:.1f}%)")
            
            report.append("â€¢ Ø£Ù†Ù…Ø§Ø· Ø§Ù„ØªØ±Ù…ÙŠØ²:")
            for pattern, count in url_patterns['encoding_patterns'].items():
                percentage = (count / len(self.all_links)) * 100
                report.append(f"  - {pattern}: {count:,} ({percentage:.1f}%)")
            
            report.append("â€¢ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø­Ø±Ù Ø§Ù„Ø®Ø§ØµØ©:")
            for char_type, count in url_patterns['special_characters'].items():
                percentage = (count / len(self.all_links)) * 100
                report.append(f"  - {char_type}: {count:,} ({percentage:.1f}%)")
            report.append("")
        
        # ØªØ­Ù„ÙŠÙ„ ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
        if 'content_distribution' in self.insights:
            content_dist = self.insights['content_distribution']
            report.append("ğŸŒ ØªØ­Ù„ÙŠÙ„ ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¬ØºØ±Ø§ÙÙŠ:")
            report.append("-" * 40)
            
            total_origins = sum(content_dist['content_origins'].values())
            if total_origins > 0:
                report.append("â€¢ Ø£ØµÙˆÙ„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰:")
                for origin, count in sorted(content_dist['content_origins'].items(), 
                                          key=lambda x: x[1], reverse=True):
                    percentage = (count / total_origins) * 100
                    report.append(f"  - {origin}: {count:,} ({percentage:.1f}%)")
            report.append("")
        
        # Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø´Ø¹Ø¨ÙŠØ©
        if 'popularity_indicators' in self.insights:
            popularity = self.insights['popularity_indicators']
            report.append("ğŸ“ˆ Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø´Ø¹Ø¨ÙŠØ© ÙˆØ§Ù„Ø§Ù‡ØªÙ…Ø§Ù…:")
            report.append("-" * 40)
            
            if popularity['high_id_content']:
                report.append(f"â€¢ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø¹Ø§Ù„ÙŠ ID (Ù…Ø¤Ø´Ø± Ø­Ø¯Ø§Ø«Ø©): {len(popularity['high_id_content']):,}")
                top_content = sorted(popularity['high_id_content'], 
                                   key=lambda x: x['id'], reverse=True)[:10]
                for content in top_content:
                    report.append(f"  - {content['type']} ID {content['id']}: {content['title']}")
            
            if popularity['series_with_many_seasons']:
                report.append("â€¢ Ø§Ù„Ù…Ø³Ù„Ø³Ù„Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ù…ÙˆØ§Ø³Ù…:")
                for series, seasons in sorted(popularity['series_with_many_seasons'].items(),
                                            key=lambda x: x[1], reverse=True)[:10]:
                    report.append(f"  - {series}: {seasons} Ù…ÙˆØ§Ø³Ù…")
            report.append("")
        
        # Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„ØªÙ‚Ù†ÙŠØ©
        if 'technical_patterns' in self.insights:
            technical = self.insights['technical_patterns']
            report.append("âš™ï¸ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„ØªÙ‚Ù†ÙŠØ©:")
            report.append("-" * 40)
            
            if 'id_gaps' in technical:
                gaps_info = technical['id_gaps']
                report.append(f"â€¢ ØªØ­Ù„ÙŠÙ„ ÙØ¬ÙˆØ§Øª IDs:")
                report.append(f"  - Ø£ÙƒØ¨Ø± ÙØ¬ÙˆØ©: {gaps_info['largest_gap']:,}")
                report.append(f"  - Ù…ØªÙˆØ³Ø· Ø§Ù„ÙØ¬ÙˆØ§Øª: {gaps_info['average_gap']:.0f}")
                if gaps_info['movie_gaps']:
                    report.append(f"  - Ø£ÙƒØ¨Ø± 5 ÙØ¬ÙˆØ§Øª: {gaps_info['movie_gaps'][:5]}")
            
            if technical['naming_conventions']:
                report.append("â€¢ Ø§ØµØ·Ù„Ø§Ø­Ø§Øª Ø§Ù„ØªØ³Ù…ÙŠØ©:")
                total_naming = sum(technical['naming_conventions'].values())
                for convention, count in technical['naming_conventions'].items():
                    percentage = (count / total_naming) * 100
                    report.append(f"  - {convention}: {count:,} ({percentage:.1f}%)")
            report.append("")
        
        # Ø§Ù„Ø®Ù„Ø§ØµØ© Ø§Ù„ØªÙ‚Ù†ÙŠØ©
        report.append("ğŸ¯ Ø§Ù„Ø®Ù„Ø§ØµØ© Ø§Ù„ØªÙ‚Ù†ÙŠØ© ÙˆØ§Ù„ØªÙˆØµÙŠØ§Øª:")
        report.append("-" * 40)
        report.append("â€¢ Ø¨Ù†ÙŠØ© URL Ù…Ù†Ø¸Ù…Ø© ÙˆÙ…Ù†Ø·Ù‚ÙŠØ© Ù…Ø¹ Ù†Ø¸Ø§Ù… ØªØ±Ù‚ÙŠÙ… ÙØ¹Ø§Ù„")
        report.append("â€¢ ØªÙ†ÙˆØ¹ ÙƒØ¨ÙŠØ± ÙÙŠ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ ÙŠØºØ·ÙŠ Ø£Ø³ÙˆØ§Ù‚ Ù…ØªØ¹Ø¯Ø¯Ø©")
        report.append("â€¢ Ù†Ø¸Ø§Ù… Ø¥Ø¯Ø§Ø±Ø© Ù…Ø­ØªÙˆÙ‰ Ù…ØªØ·ÙˆØ± Ù…Ø¹ ÙÙ‡Ø±Ø³Ø© Ø°ÙƒÙŠØ©")
        report.append("â€¢ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‡Ø°Ø§ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ù„ØªØ·ÙˆÙŠØ±:")
        report.append("  - Ù†Ø¸Ø§Ù… ØªÙˆØµÙŠØ§Øª Ø°ÙƒÙŠ")
        report.append("  - Ù…Ø­Ø±Ùƒ Ø¨Ø­Ø« Ù…ØªØ·ÙˆØ±")
        report.append("  - Ù†Ø¸Ø§Ù… ØªØµÙ†ÙŠÙ ØªÙ„Ù‚Ø§Ø¦ÙŠ")
        report.append("  - Ø£Ø¯ÙˆØ§Øª ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰")
        report.append("")
        
        return "\n".join(report)
    
    def run_analysis(self):
        """ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙƒØ§Ù…Ù„"""
        print("ğŸ”¬ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ù„Ù„Ø£Ù†Ù…Ø§Ø·...")
        
        self.load_links()
        self.analyze_url_patterns()
        self.analyze_content_distribution()
        self.analyze_popularity_indicators()
        self.analyze_technical_patterns()
        
        report = self.generate_advanced_insights_report()
        
        # Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
        output_file = f"ØªØ­Ù„ÙŠÙ„_Ø§Ù„Ø£Ù†Ù…Ø§Ø·_Ø§Ù„Ù…ØªÙ‚Ø¯Ù…_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(report)
            
        print(f"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…ØªÙ‚Ø¯Ù…: {output_file}")
        return report

if __name__ == "__main__":
    analyzer = AdvancedPatternsAnalyzer()
    report = analyzer.run_analysis()
    print("\n" + "="*70)
    print(report)