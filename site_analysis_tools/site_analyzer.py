#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ù…Ø­Ù„Ù„ Ù…ÙˆÙ‚Ø¹ ak.sv Ø§Ù„Ù…ØªØ·ÙˆØ±
ÙŠÙ‚ÙˆÙ… Ø¨ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ù„Ø¢Ù„ÙŠØ© Ø¹Ù…Ù„ Ø§Ù„Ù…ÙˆÙ‚Ø¹ ÙˆÙ…Ù…ÙŠØ²Ø§ØªÙ‡
"""

import requests
import time
import json
import re
from urllib.parse import urljoin, urlparse
from datetime import datetime
from bs4 import BeautifulSoup

class AKSVAnalyzer:
    def __init__(self):
        self.session = requests.Session()
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ User-Agent Ù…ØªÙ‚Ø¯Ù…
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ar,en-US;q=0.7,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        
        self.base_url = 'https://ak.sv'
        self.analysis_results = {
            'timestamp': datetime.now().isoformat(),
            'site_structure': {},
            'features': [],
            'content_analysis': {},
            'advertising_system': {},
            'user_interface': {},
            'technical_details': {},
            'security_analysis': {},
            'performance_metrics': {}
        }
    
    def safe_request(self, url, timeout=15):
        """Ø·Ù„Ø¨ Ø¢Ù…Ù† Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø®Ø·Ø§Ø¡"""
        try:
            print(f"ğŸ” ØªØ­Ù„ÙŠÙ„: {url}")
            response = self.session.get(url, timeout=timeout)
            response.raise_for_status()
            time.sleep(2)  # ÙØªØ±Ø© Ø§Ù†ØªØ¸Ø§Ø± Ù…Ù‡Ø°Ø¨Ø©
            return response
        except requests.exceptions.Timeout:
            print(f"â° Ø§Ù†ØªÙ‡Øª Ù…Ù‡Ù„Ø© Ø§Ù„Ø·Ù„Ø¨: {url}")
            return None
        except requests.exceptions.ConnectionError:
            print(f"ğŸš« Ù…Ø´ÙƒÙ„Ø© ÙÙŠ Ø§Ù„Ø§ØªØµØ§Ù„: {url}")
            return None
        except requests.exceptions.HTTPError as e:
            print(f"âŒ Ø®Ø·Ø£ HTTP {e.response.status_code}: {url}")
            return None
        except Exception as e:
            print(f"âš ï¸ Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹: {str(e)}")
            return None
    
    def extract_content_with_bs4(self, html_content):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… BeautifulSoup"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡ ØºÙŠØ± Ø§Ù„Ù…Ù‡Ù…Ø©
            for tag in soup(['script', 'style', 'nav', 'footer', 'header']):
                tag.decompose()
            return soup.get_text(strip=True)
        except Exception as e:
            print(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰: {e}")
            return ""
    
    def analyze_homepage(self):
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
        print("\nğŸ  ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©...")
        
        response = self.safe_request(self.base_url)
        if not response:
            return
        
        html = response.text
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù‡ÙŠÙƒÙ„ Ø§Ù„Ø¹Ø§Ù…
        self.analysis_results['site_structure']['homepage'] = {
            'title': self.extract_title(html),
            'meta_description': self.extract_meta_description(html),
            'language': self.detect_language(html),
            'navigation_menu': self.extract_navigation(html),
            'main_sections': self.extract_main_sections(html),
            'footer_info': self.extract_footer_info(html)
        }
        
        # ØªØ­Ù„ÙŠÙ„ Ù†Ø¸Ø§Ù… Ø§Ù„Ø¥Ø¹Ù„Ø§Ù†Ø§Øª
        ad_analysis = self.analyze_advertising_system(html)
        self.analysis_results['advertising_system']['homepage'] = ad_analysis
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
        content = self.extract_content_with_bs4(html)
        self.analysis_results['content_analysis']['homepage'] = {
            'content_length': len(content),
            'main_content': content[:1000] if content else "",
            'featured_content': self.extract_featured_content(html)
        }
    
    def analyze_movie_page(self, movie_url):
        """ØªØ­Ù„ÙŠÙ„ ØµÙØ­Ø© ÙÙŠÙ„Ù…"""
        print(f"\nğŸ¬ ØªØ­Ù„ÙŠÙ„ ØµÙØ­Ø© ÙÙŠÙ„Ù…: {movie_url}")
        
        response = self.safe_request(movie_url)
        if not response:
            return
        
        html = response.text
        
        movie_analysis = {
            'title': self.extract_title(html),
            'movie_info': self.extract_movie_info(html),
            'video_player': self.analyze_video_player(html),
            'download_links': self.extract_download_links(html),
            'streaming_options': self.extract_streaming_options(html),
            'related_movies': self.extract_related_content(html),
            'user_ratings': self.extract_ratings(html),
            'comments_system': self.analyze_comment_system(html)
        }
        
        self.analysis_results['content_analysis']['movie_page'] = movie_analysis
    
    def analyze_series_page(self, series_url):
        """ØªØ­Ù„ÙŠÙ„ ØµÙØ­Ø© Ù…Ø³Ù„Ø³Ù„"""
        print(f"\nğŸ“º ØªØ­Ù„ÙŠÙ„ ØµÙØ­Ø© Ù…Ø³Ù„Ø³Ù„: {series_url}")
        
        response = self.safe_request(series_url)
        if not response:
            return
        
        html = response.text
        
        series_analysis = {
            'title': self.extract_title(html),
            'series_info': self.extract_series_info(html),
            'seasons_episodes': self.extract_seasons_episodes(html),
            'episode_player': self.analyze_video_player(html),
            'episode_navigation': self.analyze_episode_navigation(html),
            'series_progress': self.analyze_series_progress(html)
        }
        
        self.analysis_results['content_analysis']['series_page'] = series_analysis
    
    def analyze_advertising_system(self, html):
        """ØªØ­Ù„ÙŠÙ„ Ù†Ø¸Ø§Ù… Ø§Ù„Ø¥Ø¹Ù„Ø§Ù†Ø§Øª"""
        ad_analysis = {
            'ad_types': [],
            'skip_mechanisms': [],
            'ad_placement': [],
            'external_ad_networks': []
        }
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø§Ù„Ù…Ø®ØªÙ„ÙØ©
        if 'google' in html.lower() and 'ads' in html.lower():
            ad_analysis['external_ad_networks'].append('Google Ads')
        
        if 'adsystem' in html.lower():
            ad_analysis['external_ad_networks'].append('AdSystem')
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø¢Ù„ÙŠØ§Øª ØªØ®Ø·ÙŠ Ø§Ù„Ø¥Ø¹Ù„Ø§Ù†Ø§Øª
        skip_patterns = [
            r'skip.*ad',
            r'ØªØ®Ø·ÙŠ.*Ø§Ù„Ø¥Ø¹Ù„Ø§Ù†',
            r'close.*ad',
            r'Ø¥ØºÙ„Ø§Ù‚.*Ø§Ù„Ø¥Ø¹Ù„Ø§Ù†',
            r'continue.*after.*ad',
            r'Ù…ØªØ§Ø¨Ø¹Ø©.*Ø¨Ø¹Ø¯.*Ø§Ù„Ø¥Ø¹Ù„Ø§Ù†'
        ]
        
        for pattern in skip_patterns:
            if re.search(pattern, html, re.IGNORECASE):
                ad_analysis['skip_mechanisms'].append(pattern)
        
        return ad_analysis
    
    def extract_title(self, html):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¹Ù†ÙˆØ§Ù† Ø§Ù„ØµÙØ­Ø©"""
        title_match = re.search(r'<title[^>]*>(.*?)</title>', html, re.IGNORECASE | re.DOTALL)
        return title_match.group(1).strip() if title_match else ""
    
    def extract_meta_description(self, html):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆØµÙ Ø§Ù„ØµÙØ­Ø©"""
        desc_match = re.search(r'<meta[^>]*name=["\']description["\'][^>]*content=["\']([^"\']*)["\']', html, re.IGNORECASE)
        return desc_match.group(1) if desc_match else ""
    
    def detect_language(self, html):
        """ÙƒØ´Ù Ù„ØºØ© Ø§Ù„Ù…ÙˆÙ‚Ø¹"""
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù„ØºØ© ÙÙŠ HTML lang attribute  
        lang_match = re.search(r'<html[^>]*lang=["\']([^"\']*)["\']', html, re.IGNORECASE)
        if lang_match:
            return lang_match.group(1)
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù„Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
        arabic_chars = len(re.findall(r'[\u0600-\u06FF]', html))
        english_chars = len(re.findall(r'[a-zA-Z]', html))
        
        if arabic_chars > english_chars:
            return 'ar'
        elif english_chars > arabic_chars:
            return 'en'
        else:
            return 'mixed'
    
    def extract_navigation(self, html):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„ØªÙ†Ù‚Ù„"""
        nav_links = []
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ØªÙ†Ù‚Ù„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
        nav_patterns = [
            r'<nav[^>]*>(.*?)</nav>',
            r'<ul[^>]*class=["\'].*nav.*["\'][^>]*>(.*?)</ul>',
            r'<div[^>]*class=["\'].*menu.*["\'][^>]*>(.*?)</div>'
        ]
        
        for pattern in nav_patterns:
            nav_match = re.search(pattern, html, re.IGNORECASE | re.DOTALL)
            if nav_match:
                links = re.findall(r'<a[^>]*href=["\']([^"\']*)["\'][^>]*>(.*?)</a>', nav_match.group(1), re.IGNORECASE | re.DOTALL)
                nav_links.extend(links)
        
        return nav_links[:10]  # Ø£ÙˆÙ„ 10 Ø±ÙˆØ§Ø¨Ø·
    
    def extract_featured_content(self, html):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ù…ÙŠØ²"""
        featured = {
            'movies': [],
            'series': [],
            'trending': []
        }
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ù…ÙŠØ²
        movie_links = re.findall(r'href=["\']([^"\']*movie[^"\']*)["\']', html)
        series_links = re.findall(r'href=["\']([^"\']*series[^"\']*)["\']', html)
        
        featured['movies'] = movie_links[:5]  # Ø£ÙˆÙ„ 5 Ø£ÙÙ„Ø§Ù…
        featured['series'] = series_links[:5]  # Ø£ÙˆÙ„ 5 Ù…Ø³Ù„Ø³Ù„Ø§Øª
        
        return featured
    
    def analyze_video_player(self, html):
        """ØªØ­Ù„ÙŠÙ„ Ù…Ø´ØºÙ„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ"""
        player_analysis = {
            'player_type': 'unknown',
            'video_sources': [],
            'streaming_servers': [],
            'download_options': [],
            'subtitle_support': False,
            'quality_options': []
        }
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ù…Ø´ØºÙ„Ø§Øª Ø§Ù„Ù…Ø®ØªÙ„ÙØ©
        if 'jwplayer' in html.lower():
            player_analysis['player_type'] = 'JW Player'
        elif 'videojs' in html.lower():
            player_analysis['player_type'] = 'Video.js'
        elif 'plyr' in html.lower():
            player_analysis['player_type'] = 'Plyr'
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…ØµØ§Ø¯Ø± Ø§Ù„ÙÙŠØ¯ÙŠÙˆ
        video_patterns = [
            r'src=["\']([^"\']*\.mp4[^"\']*)["\']',
            r'src=["\']([^"\']*\.m3u8[^"\']*)["\']',
            r'file:["\']([^"\']*)["\']'
        ]
        
        for pattern in video_patterns:
            matches = re.findall(pattern, html, re.IGNORECASE)
            player_analysis['video_sources'].extend(matches)
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø¯Ø¹Ù… Ø§Ù„ØªØ±Ø¬Ù…Ø©
        if 'subtitle' in html.lower() or 'caption' in html.lower():
            player_analysis['subtitle_support'] = True
        
        return player_analysis
    
    def generate_comprehensive_report(self):
        """Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ø´Ø§Ù…Ù„"""
        report_content = f"""# ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ ÙˆÙ…ØªØ·ÙˆØ± Ù„Ù…ÙˆÙ‚Ø¹ ak.sv

**ØªØ§Ø±ÙŠØ® Ø§Ù„ØªØ­Ù„ÙŠÙ„:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**Ø§Ù„Ù…Ø­Ù„Ù„:** Ù†Ø¸Ø§Ù… ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„Ù…ØªØ·ÙˆØ± v2.0

---

## ğŸŒ Ù†Ø¸Ø±Ø© Ø¹Ø§Ù…Ø© Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙˆÙ‚Ø¹

### Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø£Ø³Ø§Ø³ÙŠØ©
- **Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ:** {self.base_url}
- **Ù†ÙˆØ¹ Ø§Ù„Ù…ÙˆÙ‚Ø¹:** Ù…Ù†ØµØ© Ø¨Ø« ÙˆØªØ­Ù…ÙŠÙ„ Ø£ÙÙ„Ø§Ù… ÙˆÙ…Ø³Ù„Ø³Ù„Ø§Øª
- **Ø§Ù„Ù„ØºØ© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©:** {self.analysis_results.get('site_structure', {}).get('homepage', {}).get('language', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}
- **Ø§Ù„Ø¹Ù†ÙˆØ§Ù†:** {self.analysis_results.get('site_structure', {}).get('homepage', {}).get('title', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}

---

## ğŸ¯ Ø§Ù„Ù…Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù„Ù…ÙˆÙ‚Ø¹

### 1. ğŸ¬ Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰
- **Ø§Ù„Ø£ÙÙ„Ø§Ù…:** Ù…Ø¬Ù…ÙˆØ¹Ø© ÙˆØ§Ø³Ø¹Ø© Ù…Ù† Ø§Ù„Ø£ÙÙ„Ø§Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆØ§Ù„Ø£Ø¬Ù†Ø¨ÙŠØ©
- **Ø§Ù„Ù…Ø³Ù„Ø³Ù„Ø§Øª:** Ù…Ø³Ù„Ø³Ù„Ø§Øª Ù…Ù† Ù…Ø®ØªÙ„Ù Ø§Ù„Ø¨Ù„Ø¯Ø§Ù† ÙˆØ§Ù„Ø£Ù†ÙˆØ§Ø¹
- **Ø§Ù„Ø¹Ø±ÙˆØ¶ Ø§Ù„ØªÙ„ÙØ²ÙŠÙˆÙ†ÙŠØ©:** Ø¨Ø±Ø§Ù…Ø¬ ÙˆÙ…Ø³Ø§Ø¨Ù‚Ø§Øª Ù…ØªÙ†ÙˆØ¹Ø©
- **Ø§Ù„Ù…Ù†ÙˆØ¹Ø§Øª:** Ù…Ø­ØªÙˆÙ‰ Ø¥Ø¶Ø§ÙÙŠ Ù…ØªÙ†ÙˆØ¹

### 2. ğŸ–¥ï¸ ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
{json.dumps(self.analysis_results.get('user_interface', {}), ensure_ascii=False, indent=2)}

### 3. ğŸ“± Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ´ØºÙŠÙ„ ÙˆØ§Ù„Ø¨Ø«
{json.dumps(self.analysis_results.get('content_analysis', {}), ensure_ascii=False, indent=2)}

---

## ğŸ¯ Ù†Ø¸Ø§Ù… Ø§Ù„Ø¥Ø¹Ù„Ø§Ù†Ø§Øª ÙˆØ§Ù„ØªØ´ØºÙŠÙ„

### Ø¢Ù„ÙŠØ© Ø¹Ù…Ù„ Ø§Ù„Ø¥Ø¹Ù„Ø§Ù†Ø§Øª
{json.dumps(self.analysis_results.get('advertising_system', {}), ensure_ascii=False, indent=2)}

### Ø·Ø±ÙŠÙ‚Ø© ØªØ®Ø·ÙŠ Ø§Ù„Ø¥Ø¹Ù„Ø§Ù†Ø§Øª
- **Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©:** ÙŠØ¬Ø¨ ØªØ®Ø·ÙŠ Ø§Ù„Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ù„Ù„ÙˆØµÙˆÙ„ Ù„Ù„Ù…Ø­ØªÙˆÙ‰
- **Ø§Ù„Ø¢Ù„ÙŠØ§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©:** Ø£Ø²Ø±Ø§Ø± ØªØ®Ø·ÙŠØŒ Ø§Ù†ØªØ¸Ø§Ø± Ù…Ø¤Ù‚ØªØŒ Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ù…Ù†Ø¨Ø«Ù‚Ø©

---

## ğŸ”§ Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„ØªÙ‚Ù†ÙŠØ©

### Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù…ÙˆÙ‚Ø¹
{json.dumps(self.analysis_results.get('site_structure', {}), ensure_ascii=False, indent=2)}

### Ø§Ù„Ø£Ø¯Ø§Ø¡ ÙˆØ§Ù„Ø³Ø±Ø¹Ø©
{json.dumps(self.analysis_results.get('performance_metrics', {}), ensure_ascii=False, indent=2)}

---

## ğŸ›¡ï¸ Ø§Ù„Ø£Ù…Ø§Ù† ÙˆØ§Ù„Ø­Ù…Ø§ÙŠØ©
{json.dumps(self.analysis_results.get('security_analysis', {}), ensure_ascii=False, indent=2)}

---

## ğŸ“Š Ø®Ù„Ø§ØµØ© Ø§Ù„ØªØ­Ù„ÙŠÙ„

### Ù†Ù‚Ø§Ø· Ø§Ù„Ù‚ÙˆØ©
1. Ù…ÙƒØªØ¨Ø© Ù…Ø­ØªÙˆÙ‰ ÙˆØ§Ø³Ø¹Ø© ÙˆÙ…ØªÙ†ÙˆØ¹Ø©
2. Ø¯Ø¹Ù… Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
3. ÙˆØ§Ø¬Ù‡Ø© Ù…Ø³ØªØ®Ø¯Ù… Ø³Ù‡Ù„Ø© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
4. Ø®ÙŠØ§Ø±Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø© Ù„Ù„Ø¬ÙˆØ¯Ø© ÙˆØ§Ù„ØªØ­Ù…ÙŠÙ„

### Ø§Ù„ØªØ­Ø¯ÙŠØ§Øª
1. Ù†Ø¸Ø§Ù… Ø¥Ø¹Ù„Ø§Ù†Ø§Øª ÙŠØªØ·Ù„Ø¨ ØªØ®Ø·ÙŠ
2. Ù‚Ø¯ ØªØ­ØªØ§Ø¬ Ù„Ø¨Ø±Ø§Ù…Ø¬ Ø­Ø¬Ø¨ Ø§Ù„Ø¥Ø¹Ù„Ø§Ù†Ø§Øª
3. Ø¨Ø¹Ø¶ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ù…Ø­Ø¯ÙˆØ¯ Ø­Ø³Ø¨ Ø§Ù„Ù…Ù†Ø·Ù‚Ø©

---

*ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù‡Ø°Ø§ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ØªØ·ÙˆØ±Ø©*
"""
        
        # Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
        with open('ØªØ­Ù„ÙŠÙ„_Ù…ÙˆÙ‚Ø¹_akwam_Ø´Ø§Ù…Ù„.md', 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print("âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø´Ø§Ù…Ù„: ØªØ­Ù„ÙŠÙ„_Ù…ÙˆÙ‚Ø¹_akwam_Ø´Ø§Ù…Ù„.md")

def main():
    print("ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„ Ù„Ù…ÙˆÙ‚Ø¹ ak.sv...")
    
    analyzer = AKSVAnalyzer()
    
    try:
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
        analyzer.analyze_homepage()
        
        # ØªØ­Ù„ÙŠÙ„ Ø¹ÙŠÙ†Ø© Ù…Ù† ØµÙØ­Ø§Øª Ø§Ù„Ø£ÙÙ„Ø§Ù…
        sample_movies = [
            "https://ak.sv/movie/1001/hacked",
            "https://ak.sv/movie/1002/miracle-in-cell-no-7"
        ]
        
        for movie_url in sample_movies[:1]:  # ÙÙŠÙ„Ù… ÙˆØ§Ø­Ø¯ Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±
            analyzer.analyze_movie_page(movie_url)
        
        # ØªØ­Ù„ÙŠÙ„ Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„Ù…Ø³Ù„Ø³Ù„Ø§Øª
        sample_series = [
            "https://ak.sv/series/1000/criminal-minds-Ø§Ù„Ù…ÙˆØ³Ù…-Ø§Ù„Ø§ÙˆÙ„-15"
        ]
        
        for series_url in sample_series[:1]:  # Ù…Ø³Ù„Ø³Ù„ ÙˆØ§Ø­Ø¯ Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±
            analyzer.analyze_series_page(series_url)
        
        # Ø¥Ù†ØªØ§Ø¬ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
        analyzer.generate_comprehensive_report()
        
        print("\nâœ… ØªÙ… Ø¥ÙƒÙ…Ø§Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„ Ø¨Ù†Ø¬Ø§Ø­!")
        print("ğŸ“„ ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù…Ù„Ù: ØªØ­Ù„ÙŠÙ„_Ù…ÙˆÙ‚Ø¹_akwam_Ø´Ø§Ù…Ù„.md")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…")
    except Exception as e:
        print(f"\nâŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„: {str(e)}")

if __name__ == "__main__":
    main()