#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ø£Ø¯Ø§Ø© ØªØ­Ù„ÙŠÙ„ ÙˆØ£Ø®Ø° Ù„Ù‚Ø·Ø§Øª Ø´Ø§Ø´Ø© Ø´Ø§Ù…Ù„Ø© Ù„Ù„Ù…ÙˆØ§Ù‚Ø¹
Enhanced Website Screenshot and Analysis Tool
Ù…Ø·ÙˆØ± Ø®ØµÙŠØµØ§Ù‹ Ù„Ø¨ÙŠØ¦Ø© Replit Ù…Ø¹ Ø¯Ø¹Ù… ÙƒØ§Ù…Ù„ Ù„Ù„Ù‚Ø·Ø§Øª Ø§Ù„Ø´Ø§Ø´Ø© PNG
"""

import nest_asyncio
nest_asyncio.apply()

import asyncio
import aiofiles
import json
from datetime import datetime
from pathlib import Path
from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError
from bs4 import BeautifulSoup
import os
from urllib.parse import urljoin, urlparse, unquote
from fake_useragent import UserAgent
import mimetypes
import xml.etree.ElementTree as ET
import requests
from PIL import Image
import time
import logging
import re

# Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ø¸Ø§Ù… ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('screenshot_log.txt', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class EnhancedWebsiteScreenshotTool:
    def __init__(self, base_url="https://ak.sv/", output_dir="site_analysis_complete"):
        """
        Ø£Ø¯Ø§Ø© ØªØ­Ù„ÙŠÙ„ ÙˆØ£Ø®Ø° Ù„Ù‚Ø·Ø§Øª Ø´Ø§Ø´Ø© Ø´Ø§Ù…Ù„Ø©
        """
        self.base_url = base_url
        self.output_root = output_dir
        self.screenshots_root = os.path.join(output_dir, "screenshots")
        self.max_depth = 3
        self.max_pages = 30
        self.screenshot_delay = 2
        self.viewport_width = 1920
        self.viewport_height = 1080
        self.timeout = 30000
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        self.stats = {
            'total_pages': 0,
            'successful_screenshots': 0,
            'failed_screenshots': 0,
            'start_time': None,
            'end_time': None,
            'total_assets': 0,
            'successful_assets': 0
        }
        
        self.ua = UserAgent()
        self.visited = set()
        self.sitemap_urls = []
        self.failed_urls = []
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª
        self._create_directories()
        
    def _create_directories(self):
        """Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©"""
        directories = [
            self.output_root,
            self.screenshots_root,
            os.path.join(self.output_root, "html_pages"),
            os.path.join(self.output_root, "assets"),
            os.path.join(self.output_root, "metadata"),
            os.path.join(self.output_root, "reports")
        ]
        
        for directory in directories:
            os.makedirs(directory, exist_ok=True)
            
    def sanitize_filename(self, text, max_length=100):
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ ÙƒØ§Ø³Ù… Ù…Ù„Ù Ø¢Ù…Ù†"""
        if not text:
            return "unknown"
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ø­Ø±Ù Ø§Ù„Ø®Ø§ØµØ©
        safe_name = re.sub(r'[^\w\-_.]', '_', text)
        safe_name = re.sub(r'_+', '_', safe_name).strip('_')
        
        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø·ÙˆÙ„
        if len(safe_name) > max_length:
            safe_name = safe_name[:max_length]
            
        return safe_name or "unknown"
    
    def get_page_identifier(self, url):
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¹Ø±Ù ÙØ±ÙŠØ¯ Ù„Ù„ØµÙØ­Ø©"""
        try:
            parsed = urlparse(url)
            path = parsed.path.strip("/") or "home"
            query = parsed.query
            
            identifier = self.sanitize_filename(path)
            if query:
                query_safe = self.sanitize_filename(query)
                identifier += f"_q_{query_safe}"
                
            return identifier
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¹Ø±Ù Ø§Ù„ØµÙØ­Ø© {url}: {e}")
            return f"page_{len(self.visited)}"
    
    async def save_html_content(self, content, page_id, url):
        """Ø­ÙØ¸ Ù…Ø­ØªÙˆÙ‰ HTML"""
        try:
            html_dir = os.path.join(self.output_root, "html_pages", page_id)
            os.makedirs(html_dir, exist_ok=True)
            
            html_file = os.path.join(html_dir, "index.html")
            async with aiofiles.open(html_file, 'w', encoding='utf-8') as f:
                await f.write(content)
                
            # Ø­ÙØ¸ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©
            info_file = os.path.join(html_dir, "page_info.json")
            page_info = {
                'url': url,
                'page_id': page_id,
                'timestamp': datetime.now().isoformat(),
                'content_length': len(content)
            }
            
            async with aiofiles.open(info_file, 'w', encoding='utf-8') as f:
                await f.write(json.dumps(page_info, ensure_ascii=False, indent=2))
                
            return html_dir
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ HTML Ù„Ù€ {url}: {e}")
            return None
    
    async def take_enhanced_screenshot(self, page, page_id, url):
        """Ø£Ø®Ø° Ù„Ù‚Ø·Ø© Ø´Ø§Ø´Ø© Ù…Ø­Ø³Ù†Ø© Ø¨ØµÙŠØºØ© PNG"""
        try:
            # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ Ù„Ù‚Ø·Ø§Øª Ø§Ù„Ø´Ø§Ø´Ø© Ù„Ù„ØµÙØ­Ø©
            screenshot_dir = os.path.join(self.screenshots_root, page_id)
            os.makedirs(screenshot_dir, exist_ok=True)
            
            # Ø§Ù†ØªØ¸Ø§Ø± ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø© Ø¨Ø§Ù„ÙƒØ§Ù…Ù„
            await asyncio.sleep(self.screenshot_delay)
            
            # Ù„Ù‚Ø·Ø© Ø´Ø§Ø´Ø© ÙƒØ§Ù…Ù„Ø©
            full_screenshot_path = os.path.join(screenshot_dir, "full_page.png")
            await page.screenshot(
                path=full_screenshot_path, 
                full_page=True,
                type='png'
            )
            
            # Ù„Ù‚Ø·Ø© Ø´Ø§Ø´Ø© Ù„Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø±Ø¦ÙŠ ÙÙ‚Ø·
            viewport_screenshot_path = os.path.join(screenshot_dir, "viewport.png")
            await page.screenshot(
                path=viewport_screenshot_path,
                full_page=False,
                type='png'
            )
            
            # ØªØ­Ø³ÙŠÙ† Ø¬ÙˆØ¯Ø© Ø§Ù„ØµÙˆØ± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… PIL
            await self._optimize_screenshots(screenshot_dir)
            
            # Ø¥Ù†Ø´Ø§Ø¡ thumbnail ØµØºÙŠØ±
            await self._create_thumbnail(full_screenshot_path, screenshot_dir)
            
            self.stats['successful_screenshots'] += 1
            logger.info(f"âœ… ØªÙ… Ø£Ø®Ø° Ù„Ù‚Ø·Ø§Øª Ø´Ø§Ø´Ø© Ù„Ù€ {url}")
            
            return screenshot_dir
            
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ Ø£Ø®Ø° Ù„Ù‚Ø·Ø© Ø´Ø§Ø´Ø© Ù„Ù€ {url}: {e}")
            self.stats['failed_screenshots'] += 1
            self.failed_urls.append({'url': url, 'error': str(e), 'type': 'screenshot'})
            return None
    
    async def _optimize_screenshots(self, screenshot_dir):
        """ØªØ­Ø³ÙŠÙ† Ø¬ÙˆØ¯Ø© Ù„Ù‚Ø·Ø§Øª Ø§Ù„Ø´Ø§Ø´Ø©"""
        try:
            for filename in ['full_page.png', 'viewport.png']:
                filepath = os.path.join(screenshot_dir, filename)
                if os.path.exists(filepath):
                    # ÙØªØ­ Ø§Ù„ØµÙˆØ±Ø© ÙˆØªØ­Ø³ÙŠÙ†Ù‡Ø§
                    with Image.open(filepath) as img:
                        # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ RGB Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±
                        if img.mode != 'RGB':
                            img = img.convert('RGB')
                        
                        # Ø­ÙØ¸ Ø¨Ø¬ÙˆØ¯Ø© Ù…Ø­Ø³Ù†Ø©
                        optimized_path = os.path.join(screenshot_dir, f"optimized_{filename}")
                        img.save(optimized_path, 'PNG', optimize=True)
                        
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ø³ÙŠÙ† Ù„Ù‚Ø·Ø§Øª Ø§Ù„Ø´Ø§Ø´Ø©: {e}")
    
    async def _create_thumbnail(self, original_path, screenshot_dir):
        """Ø¥Ù†Ø´Ø§Ø¡ thumbnail ØµØºÙŠØ±"""
        try:
            if os.path.exists(original_path):
                with Image.open(original_path) as img:
                    # Ø¥Ù†Ø´Ø§Ø¡ thumbnail Ø¨Ø­Ø¬Ù… 300x200
                    img.thumbnail((300, 200), Image.Resampling.LANCZOS)
                    thumbnail_path = os.path.join(screenshot_dir, "thumbnail.png")
                    img.save(thumbnail_path, 'PNG')
                    
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ thumbnail: {e}")
    
    async def extract_page_metadata(self, content, url, page_id):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙˆØµÙÙŠØ© Ù„Ù„ØµÙØ­Ø©"""
        try:
            soup = BeautifulSoup(content, 'html.parser')
            
            metadata = {
                'url': url,
                'page_id': page_id,
                'timestamp': datetime.now().isoformat(),
                'title': soup.title.string if soup.title else "Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†",
                'meta_description': '',
                'meta_keywords': '',
                'h1_tags': [],
                'h2_tags': [],
                'images': [],
                'links': [],
                'forms': [],
                'scripts': [],
                'stylesheets': []
            }
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ meta tags
            for meta in soup.find_all('meta'):
                if meta.get('name') == 'description':
                    metadata['meta_description'] = meta.get('content', '')
                elif meta.get('name') == 'keywords':
                    metadata['meta_keywords'] = meta.get('content', '')
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ†
            metadata['h1_tags'] = [h1.get_text(strip=True) for h1 in soup.find_all('h1')]
            metadata['h2_tags'] = [h2.get_text(strip=True) for h2 in soup.find_all('h2')]
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙˆØ±
            for img in soup.find_all('img'):
                img_info = {
                    'src': img.get('src', ''),
                    'alt': img.get('alt', ''),
                    'title': img.get('title', '')
                }
                metadata['images'].append(img_info)
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·
            for link in soup.find_all('a', href=True):
                href = link.get('href')
                if href:
                    link_info = {
                        'href': href,
                        'text': link.get_text(strip=True),
                        'title': link.get('title', '')
                    }
                    metadata['links'].append(link_info)
            
            # Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙˆØµÙÙŠØ©
            metadata_dir = os.path.join(self.output_root, "metadata")
            metadata_file = os.path.join(metadata_dir, f"{page_id}_metadata.json")
            
            async with aiofiles.open(metadata_file, 'w', encoding='utf-8') as f:
                await f.write(json.dumps(metadata, ensure_ascii=False, indent=2))
            
            return metadata
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙˆØµÙÙŠØ© Ù„Ù€ {url}: {e}")
            return None
    
    async def crawl_and_screenshot(self, page, url, depth=0):
        """Ø§Ù„Ø²Ø­Ù ÙˆØ£Ø®Ø° Ù„Ù‚Ø·Ø§Øª Ø´Ø§Ø´Ø©"""
        if depth > self.max_depth or url in self.visited or len(self.visited) >= self.max_pages:
            return
        
        self.visited.add(url)
        self.stats['total_pages'] += 1
        
        try:
            logger.info(f"ğŸ” Ø²Ø­Ù Ø¥Ù„Ù‰: {url} (Ø§Ù„Ø¹Ù…Ù‚: {depth})")
            
            # Ø§Ù„Ø§Ù†ØªÙ‚Ø§Ù„ Ø¥Ù„Ù‰ Ø§Ù„ØµÙØ­Ø©
            response = await page.goto(url, wait_until="networkidle", timeout=self.timeout)
            if not response:
                raise Exception("ÙØ´Ù„ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø©")
            
            # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØµÙØ­Ø©
            content = await page.content()
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¹Ø±Ù Ù„Ù„ØµÙØ­Ø©
            page_id = self.get_page_identifier(url)
            
            # Ø­ÙØ¸ HTML
            await self.save_html_content(content, page_id, url)
            
            # Ø£Ø®Ø° Ù„Ù‚Ø·Ø§Øª Ø´Ø§Ø´Ø©
            await self.take_enhanced_screenshot(page, page_id, url)
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙˆØµÙÙŠØ©
            await self.extract_page_metadata(content, url, page_id)
            
            # Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ sitemap
            self.sitemap_urls.append(url)
            
            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø±ÙˆØ§Ø¨Ø· Ø£Ø®Ø±Ù‰ Ù„Ù„Ø²Ø­Ù Ø¥Ù„ÙŠÙ‡Ø§
            if depth < self.max_depth:
                soup = BeautifulSoup(content, 'html.parser')
                links = []
                
                for a in soup.find_all('a', href=True):
                    href = a.get('href')
                    if href:
                        next_url = urljoin(self.base_url, href)
                        parsed_next = urlparse(next_url)
                        parsed_base = urlparse(self.base_url)
                        
                        # ÙÙ‚Ø· Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù…Ù† Ù†ÙØ³ Ø§Ù„Ù†Ø·Ø§Ù‚
                        if parsed_next.netloc == parsed_base.netloc:
                            links.append(next_url)
                
                # Ø²Ø­Ù Ù„Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ÙØ±Ø¹ÙŠØ©
                for link in links[:5]:  # Ø­Ø¯ Ø£Ù‚ØµÙ‰ 5 Ø±ÙˆØ§Ø¨Ø· Ù„ÙƒÙ„ ØµÙØ­Ø©
                    await self.crawl_and_screenshot(page, link, depth + 1)
            
            logger.info(f"âœ… ØªÙ… Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡ Ù…Ù† Ù…Ø¹Ø§Ù„Ø¬Ø©: {url}")
            
        except PlaywrightTimeoutError:
            logger.error(f"â° Ø§Ù†ØªÙ‡Øª Ù…Ù‡Ù„Ø© ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø©: {url}")
            self.failed_urls.append({'url': url, 'error': 'timeout', 'type': 'page_load'})
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© {url}: {e}")
            self.failed_urls.append({'url': url, 'error': str(e), 'type': 'general'})
    
    async def generate_sitemap(self):
        """Ø¥Ù†Ø´Ø§Ø¡ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ù…ÙˆÙ‚Ø¹"""
        try:
            sitemap_path = os.path.join(self.output_root, "sitemap.xml")
            
            urlset = ET.Element("urlset", xmlns="http://www.sitemaps.org/schemas/sitemap/0.9")
            
            for url in self.sitemap_urls:
                url_element = ET.SubElement(urlset, "url")
                loc = ET.SubElement(url_element, "loc")
                loc.text = url
                
                lastmod = ET.SubElement(url_element, "lastmod")
                lastmod.text = datetime.now().strftime('%Y-%m-%d')
            
            tree = ET.ElementTree(urlset)
            tree.write(sitemap_path, encoding="utf-8", xml_declaration=True)
            
            logger.info(f"ğŸ“„ ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ù…ÙˆÙ‚Ø¹: {sitemap_path}")
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ù…ÙˆÙ‚Ø¹: {e}")
    
    async def generate_final_report(self):
        """Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ù†Ù‡Ø§Ø¦ÙŠ Ø´Ø§Ù…Ù„"""
        try:
            self.stats['end_time'] = datetime.now()
            if self.stats['start_time']:
                duration = self.stats['end_time'] - self.stats['start_time']
                self.stats['duration_seconds'] = duration.total_seconds()
            
            report = {
                'ØªÙ‚Ø±ÙŠØ±_Ø§Ù„ØªØ­Ù„ÙŠÙ„': {
                    'Ø§Ù„Ù…ÙˆÙ‚Ø¹_Ø§Ù„Ù…Ø³ØªÙ‡Ø¯Ù': self.base_url,
                    'ÙˆÙ‚Øª_Ø§Ù„Ø¨Ø¯Ø¡': self.stats['start_time'].isoformat() if self.stats['start_time'] else None,
                    'ÙˆÙ‚Øª_Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡': self.stats['end_time'].isoformat(),
                    'Ù…Ø¯Ø©_Ø§Ù„ØªØ­Ù„ÙŠÙ„_Ø¨Ø§Ù„Ø«ÙˆØ§Ù†ÙŠ': self.stats.get('duration_seconds', 0),
                    'Ø¥Ø¬Ù…Ø§Ù„ÙŠ_Ø§Ù„ØµÙØ­Ø§Øª': self.stats['total_pages'],
                    'Ù„Ù‚Ø·Ø§Øª_Ø´Ø§Ø´Ø©_Ù†Ø§Ø¬Ø­Ø©': self.stats['successful_screenshots'],
                    'Ù„Ù‚Ø·Ø§Øª_Ø´Ø§Ø´Ø©_ÙØ§Ø´Ù„Ø©': self.stats['failed_screenshots'],
                    'Ø§Ù„Ø±ÙˆØ§Ø¨Ø·_Ø§Ù„ÙØ§Ø´Ù„Ø©': len(self.failed_urls),
                    'Ø¥Ø¬Ù…Ø§Ù„ÙŠ_Ø±ÙˆØ§Ø¨Ø·_sitemap': len(self.sitemap_urls)
                },
                'Ø§Ù„Ù…Ù„ÙØ§Øª_Ø§Ù„Ù…ÙÙ†Ø´Ø£Ø©': {
                    'Ù…Ø¬Ù„Ø¯_Ù„Ù‚Ø·Ø§Øª_Ø§Ù„Ø´Ø§Ø´Ø©': self.screenshots_root,
                    'Ù…Ø¬Ù„Ø¯_HTML': os.path.join(self.output_root, "html_pages"),
                    'Ù…Ø¬Ù„Ø¯_Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª_Ø§Ù„ÙˆØµÙÙŠØ©': os.path.join(self.output_root, "metadata"),
                    'Ø®Ø±ÙŠØ·Ø©_Ø§Ù„Ù…ÙˆÙ‚Ø¹': os.path.join(self.output_root, "sitemap.xml")
                },
                'Ø§Ù„Ø£Ø®Ø·Ø§Ø¡_ÙˆØ§Ù„Ù…Ø´Ø§ÙƒÙ„': self.failed_urls,
                'Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª_Ù…ÙØµÙ„Ø©': {
                    'Ù…Ø¹Ø¯Ù„_Ù†Ø¬Ø§Ø­_Ù„Ù‚Ø·Ø§Øª_Ø§Ù„Ø´Ø§Ø´Ø©': f"{(self.stats['successful_screenshots'] / max(self.stats['total_pages'], 1)) * 100:.2f}%",
                }
            }
            
            report_path = os.path.join(self.output_root, "reports", "final_report.json")
            async with aiofiles.open(report_path, 'w', encoding='utf-8') as f:
                await f.write(json.dumps(report, ensure_ascii=False, indent=2))
            
            # ØªÙ‚Ø±ÙŠØ± Ù†ØµÙŠ Ù…Ø¨Ø³Ø·
            text_report_path = os.path.join(self.output_root, "reports", "summary_report.txt")
            async with aiofiles.open(text_report_path, 'w', encoding='utf-8') as f:
                await f.write(f"""
ØªÙ‚Ø±ÙŠØ± ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
============================

Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ù…Ø³ØªÙ‡Ø¯Ù: {self.base_url}
ÙˆÙ‚Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„: {self.stats['end_time'].strftime('%Y-%m-%d %H:%M:%S')}
Ù…Ø¯Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„: {self.stats.get('duration_seconds', 0):.2f} Ø«Ø§Ù†ÙŠØ©

Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª:
- Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {self.stats['total_pages']}
- Ù„Ù‚Ø·Ø§Øª Ø´Ø§Ø´Ø© Ù†Ø§Ø¬Ø­Ø©: {self.stats['successful_screenshots']}
- Ù„Ù‚Ø·Ø§Øª Ø´Ø§Ø´Ø© ÙØ§Ø´Ù„Ø©: {self.stats['failed_screenshots']}
- Ø§Ù„Ø±ÙˆØ§Ø¨Ø· ÙÙŠ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ù…ÙˆÙ‚Ø¹: {len(self.sitemap_urls)}

Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙÙ†Ø´Ø£Ø©:
- Ù„Ù‚Ø·Ø§Øª Ø§Ù„Ø´Ø§Ø´Ø©: {self.screenshots_root}
- Ù…Ù„ÙØ§Øª HTML: {os.path.join(self.output_root, "html_pages")}
- Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙˆØµÙÙŠØ©: {os.path.join(self.output_root, "metadata")}
- Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ù…ÙˆÙ‚Ø¹: {os.path.join(self.output_root, "sitemap.xml")}

Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ø¬Ø§Ø­: {(self.stats['successful_screenshots'] / max(self.stats['total_pages'], 1)) * 100:.2f}%
""")
            
            logger.info(f"ğŸ“Š ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ: {report_path}")
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ: {e}")
    
    async def run_analysis(self):
        """ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙƒØ§Ù…Ù„"""
        self.stats['start_time'] = datetime.now()
        logger.info(f"ğŸš€ Ø¨Ø¯Ø¡ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ÙˆÙ‚Ø¹: {self.base_url}")
        
        try:
            async with async_playwright() as playwright:
                # Ø¥Ø·Ù„Ø§Ù‚ Ø§Ù„Ù…ØªØµÙØ­
                browser = await playwright.chromium.launch(
                    headless=True,
                    args=['--no-sandbox', '--disable-dev-shm-usage']
                )
                
                # Ø¥Ù†Ø´Ø§Ø¡ ØµÙØ­Ø© Ø¬Ø¯ÙŠØ¯Ø©
                page = await browser.new_page()
                
                # ØªØ¹ÙŠÙŠÙ† Ø­Ø¬Ù… Ø§Ù„Ø¹Ø±Ø¶
                await page.set_viewport_size({
                    'width': self.viewport_width, 
                    'height': self.viewport_height
                })
                
                # Ø¨Ø¯Ø¡ Ø§Ù„Ø²Ø­Ù
                await self.crawl_and_screenshot(page, self.base_url)
                
                # Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ù…ØªØµÙØ­
                await browser.close()
                
            # Ø¥Ù†Ø´Ø§Ø¡ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ù…ÙˆÙ‚Ø¹
            await self.generate_sitemap()
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
            await self.generate_final_report()
            
            logger.info("âœ… ØªÙ… Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡ Ù…Ù† Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ù†Ø¬Ø§Ø­!")
            logger.info(f"ğŸ“ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù…Ø­ÙÙˆØ¸Ø© ÙÙŠ: {self.output_root}")
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ Ø¹Ø§Ù… ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„: {e}")
            raise

# Ø¯Ø§Ù„Ø© Ù„Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ø¨Ø§Ø´Ø±
async def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù„ØªØ´ØºÙŠÙ„"""
    # ÙŠÙ…ÙƒÙ† ØªØ®ØµÙŠØµ Ø§Ù„Ø±Ø§Ø¨Ø· Ù‡Ù†Ø§
    base_url = input("Ø£Ø¯Ø®Ù„ Ø±Ø§Ø¨Ø· Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ù…Ø±Ø§Ø¯ ØªØ­Ù„ÙŠÙ„Ù‡ (Ø§Ø¶ØºØ· Enter Ù„Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ https://ak.sv/): ").strip()
    if not base_url:
        base_url = "https://ak.sv/"
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø£Ø¯Ø§Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„
    tool = EnhancedWebsiteScreenshotTool(base_url=base_url)
    
    # ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ­Ù„ÙŠÙ„
    await tool.run_analysis()

if __name__ == "__main__":
    # ØªØ´ØºÙŠÙ„ Ø§Ù„Ø£Ø¯Ø§Ø©
    asyncio.run(main())