#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ø£Ø¯Ø§Ø© Ø¨Ø³ÙŠØ·Ø© Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ ÙˆØ£Ø®Ø° Ù„Ù‚Ø·Ø§Øª Ø´Ø§Ø´Ø©
Simple Website Screenshot Tool for Replit
ØªØ¹Ù…Ù„ Ø¨Ø¯ÙˆÙ† Ù…ØªØ·Ù„Ø¨Ø§Øª Ù…Ø¹Ù‚Ø¯Ø©
"""

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import os
import json
import time
from datetime import datetime
from fake_useragent import UserAgent
import xml.etree.ElementTree as ET
import re

class SimpleWebsiteAnalyzer:
    """Ø£Ø¯Ø§Ø© ØªØ­Ù„ÙŠÙ„ Ø¨Ø³ÙŠØ·Ø© Ù„Ù„Ù…ÙˆØ§Ù‚Ø¹"""
    
    def __init__(self, base_url="https://ak.sv/", output_dir="simple_site_analysis"):
        self.base_url = base_url
        self.output_dir = output_dir
        self.ua = UserAgent()
        self.session = self._setup_session()
        self.visited_urls = set()
        self.analyzed_pages = []
        self.failed_urls = []
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        self.stats = {
            'total_pages': 0,
            'successful_pages': 0,
            'failed_pages': 0,
            'images_found': 0,
            'links_found': 0,
            'start_time': None,
            'end_time': None
        }
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª
        self._create_directories()
    
    def _setup_session(self):
        """Ø¥Ø¹Ø¯Ø§Ø¯ Ø¬Ù„Ø³Ø© HTTP Ù…Ø­Ø³Ù†Ø©"""
        session = requests.Session()
        session.headers.update({
            'User-Agent': self.ua.random,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ar,en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        return session
    
    def _create_directories(self):
        """Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©"""
        directories = [
            self.output_dir,
            os.path.join(self.output_dir, "html_pages"),
            os.path.join(self.output_dir, "page_data"),
            os.path.join(self.output_dir, "assets_info"),
            os.path.join(self.output_dir, "reports")
        ]
        
        for directory in directories:
            os.makedirs(directory, exist_ok=True)
    
    def sanitize_filename(self, text, max_length=100):
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ ÙƒØ§Ø³Ù… Ù…Ù„Ù"""
        if not text:
            return "unknown"
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ø­Ø±Ù Ø§Ù„Ø®Ø§ØµØ©
        safe_name = re.sub(r'[^\w\-_.]', '_', text)
        safe_name = re.sub(r'_+', '_', safe_name).strip('_')
        
        if len(safe_name) > max_length:
            safe_name = safe_name[:max_length]
            
        return safe_name or "unknown"
    
    def get_page_id(self, url):
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¹Ø±Ù Ø§Ù„ØµÙØ­Ø©"""
        try:
            parsed = urlparse(url)
            path = parsed.path.strip("/") or "home"
            query = parsed.query
            
            page_id = self.sanitize_filename(path)
            if query:
                query_safe = self.sanitize_filename(query)
                page_id += f"_q_{query_safe}"
                
            return page_id
        except:
            return f"page_{len(self.visited_urls)}"
    
    def analyze_page(self, url, max_retries=3):
        """ØªØ­Ù„ÙŠÙ„ ØµÙØ­Ø© ÙˆØ§Ø­Ø¯Ø©"""
        if url in self.visited_urls:
            return None
            
        self.visited_urls.add(url)
        self.stats['total_pages'] += 1
        
        for attempt in range(max_retries):
            try:
                print(f"ğŸ” ØªØ­Ù„ÙŠÙ„: {url} (Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© {attempt + 1})")
                
                response = self.session.get(url, timeout=15)
                response.raise_for_status()
                
                # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
                soup = BeautifulSoup(response.content, 'html.parser')
                page_id = self.get_page_id(url)
                
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
                page_data = self._extract_page_data(soup, url, response)
                
                # Ø­ÙØ¸ HTML
                self._save_html(response.text, page_id, url)
                
                # Ø­ÙØ¸ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙØ­Ø©
                self._save_page_data(page_data, page_id)
                
                # Ø­ÙØ¸ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø£ØµÙˆÙ„
                self._save_assets_info(page_data['assets'], page_id)
                
                self.analyzed_pages.append(page_data)
                self.stats['successful_pages'] += 1
                
                print(f"âœ… ØªÙ… ØªØ­Ù„ÙŠÙ„: {url}")
                return page_data
                
            except requests.exceptions.RequestException as e:
                print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø·Ù„Ø¨ Ù„Ù„ØµÙØ­Ø© {url}: {e}")
                if attempt == max_retries - 1:
                    self.failed_urls.append({'url': url, 'error': str(e)})
                    self.stats['failed_pages'] += 1
                else:
                    time.sleep(2)  # Ø§Ù†ØªØ¸Ø§Ø± Ù‚Ø¨Ù„ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„ØªØ§Ù„ÙŠØ©
                    
            except Exception as e:
                print(f"âŒ Ø®Ø·Ø£ Ø¹Ø§Ù… ÙÙŠ ØªØ­Ù„ÙŠÙ„ {url}: {e}")
                self.failed_urls.append({'url': url, 'error': str(e)})
                self.stats['failed_pages'] += 1
                break
        
        return None
    
    def _extract_page_data(self, soup, url, response):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙØ­Ø©"""
        # Ø§Ù„Ø¹Ù†ÙˆØ§Ù† ÙˆØ§Ù„ÙˆØµÙ
        title = soup.title.string.strip() if soup.title else "Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†"
        
        meta_description = ""
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc:
            meta_description = meta_desc.get('content', '')
        
        # Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ†
        headings = {
            'h1': [h.get_text(strip=True) for h in soup.find_all('h1')],
            'h2': [h.get_text(strip=True) for h in soup.find_all('h2')],
            'h3': [h.get_text(strip=True) for h in soup.find_all('h3')]
        }
        
        # Ø§Ù„Ø±ÙˆØ§Ø¨Ø·
        links = []
        for a in soup.find_all('a', href=True):
            href = a.get('href')
            if href:
                full_url = urljoin(url, href)
                links.append({
                    'href': full_url,
                    'text': a.get_text(strip=True),
                    'title': a.get('title', '')
                })
        
        # Ø§Ù„ØµÙˆØ±
        images = []
        for img in soup.find_all('img'):
            src = img.get('src')
            if src:
                full_src = urljoin(url, src)
                images.append({
                    'src': full_src,
                    'alt': img.get('alt', ''),
                    'title': img.get('title', ''),
                    'width': img.get('width', ''),
                    'height': img.get('height', '')
                })
        
        # Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø£Ø®Ø±Ù‰
        forms = len(soup.find_all('form'))
        scripts = len(soup.find_all('script'))
        stylesheets = len(soup.find_all('link', rel='stylesheet'))
        
        # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        self.stats['images_found'] += len(images)
        self.stats['links_found'] += len(links)
        
        return {
            'url': url,
            'title': title,
            'meta_description': meta_description,
            'status_code': response.status_code,
            'content_length': len(response.content),
            'timestamp': datetime.now().isoformat(),
            'headings': headings,
            'links': links,
            'images': images,
            'assets': {
                'forms_count': forms,
                'scripts_count': scripts,
                'stylesheets_count': stylesheets,
                'images_count': len(images),
                'links_count': len(links)
            },
            'page_text_length': len(soup.get_text()),
            'response_headers': dict(response.headers)
        }
    
    def _save_html(self, content, page_id, url):
        """Ø­ÙØ¸ Ù…Ø­ØªÙˆÙ‰ HTML"""
        try:
            html_file = os.path.join(self.output_dir, "html_pages", f"{page_id}.html")
            with open(html_file, 'w', encoding='utf-8') as f:
                f.write(content)
            
            # Ø­ÙØ¸ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØµÙØ­Ø©
            info_file = os.path.join(self.output_dir, "html_pages", f"{page_id}_info.txt")
            with open(info_file, 'w', encoding='utf-8') as f:
                f.write(f"URL: {url}\n")
                f.write(f"Page ID: {page_id}\n")
                f.write(f"Saved: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"Content Length: {len(content)} characters\n")
                
        except Exception as e:
            print(f"Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ HTML Ù„Ù€ {page_id}: {e}")
    
    def _save_page_data(self, page_data, page_id):
        """Ø­ÙØ¸ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙØ­Ø©"""
        try:
            data_file = os.path.join(self.output_dir, "page_data", f"{page_id}_data.json")
            with open(data_file, 'w', encoding='utf-8') as f:
                json.dump(page_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙØ­Ø© {page_id}: {e}")
    
    def _save_assets_info(self, assets_info, page_id):
        """Ø­ÙØ¸ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø£ØµÙˆÙ„"""
        try:
            assets_file = os.path.join(self.output_dir, "assets_info", f"{page_id}_assets.json")
            with open(assets_file, 'w', encoding='utf-8') as f:
                json.dump(assets_info, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø£ØµÙˆÙ„ {page_id}: {e}")
    
    def discover_pages(self, start_url, max_pages=20, max_depth=2):
        """Ø§ÙƒØªØ´Ø§Ù ØµÙØ­Ø§Øª Ø§Ù„Ù…ÙˆÙ‚Ø¹"""
        discovered_urls = set([start_url])
        current_depth = 0
        
        while current_depth < max_depth and len(discovered_urls) < max_pages:
            current_level_urls = list(discovered_urls - self.visited_urls)
            if not current_level_urls:
                break
                
            print(f"ğŸŒ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø¹Ù…Ù‚ {current_depth + 1}: Ù…Ø¹Ø§Ù„Ø¬Ø© {len(current_level_urls)} ØµÙØ­Ø©")
            
            new_urls = set()
            for url in current_level_urls[:max_pages]:
                page_data = self.analyze_page(url)
                if page_data:
                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø±ÙˆØ§Ø¨Ø· Ø¬Ø¯ÙŠØ¯Ø© Ù…Ù† Ù†ÙØ³ Ø§Ù„Ù†Ø·Ø§Ù‚
                    for link in page_data['links']:
                        link_url = link['href']
                        if self._is_same_domain(link_url, start_url) and link_url not in discovered_urls:
                            new_urls.add(link_url)
                
                if len(discovered_urls) >= max_pages:
                    break
            
            discovered_urls.update(new_urls)
            current_depth += 1
            
            # ØªØ­Ø¯ÙŠØ¯ Ø¹Ø¯Ø¯ Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ø§Ù„Ù…ÙƒØªØ´ÙØ©
            print(f"ğŸ” ØªÙ… Ø§ÙƒØªØ´Ø§Ù {len(new_urls)} Ø±Ø§Ø¨Ø· Ø¬Ø¯ÙŠØ¯")
        
        return discovered_urls
    
    def _is_same_domain(self, url1, url2):
        """ÙØ­Øµ Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø±Ø§Ø¨Ø·Ø§Ù† Ù…Ù† Ù†ÙØ³ Ø§Ù„Ù†Ø·Ø§Ù‚"""
        try:
            domain1 = urlparse(url1).netloc
            domain2 = urlparse(url2).netloc
            return domain1 == domain2
        except:
            return False
    
    def generate_sitemap(self):
        """Ø¥Ù†Ø´Ø§Ø¡ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ù…ÙˆÙ‚Ø¹ XML"""
        try:
            sitemap_path = os.path.join(self.output_dir, "sitemap.xml")
            
            urlset = ET.Element("urlset", xmlns="http://www.sitemaps.org/schemas/sitemap/0.9")
            
            for page_data in self.analyzed_pages:
                url_element = ET.SubElement(urlset, "url")
                loc = ET.SubElement(url_element, "loc")
                loc.text = page_data['url']
                
                lastmod = ET.SubElement(url_element, "lastmod")
                lastmod.text = datetime.now().strftime('%Y-%m-%d')
            
            tree = ET.ElementTree(urlset)
            tree.write(sitemap_path, encoding="utf-8", xml_declaration=True)
            
            print(f"ğŸ“„ ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ù…ÙˆÙ‚Ø¹: {sitemap_path}")
            
        except Exception as e:
            print(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ù…ÙˆÙ‚Ø¹: {e}")
    
    def generate_reports(self):
        """Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©"""
        try:
            self.stats['end_time'] = datetime.now()
            if self.stats['start_time']:
                duration = self.stats['end_time'] - self.stats['start_time']
                self.stats['duration_seconds'] = duration.total_seconds()
            
            # ØªÙ‚Ø±ÙŠØ± JSON Ù…ÙØµÙ„
            detailed_report = {
                'Ù…ÙˆÙ‚Ø¹_Ø§Ù„ØªØ­Ù„ÙŠÙ„': self.base_url,
                'Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª': self.stats,
                'Ø§Ù„ØµÙØ­Ø§Øª_Ø§Ù„Ù…Ø­Ù„Ù„Ø©': self.analyzed_pages,
                'Ø§Ù„Ø±ÙˆØ§Ø¨Ø·_Ø§Ù„ÙØ§Ø´Ù„Ø©': self.failed_urls,
                'Ù…Ù„Ø®Øµ_Ø§Ù„Ø£ØµÙˆÙ„': {
                    'Ø¥Ø¬Ù…Ø§Ù„ÙŠ_Ø§Ù„ØµÙˆØ±': self.stats['images_found'],
                    'Ø¥Ø¬Ù…Ø§Ù„ÙŠ_Ø§Ù„Ø±ÙˆØ§Ø¨Ø·': self.stats['links_found']
                }
            }
            
            report_file = os.path.join(self.output_dir, "reports", "detailed_report.json")
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(detailed_report, f, ensure_ascii=False, indent=2)
            
            # ØªÙ‚Ø±ÙŠØ± Ù†ØµÙŠ Ù…Ø¨Ø³Ø·
            summary_file = os.path.join(self.output_dir, "reports", "summary_report.txt")
            with open(summary_file, 'w', encoding='utf-8') as f:
                f.write(f"""
ØªÙ‚Ø±ÙŠØ± ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ÙˆÙ‚Ø¹
==================

Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ù…Ø­Ù„Ù„: {self.base_url}
ÙˆÙ‚Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„: {self.stats['end_time'].strftime('%Y-%m-%d %H:%M:%S')}
Ù…Ø¯Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„: {self.stats.get('duration_seconds', 0):.2f} Ø«Ø§Ù†ÙŠØ©

Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª:
- Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØµÙØ­Ø§Øª: {self.stats['total_pages']}
- ØµÙØ­Ø§Øª Ù†Ø§Ø¬Ø­Ø©: {self.stats['successful_pages']}
- ØµÙØ­Ø§Øª ÙØ§Ø´Ù„Ø©: {self.stats['failed_pages']}
- Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØµÙˆØ±: {self.stats['images_found']}
- Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·: {self.stats['links_found']}

Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ø¬Ø§Ø­: {(self.stats['successful_pages'] / max(self.stats['total_pages'], 1)) * 100:.1f}%

Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙÙ†Ø´Ø£Ø©:
- ØµÙØ­Ø§Øª HTML: {os.path.join(self.output_dir, "html_pages")}
- Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙØ­Ø§Øª: {os.path.join(self.output_dir, "page_data")}
- Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø£ØµÙˆÙ„: {os.path.join(self.output_dir, "assets_info")}
- Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ù…ÙˆÙ‚Ø¹: {os.path.join(self.output_dir, "sitemap.xml")}
""")
            
            print(f"ğŸ“Š ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ± ÙÙŠ: {os.path.join(self.output_dir, 'reports')}")
            
        except Exception as e:
            print(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ±: {e}")
    
    def run_full_analysis(self, max_pages=20, max_depth=2):
        """ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙƒØ§Ù…Ù„"""
        self.stats['start_time'] = datetime.now()
        
        print(f"ğŸš€ Ø¨Ø¯Ø¡ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ÙˆÙ‚Ø¹: {self.base_url}")
        print(f"ğŸ“Š Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„: {max_pages} ØµÙØ­Ø©ØŒ Ø¹Ù…Ù‚ {max_depth}")
        print("-" * 60)
        
        try:
            # Ø§ÙƒØªØ´Ø§Ù ÙˆØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙØ­Ø§Øª
            discovered_urls = self.discover_pages(self.base_url, max_pages, max_depth)
            
            print(f"\nğŸ“ˆ Ù…Ù„Ø®Øµ Ø§Ù„Ù†ØªØ§Ø¦Ø¬:")
            print(f"- ØªÙ… Ø§ÙƒØªØ´Ø§Ù: {len(discovered_urls)} Ø±Ø§Ø¨Ø·")
            print(f"- ØªÙ… ØªØ­Ù„ÙŠÙ„: {self.stats['successful_pages']} ØµÙØ­Ø© Ø¨Ù†Ø¬Ø§Ø­")
            print(f"- ÙØ´Ù„ ÙÙŠ: {self.stats['failed_pages']} ØµÙØ­Ø©")
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ù…ÙˆÙ‚Ø¹
            self.generate_sitemap()
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ±
            self.generate_reports()
            
            print(f"\nâœ… ØªÙ… Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡ Ù…Ù† Ø§Ù„ØªØ­Ù„ÙŠÙ„!")
            print(f"ğŸ“ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù…Ø­ÙÙˆØ¸Ø© ÙÙŠ: {self.output_dir}")
            
            return True
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ Ø¹Ø§Ù… ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„: {e}")
            return False

def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù„ØªØ´ØºÙŠÙ„"""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   Ø£Ø¯Ø§Ø© ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„Ø¨Ø³ÙŠØ·Ø©                  â•‘
â•‘              Simple Website Analysis Tool                    â•‘
â•‘                                                              â•‘
â•‘        Ù…Ø·ÙˆØ±Ø© Ø®ØµÙŠØµØ§Ù‹ Ù„Ø¨ÙŠØ¦Ø© Replit - ØªØ¹Ù…Ù„ Ø¨Ø¯ÙˆÙ† ØªØ¹Ù‚ÙŠØ¯Ø§Øª         â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ù…Ù† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
    base_url = input("Ø£Ø¯Ø®Ù„ Ø±Ø§Ø¨Ø· Ø§Ù„Ù…ÙˆÙ‚Ø¹ (Enter Ù„Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ https://ak.sv/): ").strip()
    if not base_url:
        base_url = "https://ak.sv/"
    
    output_dir = input("Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø­ÙØ¸ (Enter Ù„Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ simple_site_analysis): ").strip()
    if not output_dir:
        output_dir = "simple_site_analysis"
    
    try:
        max_pages = int(input("Ø¹Ø¯Ø¯ Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„Ù‚ØµÙˆÙ‰ (Enter Ù„Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ 20): ").strip() or "20")
    except ValueError:
        max_pages = 20
    
    try:
        max_depth = int(input("Ø¹Ù…Ù‚ Ø§Ù„ØªØ­Ù„ÙŠÙ„ (Enter Ù„Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ 2): ").strip() or "2")
    except ValueError:
        max_depth = 2
    
    # Ø¥Ù†Ø´Ø§Ø¡ ÙˆØªØ´ØºÙŠÙ„ Ø§Ù„Ø£Ø¯Ø§Ø©
    analyzer = SimpleWebsiteAnalyzer(base_url=base_url, output_dir=output_dir)
    
    try:
        result = analyzer.run_full_analysis(max_pages=max_pages, max_depth=max_depth)
        if result:
            print("\nğŸ‰ ØªÙ… Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ù†Ø¬Ø§Ø­!")
        else:
            print("\nâŒ ÙØ´Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„")
    except KeyboardInterrupt:
        print("\nâš ï¸ ØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…")
    except Exception as e:
        print(f"\nâŒ Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹: {e}")

if __name__ == "__main__":
    main()