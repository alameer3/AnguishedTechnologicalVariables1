#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ø£Ø¯Ø§Ø© ØªØ­Ù„ÙŠÙ„ ÙˆØ£Ø®Ø° Ù„Ù‚Ø·Ø§Øª Ø´Ø§Ø´Ø© Ø´Ø§Ù…Ù„Ø© Ù„Ù„Ù…ÙˆØ§Ù‚Ø¹
Enhanced Website Screenshot and Analysis Tool
Ù…Ø·ÙˆØ± Ø®ØµÙŠØµØ§Ù‹ Ù„Ø¨ÙŠØ¦Ø© Replit Ù…Ø¹ Ø¯Ø¹Ù… ÙƒØ§Ù…Ù„ Ù„Ù„Ù‚Ø·Ø§Øª Ø§Ù„Ø´Ø§Ø´Ø© PNG
"""

import nest_asyncio
nest_asyncio.apply()

import asyncio
import aiofiles
import json
from datetime import datetime
from pathlib import Path
from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError
from bs4 import BeautifulSoup
import os
from urllib.parse import urljoin, urlparse, unquote
from fake_useragent import UserAgent
import mimetypes
import xml.etree.ElementTree as ET
import requests
from PIL import Image
import time
import logging

# Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ø¸Ø§Ù… ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('screenshot_log.txt', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…Ø­Ø³Ù†Ø©
class ScreenshotConfig:
    def __init__(self):
        self.base_url = "https://ak.sv/"  # ÙŠÙ…ÙƒÙ† ØªØ¹Ø¯ÙŠÙ„Ù‡
        self.output_root = "site_analysis_complete"
        self.screenshots_root = "screenshots"
        self.max_depth = 3  # Ù…Ø®ÙØ¶ Ù„Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø£ÙØ¶Ù„
        self.max_pages = 50  # Ø­Ø¯ Ø£Ù‚ØµÙ‰ Ù„Ù„ØµÙØ­Ø§Øª
        self.screenshot_delay = 2  # Ø«Ø§Ù†ÙŠØ© Ø§Ù†ØªØ¸Ø§Ø± Ù‚Ø¨Ù„ Ø£Ø®Ø° Ù„Ù‚Ø·Ø© Ø§Ù„Ø´Ø§Ø´Ø©
        self.viewport_width = 1920
        self.viewport_height = 1080
        self.screenshot_quality = 90
        self.timeout = 30000  # 30 Ø«Ø§Ù†ÙŠØ©
        
config = ScreenshotConfig()
ua = UserAgent()
visited = set()
sitemap_urls = []
failed_urls = []
screenshot_stats = {
    'total_screenshots': 0,
    'successful_screenshots': 0,
    'failed_screenshots': 0,
    'start_time': None,
    'end_time': None
}

# Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª
os.makedirs(config.output_root, exist_ok=True)
os.makedirs(config.screenshots_root, exist_ok=True)

def sanitize_path(url):
    """ØªÙ†Ø¸ÙŠÙ Ù…Ø³Ø§Ø± URL Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ ÙƒØ§Ø³Ù… Ù…Ø¬Ù„Ø¯"""
    try:
        parsed = urlparse(url)
        path = parsed.path.strip("/").replace("/", "_") or "home"
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ø­Ø±Ù Ø§Ù„Ø®Ø§ØµØ© ÙˆØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø§Ø³Ù…
        path = "".join(c for c in path if c.isalnum() or c in "_-")
        return path[:100]  # ØªØ­Ø¯ÙŠØ¯ Ø·ÙˆÙ„ Ø§Ù„Ø§Ø³Ù…
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø³Ø§Ø± {url}: {e}")
        return "unknown_page"

def create_safe_filename(text, max_length=100):
    """Ø¥Ù†Ø´Ø§Ø¡ Ø§Ø³Ù… Ù…Ù„Ù Ø¢Ù…Ù† Ù…Ù† Ø§Ù„Ù†Øµ"""
    if not text:
        return "unknown"
    
    # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ
    safe_name = "".join(c for c in text if c.isalnum() or c in "_-. ")
    safe_name = safe_name.replace(" ", "_").strip("_")
    
    # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø·ÙˆÙ„
    if len(safe_name) > max_length:
        safe_name = safe_name[:max_length]
    
    return safe_name or "unknown"

def save_html(content, folder):
    os.makedirs(folder, exist_ok=True)
    filename = os.path.join(folder, "index.html")
    with open(filename, "w", encoding="utf-8") as f:
        f.write(content)

def get_asset_folder_by_mime(mime):
    if not mime:
        return "others"
    if mime.startswith("image/"):
        return "images"
    if mime.startswith("font/") or mime in ["application/font-woff", "application/font-woff2"]:
        return "fonts"
    if mime == "text/css":
        return "css"
    if mime == "application/javascript" or mime == "text/javascript":
        return "js"
    if mime.startswith("video/"):
        return "videos"
    if mime.startswith("audio/"):
        return "audios"
    if mime in ["application/pdf", "application/msword", "application/vnd.openxmlformats-officedocument.wordprocessingml.document"]:
        return "docs"
    return "others"

def save_binary(url, page_folder):
    parsed = urlparse(url)
    filename = os.path.basename(parsed.path)
    if not filename or '.' not in filename:
        filename = "file"

    mime, _ = mimetypes.guess_type(filename)
    folder_name = get_asset_folder_by_mime(mime)
    folder = os.path.join(page_folder, folder_name)
    os.makedirs(folder, exist_ok=True)

    filepath = os.path.join(folder, filename)
    if os.path.exists(filepath):
        return filepath
    try:
        headers = {"User-Agent": ua.random}
        r = requests.get(url, headers=headers, timeout=15)
        if r.status_code == 200:
            with open(filepath, "wb") as f:
                f.write(r.content)
            return filepath
    except Exception as e:
        print(f"Failed to download asset {url} - {e}")
    return None

def extract_assets(soup, page_folder, page_url):
    asset_tags = {
        "img": "src",
        "link": "href",
        "script": "src",
        "source": "src",
        "video": "src",
        "audio": "src",
    }
    for tag, attr in asset_tags.items():
        for el in soup.find_all(tag):
            src = el.get(attr)
            if src and not src.startswith("data:"):
                full_url = urljoin(page_url, src)
                save_binary(full_url, page_folder)

def get_meta_info(soup):
    metas = {}
    for meta in soup.find_all("meta"):
        if meta.get("name"):
            metas[meta.get("name")] = meta.get("content", "")
        elif meta.get("property"):
            metas[meta.get("property")] = meta.get("content", "")
    return metas

def detect_cms(headers, soup):
    # Ø£Ù…Ø«Ù„Ø© Ø¨Ø³ÙŠØ·Ø© Ø¹Ù„Ù‰ Ø§Ù„ÙƒØ´Ù
    server = headers.get("server", "").lower()
    powered_by = headers.get("x-powered-by", "").lower()
    cms = "Unknown"
    if "wordpress" in powered_by or "wp-" in str(soup):
        cms = "WordPress"
    elif "joomla" in powered_by or "joomla" in str(soup).lower():
        cms = "Joomla"
    elif "drupal" in powered_by or "drupal" in str(soup).lower():
        cms = "Drupal"
    elif "shopify" in powered_by or "shopify" in str(soup).lower():
        cms = "Shopify"
    return cms

def check_links(links):
    broken = []
    headers = {"User-Agent": ua.random}
    for link in links:
        try:
            r = requests.head(link, headers=headers, allow_redirects=True, timeout=5)
            if r.status_code >= 400:
                broken.append((link, r.status_code))
        except:
            broken.append((link, "Failed"))
    return broken

async def crawl(page, url, depth=0):
    if depth > config.max_depth or url in visited:
        return
    visited.add(url)
    try:
        response = await page.goto(url, wait_until="networkidle", timeout=30000)
        content = await page.content()
        headers = response.headers if response else {}

        page_folder = os.path.join(config.output_root, sanitize_path(url))
        save_html(content, page_folder)

        # Screenshot
        screenshots_folder = os.path.join(page_folder, "screenshots")
        os.makedirs(screenshots_folder, exist_ok=True)
        screenshot_path = os.path.join(screenshots_folder, "screenshot.png")
        await page.screenshot(path=screenshot_path, full_page=True, type='png')

        soup = BeautifulSoup(content, "html.parser")

        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø£ØµÙˆÙ„
        extract_assets(soup, page_folder, url)

        # Ø­ÙØ¸ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù€ meta
        metas = get_meta_info(soup)
        with open(os.path.join(page_folder, "meta_info.txt"), "w", encoding="utf-8") as f:
            for k, v in metas.items():
                f.write(f"{k}: {v}\n")

        # ÙƒØ´Ù CMS
        cms = detect_cms(headers, soup)
        with open(os.path.join(page_folder, "cms_info.txt"), "w", encoding="utf-8") as f:
            f.write(f"CMS Detected: {cms}\n")

        sitemap_urls.append(url)

        # ØªØªØ¨Ø¹ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ©
        links = []
        for a in soup.find_all("a", href=True):
            href = a.get('href')
            if href:
                next_url = urljoin(config.base_url, href)
                if urlparse(next_url).netloc == urlparse(config.base_url).netloc:
                    links.append(next_url)

        # ÙØ­Øµ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…Ø¹Ø·Ù„Ø©
        broken_links = check_links(links)
        with open(os.path.join(page_folder, "broken_links.txt"), "w", encoding="utf-8") as f:
            for link, status in broken_links:
                f.write(f"{link} -> {status}\n")

        print(f"âœ… Crawled: {url} | Depth: {depth} | CMS: {cms}")

        for link in links[:5]:  # Ø­Ø¯ Ø£Ù‚ØµÙ‰ 5 Ø±ÙˆØ§Ø¨Ø· Ù„ÙƒÙ„ ØµÙØ­Ø©
            await crawl(page, link, depth + 1)

    except Exception as e:
        print(f"âŒ Failed at {url}: {e}")

def save_sitemap(urls, filename="sitemap.xml"):
    urlset = ET.Element("urlset", xmlns="http://www.sitemaps.org/schemas/sitemap/0.9")
    for url in urls:
        url_el = ET.SubElement(urlset, "url")
        loc = ET.SubElement(url_el, "loc")
        loc.text = url
    tree = ET.ElementTree(urlset)
    tree.write(filename, encoding="utf-8", xml_declaration=True)

async def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    print(f"ğŸš€ Ø¨Ø¯Ø¡ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ÙˆÙ‚Ø¹: {config.base_url}")
    screenshot_stats['start_time'] = datetime.now()
    
    try:
        async with async_playwright() as p:
            browser = await p.chromium.launch(
                headless=True,
                args=['--no-sandbox', '--disable-dev-shm-usage']
            )
            page = await browser.new_page()
            
            # ØªØ¹ÙŠÙŠÙ† Ø­Ø¬Ù… Ø§Ù„Ø¹Ø±Ø¶
            await page.set_viewport_size({
                'width': config.viewport_width, 
                'height': config.viewport_height
            })
            
            await crawl(page, config.base_url, 0)
            await browser.close()
            
        save_sitemap(sitemap_urls, os.path.join(config.output_root, "sitemap.xml"))
        
        screenshot_stats['end_time'] = datetime.now()
        duration = screenshot_stats['end_time'] - screenshot_stats['start_time']
        
        print(f"âœ… ØªÙ… Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡ Ù…Ù† Ø§Ù„ØªØ­Ù„ÙŠÙ„!")
        print(f"â±ï¸ Ø§Ù„Ù…Ø¯Ø©: {duration.total_seconds():.2f} Ø«Ø§Ù†ÙŠØ©")
        print(f"ğŸ“Š Ù„Ù‚Ø·Ø§Øª Ø´Ø§Ø´Ø© Ù†Ø§Ø¬Ø­Ø©: {screenshot_stats['successful_screenshots']}")
        print(f"âŒ Ù„Ù‚Ø·Ø§Øª Ø´Ø§Ø´Ø© ÙØ§Ø´Ù„Ø©: {screenshot_stats['failed_screenshots']}")
        print(f"ğŸŒ Sitemap saved to {config.output_root}/sitemap.xml")
        print(f"ğŸ“ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ: {config.output_root}")
        
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ Ø¹Ø§Ù…: {e}")
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„: {e}")

def run_screenshot_tool():
    """ØªØ´ØºÙŠÙ„ Ø§Ù„Ø£Ø¯Ø§Ø©"""
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nâš ï¸ ØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…")
    except Exception as e:
        print(f"\nâŒ Ø®Ø·Ø£ ÙÙŠ ØªØ´ØºÙŠÙ„ Ø§Ù„Ø£Ø¯Ø§Ø©: {e}")

if __name__ == "__main__":
    run_screenshot_tool()