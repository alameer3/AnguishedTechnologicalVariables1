#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ø£Ø¯Ø§Ø© ØªØ­Ù„ÙŠÙ„ ÙˆØ£Ø®Ø° Ù„Ù‚Ø·Ø§Øª Ø´Ø§Ø´Ø© Ø´Ø§Ù…Ù„Ø© Ù„Ù„Ù…ÙˆØ§Ù‚Ø¹
Enhanced Website Screenshot and Analysis Tool
Ù…Ø·ÙˆØ± Ø®ØµÙŠØµØ§Ù‹ Ù„Ø¨ÙŠØ¦Ø© Replit Ù…Ø¹ Ø¯Ø¹Ù… ÙƒØ§Ù…Ù„ Ù„Ù„Ù‚Ø·Ø§Øª Ø§Ù„Ø´Ø§Ø´Ø© PNG
ÙŠÙ‚Ø±Ø£ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù…Ù† Ù…Ù„Ù Ø±ÙˆØ§Ø¨Ø·_AKWAM_ØªØ±ØªÙŠØ¨_Ù‡Ø±Ù…ÙŠ_Ø´Ø§Ù…Ù„.txt
"""

import nest_asyncio
nest_asyncio.apply()

import asyncio
import aiofiles
import json
import re
from datetime import datetime
from pathlib import Path
from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError
from bs4 import BeautifulSoup
import os
from urllib.parse import urljoin, urlparse, unquote
from fake_useragent import UserAgent
import mimetypes
import xml.etree.ElementTree as ET
import requests
from PIL import Image
import time
import logging

# =====================================================
# 1. Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ø¸Ø§Ù… ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ ÙˆØ§Ù„ØªÙƒÙˆÙŠÙ† Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
# =====================================================

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('screenshot_log.txt', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class ScreenshotConfig:
    """ÙØ¦Ø© Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø© Ø§Ù„Ø´Ø§Ù…Ù„Ø©"""
    def __init__(self):
        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø£Ø³Ø§Ø³ÙŠØ©
        self.base_url = "https://ak.sv/"
        self.output_root = "site_analysis_complete"
        self.screenshots_root = "screenshots"
        self.links_file = "Ø±ÙˆØ§Ø¨Ø·_AKWAM_ØªØ±ØªÙŠØ¨_Ù‡Ø±Ù…ÙŠ_Ø´Ø§Ù…Ù„.txt"
        
        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„
        self.max_depth = 3
        self.max_pages = 200
        self.max_links_per_batch = 50
        self.screenshot_delay = 2
        self.timeout = 30000
        
        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù„Ù‚Ø·Ø© Ø§Ù„Ø´Ø§Ø´Ø©
        self.viewport_width = 1920
        self.viewport_height = 1080
        self.screenshot_quality = 90
        self.full_page_screenshot = True
        self.screenshot_format = 'png'
        
        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡
        self.concurrent_requests = 3
        self.retry_attempts = 3
        self.delay_between_requests = 1
        
        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ø­ØªÙˆÙ‰
        self.save_html = True
        self.save_assets = True
        self.analyze_cms = True
        self.check_broken_links = True
        self.generate_sitemap = True

# =====================================================
# 2. Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¹Ø§Ù…Ø© ÙˆØ¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª
# =====================================================

config = ScreenshotConfig()
ua = UserAgent()
visited = set()
sitemap_urls = []
failed_urls = []
processed_links = []

screenshot_stats = {
    'total_screenshots': 0,
    'successful_screenshots': 0,
    'failed_screenshots': 0,
    'total_pages_processed': 0,
    'total_assets_downloaded': 0,
    'start_time': None,
    'end_time': None,
    'duration_seconds': 0,
    'links_from_file': 0,
    'links_discovered': 0
}

def create_directory_structure():
    """Ø¥Ù†Ø´Ø§Ø¡ Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨"""
    directories = [
        config.output_root,
        config.screenshots_root,
        os.path.join(config.output_root, "html_pages"),
        os.path.join(config.output_root, "assets", "images"),
        os.path.join(config.output_root, "assets", "css"),
        os.path.join(config.output_root, "assets", "js"),
        os.path.join(config.output_root, "assets", "fonts"),
        os.path.join(config.output_root, "assets", "videos"),
        os.path.join(config.output_root, "assets", "audios"),
        os.path.join(config.output_root, "assets", "docs"),
        os.path.join(config.output_root, "assets", "others"),
        os.path.join(config.output_root, "metadata"),
        os.path.join(config.output_root, "reports"),
        os.path.join(config.output_root, "broken_links"),
        os.path.join(config.output_root, "cms_detection")
    ]
    
    for directory in directories:
        os.makedirs(directory, exist_ok=True)
        
    logger.info(f"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª ÙÙŠ: {config.output_root}")

# Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª Ø¹Ù†Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙˆØ­Ø¯Ø©
create_directory_structure()

# =====================================================
# 3. ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„ÙØ§Øª ÙˆØ§Ù„Ù†ØµÙˆØµ
# =====================================================

def sanitize_path(url):
    """ØªÙ†Ø¸ÙŠÙ Ù…Ø³Ø§Ø± URL Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ ÙƒØ§Ø³Ù… Ù…Ø¬Ù„Ø¯"""
    try:
        parsed = urlparse(url)
        path = parsed.path.strip("/").replace("/", "_") or "home"
        query = parsed.query
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø³Ø§Ø± Ù…Ù† Ø§Ù„Ø£Ø­Ø±Ù Ø§Ù„Ø®Ø§ØµØ©
        safe_path = re.sub(r'[^\w\-_.]', '_', path)
        safe_path = re.sub(r'_+', '_', safe_path).strip('_')
        
        # Ø¥Ø¶Ø§ÙØ© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¥Ø°Ø§ ÙˆØ¬Ø¯Øª
        if query:
            safe_query = re.sub(r'[^\w\-_.]', '_', query)[:50]  # ØªØ­Ø¯ÙŠØ¯ Ø·ÙˆÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
            safe_path += f"_q_{safe_query}"
        
        # ØªØ­Ø¯ÙŠØ¯ Ø·ÙˆÙ„ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
        return safe_path[:100] if safe_path else "unknown_page"
        
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø³Ø§Ø± {url}: {e}")
        return f"page_{len(visited)}"

def create_safe_filename(text, max_length=100):
    """Ø¥Ù†Ø´Ø§Ø¡ Ø§Ø³Ù… Ù…Ù„Ù Ø¢Ù…Ù† Ù…Ù† Ø§Ù„Ù†Øµ"""
    if not text:
        return "unknown"
    
    # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ Ù…Ù† Ø§Ù„Ø£Ø­Ø±Ù Ø§Ù„Ø®Ø§ØµØ©
    safe_name = re.sub(r'[^\w\-_.Ø£-ÙŠ]', '_', str(text))
    safe_name = re.sub(r'_+', '_', safe_name).strip('_')
    
    # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø·ÙˆÙ„
    if len(safe_name) > max_length:
        safe_name = safe_name[:max_length]
    
    return safe_name or "unknown"

def load_links_from_file(file_path):
    """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù…Ù† Ù…Ù„Ù Ø§Ù„Ù†Øµ"""
    links = []
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            content = file.read()
            
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… regex
        url_pattern = r'https://ak\.sv/[^\s\n\r\t]*'
        found_links = re.findall(url_pattern, content)
        
        # ØªÙ†Ø¸ÙŠÙ ÙˆØªØµÙÙŠØ© Ø§Ù„Ø±ÙˆØ§Ø¨Ø·
        for link in found_links:
            # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ø­Ø±Ù Ø§Ù„Ø®Ø§ØµØ© Ù…Ù† Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ø±Ø§Ø¨Ø·
            clean_link = link.rstrip('.,;:!?)')
            if clean_link and clean_link not in links:
                links.append(clean_link)
        
        screenshot_stats['links_from_file'] = len(links)
        logger.info(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(links)} Ø±Ø§Ø¨Ø· Ù…Ù† Ø§Ù„Ù…Ù„Ù: {file_path}")
        
        return links
        
    except FileNotFoundError:
        logger.error(f"âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù„Ù: {file_path}")
        return []
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù {file_path}: {e}")
        return []

# =====================================================
# 4. ÙˆØ¸Ø§Ø¦Ù Ø­ÙØ¸ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ ÙˆØ§Ù„Ø£ØµÙˆÙ„
# =====================================================

def save_html(content, folder):
    """Ø­ÙØ¸ Ù…Ø­ØªÙˆÙ‰ HTML ÙÙŠ Ù…Ø¬Ù„Ø¯ Ù…Ø­Ø¯Ø¯"""
    try:
        os.makedirs(folder, exist_ok=True)
        filename = os.path.join(folder, "index.html")
        
        with open(filename, "w", encoding="utf-8") as f:
            f.write(content)
            
        # Ø­ÙØ¸ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ© Ø¹Ù† Ø§Ù„Ù…Ù„Ù
        info_filename = os.path.join(folder, "page_info.txt")
        with open(info_filename, "w", encoding="utf-8") as f:
            f.write(f"ØªÙ… Ø§Ù„Ø­ÙØ¸: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"Ø­Ø¬Ù… Ø§Ù„Ù…Ø­ØªÙˆÙ‰: {len(content)} Ø­Ø±Ù\n")
            f.write(f"ØªØ±Ù…ÙŠØ² Ø§Ù„Ù…Ù„Ù: UTF-8\n")
            
        return filename
        
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ HTML: {e}")
        return None

def get_asset_folder_by_mime(mime):
    """ØªØ­Ø¯ÙŠØ¯ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø­ÙØ¸ Ø­Ø³Ø¨ Ù†ÙˆØ¹ Ø§Ù„Ù…Ù„Ù"""
    if not mime:
        return "others"
    
    mime_lower = mime.lower()
    
    if mime_lower.startswith("image/"):
        return "images"
    elif mime_lower.startswith("font/") or mime_lower in ["application/font-woff", "application/font-woff2"]:
        return "fonts"
    elif mime_lower == "text/css":
        return "css"
    elif mime_lower in ["application/javascript", "text/javascript"]:
        return "js"
    elif mime_lower.startswith("video/"):
        return "videos"
    elif mime_lower.startswith("audio/"):
        return "audios"
    elif mime_lower in ["application/pdf", "application/msword", 
                       "application/vnd.openxmlformats-officedocument.wordprocessingml.document"]:
        return "docs"
    else:
        return "others"

def save_binary(url, page_folder):
    """ØªØ­Ù…ÙŠÙ„ ÙˆØ­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠØ© (Ø§Ù„ØµÙˆØ±ØŒ CSSØŒ JSØŒ Ø¥Ù„Ø®)"""
    try:
        parsed = urlparse(url)
        filename = os.path.basename(parsed.path)
        
        if not filename or '.' not in filename:
            filename = f"file_{hash(url) % 10000}"

        # ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„Ù…Ù„Ù ÙˆÙ…Ø¬Ù„Ø¯ Ø§Ù„Ø­ÙØ¸
        mime, _ = mimetypes.guess_type(filename)
        folder_name = get_asset_folder_by_mime(mime)
        folder = os.path.join(page_folder, "assets", folder_name)
        os.makedirs(folder, exist_ok=True)

        filepath = os.path.join(folder, filename)
        
        # ØªØ¬Ù†Ø¨ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù…Ù„Ù Ù…ÙˆØ¬ÙˆØ¯
        if os.path.exists(filepath):
            return filepath

        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù
        headers = {"User-Agent": ua.random}
        response = requests.get(url, headers=headers, timeout=15, stream=True)
        
        if response.status_code == 200:
            with open(filepath, "wb") as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            screenshot_stats['total_assets_downloaded'] += 1
            return filepath
        else:
            logger.warning(f"ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø£ØµÙ„ {url}: ÙƒÙˆØ¯ Ø§Ù„Ø­Ø§Ù„Ø© {response.status_code}")
            
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø£ØµÙ„ {url}: {e}")
        
    return None

def extract_assets(soup, page_folder, page_url):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆØªØ­Ù…ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£ØµÙˆÙ„ Ù…Ù† Ø§Ù„ØµÙØ­Ø©"""
    asset_tags = {
        "img": "src",
        "link": "href", 
        "script": "src",
        "source": "src",
        "video": "src",
        "audio": "src",
        "embed": "src",
        "object": "data"
    }
    
    extracted_assets = []
    
    for tag, attr in asset_tags.items():
        for element in soup.find_all(tag):
            src = element.get(attr)
            
            if src and not src.startswith("data:") and not src.startswith("javascript:"):
                try:
                    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ù†Ø³Ø¨ÙŠ Ø¥Ù„Ù‰ Ù…Ø·Ù„Ù‚
                    full_url = urljoin(page_url, src)
                    
                    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø£ØµÙ„
                    saved_path = save_binary(full_url, page_folder)
                    
                    if saved_path:
                        extracted_assets.append({
                            'tag': tag,
                            'original_url': full_url,
                            'saved_path': saved_path,
                            'attribute': attr
                        })
                        
                except Exception as e:
                    logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£ØµÙ„ {src}: {e}")
    
    # Ø­ÙØ¸ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø£ØµÙˆÙ„
    assets_info_file = os.path.join(page_folder, "assets_info.json")
    try:
        with open(assets_info_file, "w", encoding="utf-8") as f:
            json.dump(extracted_assets, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø£ØµÙˆÙ„: {e}")
    
    return extracted_assets

# =====================================================
# 5. ÙˆØ¸Ø§Ø¦Ù Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙˆØµÙÙŠØ©
# =====================================================

def get_meta_info(soup):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬Ù…ÙŠØ¹ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Meta Ù…Ù† Ø§Ù„ØµÙØ­Ø©"""
    meta_info = {}
    
    try:
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬Ù…ÙŠØ¹ meta tags
        for meta in soup.find_all("meta"):
            # Meta tags with name attribute
            if meta.get("name"):
                meta_info[f"name_{meta.get('name')}"] = meta.get("content", "")
            # Meta tags with property attribute (Open Graph, etc.)
            elif meta.get("property"):
                meta_info[f"property_{meta.get('property')}"] = meta.get("content", "")
            # Meta tags with http-equiv attribute
            elif meta.get("http-equiv"):
                meta_info[f"http_equiv_{meta.get('http-equiv')}"] = meta.get("content", "")
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©
        if soup.title:
            meta_info["page_title"] = soup.title.string.strip()
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ canonical URL
        canonical = soup.find("link", rel="canonical")
        if canonical:
            meta_info["canonical_url"] = canonical.get("href", "")
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù„ØºØ©
        html_tag = soup.find("html")
        if html_tag:
            meta_info["page_lang"] = html_tag.get("lang", "")
        
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Meta: {e}")
    
    return meta_info

def detect_cms(headers, soup):
    """ÙƒØ´Ù Ù†Ø¸Ø§Ù… Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…"""
    cms_indicators = {
        "WordPress": [
            "wp-content", "wp-includes", "wp-admin", 
            "wordpress", "woocommerce"
        ],
        "Joomla": [
            "joomla", "com_content", "mod_", "administrator"
        ],
        "Drupal": [
            "drupal", "sites/all", "sites/default"
        ],
        "Shopify": [
            "shopify", "cdn.shopify.com", "checkout.shopify.com"
        ],
        "Magento": [
            "magento", "mage", "varien"
        ],
        "PrestaShop": [
            "prestashop", "ps_", "prestashop.com"
        ],
        "Laravel": [
            "laravel_session", "csrf-token", "laravel"
        ],
        "React": [
            "react", "__REACT", "react-dom"
        ],
        "Vue.js": [
            "vue", "__VUE__", "v-"
        ],
        "Angular": [
            "angular", "ng-", "__ANGULAR"
        ]
    }
    
    detected_cms = []
    
    try:
        # ÙØ­Øµ Ø§Ù„Ù‡ÙŠØ¯Ø±Ø²
        server = headers.get("server", "").lower()
        powered_by = headers.get("x-powered-by", "").lower()
        
        # ÙØ­Øµ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØµÙØ­Ø©
        page_content = str(soup).lower()
        
        for cms, indicators in cms_indicators.items():
            for indicator in indicators:
                if (indicator.lower() in server or 
                    indicator.lower() in powered_by or 
                    indicator.lower() in page_content):
                    if cms not in detected_cms:
                        detected_cms.append(cms)
                    break
        
        return detected_cms if detected_cms else ["Unknown"]
        
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ ÙƒØ´Ù CMS: {e}")
        return ["Error in Detection"]

def extract_page_structure(soup):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù‡ÙŠÙƒÙ„ Ø§Ù„ØµÙØ­Ø© ÙˆØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰"""
    structure = {
        'headings': {},
        'links': [],
        'images': [],
        'forms': [],
        'tables': [],
        'lists': [],
        'paragraphs_count': 0,
        'word_count': 0
    }
    
    try:
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ†
        for i in range(1, 7):
            headings = soup.find_all(f'h{i}')
            structure['headings'][f'h{i}'] = [h.get_text(strip=True) for h in headings]
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·
        for link in soup.find_all('a', href=True):
            structure['links'].append({
                'href': link.get('href'),
                'text': link.get_text(strip=True),
                'title': link.get('title', '')
            })
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙˆØ±
        for img in soup.find_all('img'):
            structure['images'].append({
                'src': img.get('src', ''),
                'alt': img.get('alt', ''),
                'title': img.get('title', ''),
                'width': img.get('width', ''),
                'height': img.get('height', '')
            })
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
        for form in soup.find_all('form'):
            inputs = [inp.get('type', 'text') for inp in form.find_all('input')]
            structure['forms'].append({
                'action': form.get('action', ''),
                'method': form.get('method', 'get'),
                'inputs': inputs
            })
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„
        for table in soup.find_all('table'):
            rows = len(table.find_all('tr'))
            cols = len(table.find_all('th')) or len(table.find_all('td')[:1])
            structure['tables'].append({
                'rows': rows,
                'columns': cols
            })
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„ÙÙ‚Ø±Ø§Øª ÙˆØ§Ù„ÙƒÙ„Ù…Ø§Øª
        paragraphs = soup.find_all('p')
        structure['paragraphs_count'] = len(paragraphs)
        
        text_content = soup.get_text()
        structure['word_count'] = len(text_content.split())
        
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù‡ÙŠÙƒÙ„ Ø§Ù„ØµÙØ­Ø©: {e}")
    
    return structure

# =====================================================
# 6. ÙˆØ¸Ø§Ø¦Ù ÙØ­Øµ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…Ø¹Ø·Ù„Ø©
# =====================================================

def check_links(links):
    """ÙØ­Øµ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…Ø¹Ø·Ù„Ø©"""
    broken_links = []
    headers = {"User-Agent": ua.random}
    
    for link in links[:20]:  # ÙØ­Øµ Ø£ÙˆÙ„ 20 Ø±Ø§Ø¨Ø· ÙÙ‚Ø· Ù„ØªÙˆÙÙŠØ± Ø§Ù„ÙˆÙ‚Øª
        try:
            response = requests.head(link, headers=headers, allow_redirects=True, timeout=10)
            if response.status_code >= 400:
                broken_links.append({
                    'url': link,
                    'status_code': response.status_code,
                    'error': f"HTTP {response.status_code}"
                })
        except requests.exceptions.RequestException as e:
            broken_links.append({
                'url': link,
                'status_code': 0,
                'error': str(e)
            })
        except Exception as e:
            broken_links.append({
                'url': link,
                'status_code': 0,
                'error': f"Unknown error: {str(e)}"
            })
    
    return broken_links

# =====================================================
# 7. ÙˆØ¸Ø§Ø¦Ù Ø£Ø®Ø° Ù„Ù‚Ø·Ø§Øª Ø§Ù„Ø´Ø§Ø´Ø© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
# =====================================================

async def take_enhanced_screenshot(page, page_folder, url):
    """Ø£Ø®Ø° Ù„Ù‚Ø·Ø§Øª Ø´Ø§Ø´Ø© Ù…ØªÙ‚Ø¯Ù…Ø© ÙˆÙ…ØªØ¹Ø¯Ø¯Ø©"""
    screenshots_taken = []
    
    try:
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ Ù„Ù‚Ø·Ø§Øª Ø§Ù„Ø´Ø§Ø´Ø©
        screenshots_folder = os.path.join(page_folder, "screenshots")
        os.makedirs(screenshots_folder, exist_ok=True)
        
        # Ø§Ù†ØªØ¸Ø§Ø± ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø© Ø¨Ø§Ù„ÙƒØ§Ù…Ù„
        await asyncio.sleep(config.screenshot_delay)
        
        # Ù„Ù‚Ø·Ø© Ø´Ø§Ø´Ø© ÙƒØ§Ù…Ù„Ø© Ù„Ù„ØµÙØ­Ø©
        full_screenshot_path = os.path.join(screenshots_folder, "full_page.png")
        await page.screenshot(
            path=full_screenshot_path, 
            full_page=True,
            type=config.screenshot_format
        )
        screenshots_taken.append(full_screenshot_path)
        
        # Ù„Ù‚Ø·Ø© Ø´Ø§Ø´Ø© Ù„Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø±Ø¦ÙŠ ÙÙ‚Ø·
        viewport_screenshot_path = os.path.join(screenshots_folder, "viewport.png")
        await page.screenshot(
            path=viewport_screenshot_path,
            full_page=False,
            type=config.screenshot_format
        )
        screenshots_taken.append(viewport_screenshot_path)
        
        # Ù„Ù‚Ø·Ø© Ø´Ø§Ø´Ø© Ù„Ù„Ù…ÙˆØ¨Ø§ÙŠÙ„ (Ø¹Ø±Ø¶ Ø¶ÙŠÙ‚)
        await page.set_viewport_size({'width': 375, 'height': 667})
        mobile_screenshot_path = os.path.join(screenshots_folder, "mobile_view.png")
        await page.screenshot(
            path=mobile_screenshot_path,
            full_page=True,
            type=config.screenshot_format
        )
        screenshots_taken.append(mobile_screenshot_path)
        
        # Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø­Ø¬Ù… Ø§Ù„Ø¹Ø±Ø¶ Ø§Ù„Ø£ØµÙ„ÙŠ
        await page.set_viewport_size({
            'width': config.viewport_width, 
            'height': config.viewport_height
        })
        
        # ØªØ­Ø³ÙŠÙ† Ø¬ÙˆØ¯Ø© Ø§Ù„ØµÙˆØ±
        await optimize_screenshots(screenshots_folder)
        
        screenshot_stats['successful_screenshots'] += 1
        screenshot_stats['total_screenshots'] += len(screenshots_taken)
        
        logger.info(f"âœ… ØªÙ… Ø£Ø®Ø° {len(screenshots_taken)} Ù„Ù‚Ø·Ø© Ø´Ø§Ø´Ø© Ù„Ù€ {url}")
        return screenshots_taken
        
    except Exception as e:
        logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ Ø£Ø®Ø° Ù„Ù‚Ø·Ø© Ø´Ø§Ø´Ø© Ù„Ù€ {url}: {e}")
        screenshot_stats['failed_screenshots'] += 1
        failed_urls.append({
            'url': url,
            'error': str(e),
            'timestamp': datetime.now().isoformat(),
            'error_type': 'screenshot_error'
        })
        return []

async def optimize_screenshots(screenshots_folder):
    """ØªØ­Ø³ÙŠÙ† Ø¬ÙˆØ¯Ø© ÙˆØ­Ø¬Ù… Ù„Ù‚Ø·Ø§Øª Ø§Ù„Ø´Ø§Ø´Ø©"""
    try:
        for filename in os.listdir(screenshots_folder):
            if filename.endswith('.png'):
                filepath = os.path.join(screenshots_folder, filename)
                
                # ÙØªØ­ Ø§Ù„ØµÙˆØ±Ø© ÙˆØªØ­Ø³ÙŠÙ†Ù‡Ø§
                with Image.open(filepath) as img:
                    # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ RGB Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±
                    if img.mode in ('RGBA', 'LA', 'P'):
                        background = Image.new('RGB', img.size, (255, 255, 255))
                        if img.mode == 'P':
                            img = img.convert('RGBA')
                        background.paste(img, mask=img.split()[-1] if img.mode == 'RGBA' else None)
                        img = background
                    
                    # Ø­ÙØ¸ Ø¨Ø¬ÙˆØ¯Ø© Ù…Ø­Ø³Ù†Ø©
                    optimized_path = os.path.join(screenshots_folder, f"opt_{filename}")
                    img.save(optimized_path, 'PNG', optimize=True, quality=config.screenshot_quality)
                    
                    # Ø¥Ù†Ø´Ø§Ø¡ thumbnail
                    img.thumbnail((300, 200), Image.Resampling.LANCZOS)
                    thumbnail_path = os.path.join(screenshots_folder, f"thumb_{filename}")
                    img.save(thumbnail_path, 'PNG')
                    
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ø³ÙŠÙ† Ù„Ù‚Ø·Ø§Øª Ø§Ù„Ø´Ø§Ø´Ø©: {e}")

# =====================================================
# 8. Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù„Ø²Ø­Ù ÙˆØ§Ù„ØªØ­Ù„ÙŠÙ„
# =====================================================

async def crawl(page, url, depth=0):
    """Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù„Ø²Ø­Ù ÙˆØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙØ­Ø§Øª"""
    
    # ÙØ­Øµ Ø´Ø±ÙˆØ· Ø§Ù„ØªÙˆÙ‚Ù
    if (depth > config.max_depth or 
        url in visited or 
        len(visited) >= config.max_pages):
        return
    
    visited.add(url)
    screenshot_stats['total_pages_processed'] += 1
    
    try:
        logger.info(f"ğŸ” Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙØ­Ø©: {url} (Ø§Ù„Ø¹Ù…Ù‚: {depth})")
        
        # Ø§Ù„Ø§Ù†ØªÙ‚Ø§Ù„ Ø¥Ù„Ù‰ Ø§Ù„ØµÙØ­Ø©
        response = await page.goto(
            url, 
            wait_until="networkidle", 
            timeout=config.timeout
        )
        
        if not response:
            raise Exception("ÙØ´Ù„ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø© - Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø§Ø³ØªØ¬Ø§Ø¨Ø©")
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ ÙˆØ§Ù„Ù‡ÙŠØ¯Ø±Ø²
        content = await page.content()
        headers = response.headers if response else {}
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ Ù„Ù„ØµÙØ­Ø©
        page_folder = os.path.join(config.output_root, sanitize_path(url))
        os.makedirs(page_folder, exist_ok=True)
        
        # Ø­ÙØ¸ HTML Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ÙØ¹Ù„
        if config.save_html:
            save_html(content, page_folder)
        
        # Ø£Ø®Ø° Ù„Ù‚Ø·Ø§Øª Ø§Ù„Ø´Ø§Ø´Ø©
        screenshots = await take_enhanced_screenshot(page, page_folder, url)
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… BeautifulSoup
        soup = BeautifulSoup(content, "html.parser")
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø£ØµÙˆÙ„ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ÙØ¹Ù„
        if config.save_assets:
            extracted_assets = extract_assets(soup, page_folder, url)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙˆØµÙÙŠØ©
        meta_info = get_meta_info(soup)
        with open(os.path.join(page_folder, "meta_info.json"), "w", encoding="utf-8") as f:
            json.dump(meta_info, f, ensure_ascii=False, indent=2)
        
        # ÙƒØ´Ù Ù†Ø¸Ø§Ù… Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ÙØ¹Ù„
        if config.analyze_cms:
            cms_detected = detect_cms(headers, soup)
            cms_info = {
                'detected_cms': cms_detected,
                'confidence': 'high' if len(cms_detected) == 1 else 'medium',
                'detection_time': datetime.now().isoformat()
            }
            with open(os.path.join(page_folder, "cms_info.json"), "w", encoding="utf-8") as f:
                json.dump(cms_info, f, ensure_ascii=False, indent=2)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù‡ÙŠÙƒÙ„ Ø§Ù„ØµÙØ­Ø©
        page_structure = extract_page_structure(soup)
        with open(os.path.join(page_folder, "page_structure.json"), "w", encoding="utf-8") as f:
            json.dump(page_structure, f, ensure_ascii=False, indent=2)
        
        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„ØµÙØ­Ø© Ø¥Ù„Ù‰ sitemap
        sitemap_urls.append(url)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ© Ù„Ù„Ø²Ø­Ù Ø§Ù„Ø¹Ù…ÙŠÙ‚
        internal_links = []
        if depth < config.max_depth:
            for a in soup.find_all("a", href=True):
                href = a.get('href')
                if href:
                    next_url = urljoin(config.base_url, href)
                    parsed_next = urlparse(next_url)
                    parsed_base = urlparse(config.base_url)
                    
                    # ÙÙ‚Ø· Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù…Ù† Ù†ÙØ³ Ø§Ù„Ù†Ø·Ø§Ù‚
                    if parsed_next.netloc == parsed_base.netloc:
                        internal_links.append(next_url)
        
        # ÙØ­Øµ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…Ø¹Ø·Ù„Ø© Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ÙØ¹Ù„
        if config.check_broken_links and internal_links:
            broken_links = check_links(internal_links[:10])  # ÙØ­Øµ Ø£ÙˆÙ„ 10 Ø±ÙˆØ§Ø¨Ø·
            if broken_links:
                with open(os.path.join(page_folder, "broken_links.json"), "w", encoding="utf-8") as f:
                    json.dump(broken_links, f, ensure_ascii=False, indent=2)
        
        # Ø­ÙØ¸ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø´Ø§Ù…Ù„Ø© Ø¹Ù† Ø§Ù„ØµÙØ­Ø©
        page_summary = {
            'url': url,
            'depth': depth,
            'processing_time': datetime.now().isoformat(),
            'status_code': response.status,
            'content_length': len(content),
            'screenshots_count': len(screenshots),
            'assets_extracted': len(extracted_assets) if config.save_assets else 0,
            'internal_links_found': len(internal_links),
            'cms_detected': cms_detected if config.analyze_cms else [],
            'meta_tags_count': len(meta_info),
            'headings_count': sum(len(headings) for headings in page_structure['headings'].values()),
            'images_count': len(page_structure['images']),
            'forms_count': len(page_structure['forms']),
            'word_count': page_structure['word_count']
        }
        
        with open(os.path.join(page_folder, "page_summary.json"), "w", encoding="utf-8") as f:
            json.dump(page_summary, f, ensure_ascii=False, indent=2)
        
        processed_links.append(page_summary)
        
        logger.info(f"âœ… ØªÙ…Øª Ù…Ø¹Ø§Ù„Ø¬Ø©: {url} | Ø§Ù„Ø¹Ù…Ù‚: {depth} | CMS: {cms_detected if config.analyze_cms else 'ØºÙŠØ± Ù…ÙØ¹Ù„'}")
        
        # Ø§Ù„Ø²Ø­Ù Ù„Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ÙØ±Ø¹ÙŠØ© (Ù…Ø­Ø¯ÙˆØ¯ Ø§Ù„Ø¹Ø¯Ø¯)
        for link in internal_links[:3]:  # Ø£ÙˆÙ„ 3 Ø±ÙˆØ§Ø¨Ø· ÙÙ‚Ø· Ù„ÙƒÙ„ ØµÙØ­Ø©
            if len(visited) < config.max_pages:
                await asyncio.sleep(config.delay_between_requests)  # ØªØ£Ø®ÙŠØ± Ø¨ÙŠÙ† Ø§Ù„Ø·Ù„Ø¨Ø§Øª
                await crawl(page, link, depth + 1)
            else:
                break
        
    except PlaywrightTimeoutError:
        error_msg = f"Ø§Ù†ØªÙ‡Øª Ù…Ù‡Ù„Ø© ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø©: {url}"
        logger.error(f"â° {error_msg}")
        failed_urls.append({
            'url': url,
            'error': 'timeout',
            'timestamp': datetime.now().isoformat(),
            'error_type': 'timeout_error'
        })
        
    except Exception as e:
        error_msg = f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© {url}: {str(e)}"
        logger.error(f"âŒ {error_msg}")
        failed_urls.append({
            'url': url,
            'error': str(e),
            'timestamp': datetime.now().isoformat(),
            'error_type': 'general_error'
        })

# =====================================================
# 9. ÙˆØ¸Ø§Ø¦Ù Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ± ÙˆØ§Ù„Ø®Ø±Ø§Ø¦Ø·
# =====================================================

def save_sitemap(urls, filename="sitemap.xml"):
    """Ø¥Ù†Ø´Ø§Ø¡ ÙˆØ­ÙØ¸ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ù…ÙˆÙ‚Ø¹ XML"""
    try:
        urlset = ET.Element("urlset", xmlns="http://www.sitemaps.org/schemas/sitemap/0.9")
        
        for url in urls:
            url_element = ET.SubElement(urlset, "url")
            loc = ET.SubElement(url_element, "loc")
            loc.text = url
            
            # Ø¥Ø¶Ø§ÙØ© ØªØ§Ø±ÙŠØ® Ø¢Ø®Ø± ØªØ¹Ø¯ÙŠÙ„
            lastmod = ET.SubElement(url_element, "lastmod")
            lastmod.text = datetime.now().strftime('%Y-%m-%d')
            
            # Ø¥Ø¶Ø§ÙØ© ØªÙƒØ±Ø§Ø± Ø§Ù„ØªØºÙŠÙŠØ±
            changefreq = ET.SubElement(url_element, "changefreq")
            changefreq.text = "weekly"
            
            # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ©
            priority = ET.SubElement(url_element, "priority")
            priority.text = "0.8"
        
        tree = ET.ElementTree(urlset)
        tree.write(filename, encoding="utf-8", xml_declaration=True)
        
        logger.info(f"ğŸ“„ ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ù…ÙˆÙ‚Ø¹: {filename}")
        
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ù…ÙˆÙ‚Ø¹: {e}")

async def generate_comprehensive_report():
    """Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ø´Ø§Ù…Ù„ ÙˆÙ…ÙØµÙ„"""
    try:
        screenshot_stats['end_time'] = datetime.now()
        if screenshot_stats['start_time']:
            duration = screenshot_stats['end_time'] - screenshot_stats['start_time']
            screenshot_stats['duration_seconds'] = duration.total_seconds()
        
        # ØªÙ‚Ø±ÙŠØ± Ù…ÙØµÙ„
        comprehensive_report = {
            'ØªÙ‚Ø±ÙŠØ±_Ø§Ù„ØªØ­Ù„ÙŠÙ„_Ø§Ù„Ø´Ø§Ù…Ù„': {
                'Ù…Ø¹Ù„ÙˆÙ…Ø§Øª_Ø¹Ø§Ù…Ø©': {
                    'Ø§Ù„Ù…ÙˆÙ‚Ø¹_Ø§Ù„Ù…Ø³ØªÙ‡Ø¯Ù': config.base_url,
                    'ÙˆÙ‚Øª_Ø§Ù„Ø¨Ø¯Ø¡': screenshot_stats['start_time'].isoformat() if screenshot_stats['start_time'] else None,
                    'ÙˆÙ‚Øª_Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡': screenshot_stats['end_time'].isoformat(),
                    'Ù…Ø¯Ø©_Ø§Ù„ØªØ­Ù„ÙŠÙ„_Ø¨Ø§Ù„Ø«ÙˆØ§Ù†ÙŠ': screenshot_stats['duration_seconds'],
                    'Ù…Ø¯Ø©_Ø§Ù„ØªØ­Ù„ÙŠÙ„_Ø¨Ø§Ù„Ø¯Ù‚Ø§Ø¦Ù‚': round(screenshot_stats['duration_seconds'] / 60, 2)
                },
                'Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª_Ø§Ù„ØµÙØ­Ø§Øª': {
                    'Ø¥Ø¬Ù…Ø§Ù„ÙŠ_Ø§Ù„ØµÙØ­Ø§Øª_Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©': screenshot_stats['total_pages_processed'],
                    'Ø§Ù„Ø±ÙˆØ§Ø¨Ø·_Ù…Ù†_Ø§Ù„Ù…Ù„Ù': screenshot_stats['links_from_file'],
                    'Ø§Ù„Ø±ÙˆØ§Ø¨Ø·_Ø§Ù„Ù…ÙƒØªØ´ÙØ©': len(visited) - screenshot_stats['links_from_file'],
                    'Ø§Ù„ØµÙØ­Ø§Øª_ÙÙŠ_sitemap': len(sitemap_urls),
                    'Ø§Ù„ØµÙØ­Ø§Øª_Ø§Ù„ÙØ§Ø´Ù„Ø©': len(failed_urls)
                },
                'Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª_Ù„Ù‚Ø·Ø§Øª_Ø§Ù„Ø´Ø§Ø´Ø©': {
                    'Ù„Ù‚Ø·Ø§Øª_Ø´Ø§Ø´Ø©_Ù†Ø§Ø¬Ø­Ø©': screenshot_stats['successful_screenshots'],
                    'Ù„Ù‚Ø·Ø§Øª_Ø´Ø§Ø´Ø©_ÙØ§Ø´Ù„Ø©': screenshot_stats['failed_screenshots'],
                    'Ø¥Ø¬Ù…Ø§Ù„ÙŠ_Ù„Ù‚Ø·Ø§Øª_Ø§Ù„Ø´Ø§Ø´Ø©': screenshot_stats['total_screenshots'],
                    'Ù…Ø¹Ø¯Ù„_Ù†Ø¬Ø§Ø­_Ù„Ù‚Ø·Ø§Øª_Ø§Ù„Ø´Ø§Ø´Ø©': f"{(screenshot_stats['successful_screenshots'] / max(screenshot_stats['total_pages_processed'], 1)) * 100:.2f}%"
                },
                'Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª_Ø§Ù„Ø£ØµÙˆÙ„': {
                    'Ø¥Ø¬Ù…Ø§Ù„ÙŠ_Ø§Ù„Ø£ØµÙˆÙ„_Ø§Ù„Ù…Ø­Ù…Ù„Ø©': screenshot_stats['total_assets_downloaded']
                },
                'Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª_Ø§Ù„ØªØ­Ù„ÙŠÙ„': {
                    'Ø§Ù„Ø­Ø¯_Ø§Ù„Ø£Ù‚ØµÙ‰_Ù„Ù„ØµÙØ­Ø§Øª': config.max_pages,
                    'Ø§Ù„Ø¹Ù…Ù‚_Ø§Ù„Ø£Ù‚ØµÙ‰': config.max_depth,
                    'Ø­ÙØ¸_HTML': config.save_html,
                    'Ø­ÙØ¸_Ø§Ù„Ø£ØµÙˆÙ„': config.save_assets,
                    'ØªØ­Ù„ÙŠÙ„_CMS': config.analyze_cms,
                    'ÙØ­Øµ_Ø§Ù„Ø±ÙˆØ§Ø¨Ø·_Ø§Ù„Ù…Ø¹Ø·Ù„Ø©': config.check_broken_links,
                    'Ø¥Ù†Ø´Ø§Ø¡_sitemap': config.generate_sitemap
                }
            },
            'ØªÙØ§ØµÙŠÙ„_Ø§Ù„ØµÙØ­Ø§Øª_Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©': processed_links,
            'Ø§Ù„Ø£Ø®Ø·Ø§Ø¡_ÙˆØ§Ù„Ù…Ø´Ø§ÙƒÙ„': {
                'Ø§Ù„Ø±ÙˆØ§Ø¨Ø·_Ø§Ù„ÙØ§Ø´Ù„Ø©': failed_urls,
                'Ø¥Ø¬Ù…Ø§Ù„ÙŠ_Ø§Ù„Ø£Ø®Ø·Ø§Ø¡': len(failed_urls)
            },
            'Ø§Ù„Ù…Ù„ÙØ§Øª_Ø§Ù„Ù…ÙÙ†Ø´Ø£Ø©': {
                'Ù…Ø¬Ù„Ø¯_Ø§Ù„Ù†ØªØ§Ø¦Ø¬_Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ': config.output_root,
                'Ù…Ø¬Ù„Ø¯_Ù„Ù‚Ø·Ø§Øª_Ø§Ù„Ø´Ø§Ø´Ø©': config.screenshots_root,
                'Ù…Ø¬Ù„Ø¯_HTML': os.path.join(config.output_root, "html_pages"),
                'Ù…Ø¬Ù„Ø¯_Ø§Ù„Ø£ØµÙˆÙ„': os.path.join(config.output_root, "assets"),
                'Ù…Ø¬Ù„Ø¯_Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª_Ø§Ù„ÙˆØµÙÙŠØ©': os.path.join(config.output_root, "metadata"),
                'Ø®Ø±ÙŠØ·Ø©_Ø§Ù„Ù…ÙˆÙ‚Ø¹': os.path.join(config.output_root, "sitemap.xml")
            }
        }
        
        # Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù…ÙØµÙ„
        report_file = os.path.join(config.output_root, "reports", "comprehensive_report.json")
        async with aiofiles.open(report_file, 'w', encoding='utf-8') as f:
            await f.write(json.dumps(comprehensive_report, ensure_ascii=False, indent=2))
        
        # Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ù†ØµÙŠ Ù…Ø¨Ø³Ø·
        text_report = f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    ØªÙ‚Ø±ÙŠØ± ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø´Ø§Ù…Ù„                    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸŒ Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ù…Ø­Ù„Ù„: {config.base_url}
ğŸ“… ØªØ§Ø±ÙŠØ® Ø§Ù„ØªØ­Ù„ÙŠÙ„: {screenshot_stats['end_time'].strftime('%Y-%m-%d %H:%M:%S')}
â±ï¸ Ù…Ø¯Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„: {round(screenshot_stats['duration_seconds'] / 60, 2)} Ø¯Ù‚ÙŠÙ‚Ø©

ğŸ“Š Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø¹Ø§Ù…Ø©:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âœ… Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨Ù†Ø¬Ø§Ø­: {screenshot_stats['total_pages_processed']}
ğŸ“„ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù…Ù† Ø§Ù„Ù…Ù„Ù: {screenshot_stats['links_from_file']}
ğŸ” Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…ÙƒØªØ´ÙØ©: {len(visited) - screenshot_stats['links_from_file']}
âŒ Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„ÙØ§Ø´Ù„Ø©: {len(failed_urls)}

ğŸ“¸ Ù„Ù‚Ø·Ø§Øª Ø§Ù„Ø´Ø§Ø´Ø©:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âœ… Ù„Ù‚Ø·Ø§Øª Ù†Ø§Ø¬Ø­Ø©: {screenshot_stats['successful_screenshots']}
âŒ Ù„Ù‚Ø·Ø§Øª ÙØ§Ø´Ù„Ø©: {screenshot_stats['failed_screenshots']}
ğŸ“Š Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù„Ù‚Ø·Ø§Øª: {screenshot_stats['total_screenshots']}
ğŸ“ˆ Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ø¬Ø§Ø­: {(screenshot_stats['successful_screenshots'] / max(screenshot_stats['total_pages_processed'], 1)) * 100:.2f}%

ğŸ’¾ Ø§Ù„Ø£ØµÙˆÙ„ Ø§Ù„Ù…Ø­Ù…Ù„Ø©:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“¦ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø£ØµÙˆÙ„: {screenshot_stats['total_assets_downloaded']}

ğŸ“ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙÙ†Ø´Ø£Ø©:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ—‚ï¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©: {config.output_root}
ğŸ“¸ Ù„Ù‚Ø·Ø§Øª Ø§Ù„Ø´Ø§Ø´Ø©: {config.screenshots_root}
ğŸŒ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ù…ÙˆÙ‚Ø¹: {os.path.join(config.output_root, "sitemap.xml")}
ğŸ“Š Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ± Ø§Ù„Ù…ÙØµÙ„Ø©: {os.path.join(config.output_root, "reports")}

âš™ï¸ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“„ Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„ØµÙØ­Ø§Øª: {config.max_pages}
ğŸ” Ø§Ù„Ø¹Ù…Ù‚ Ø§Ù„Ø£Ù‚ØµÙ‰: {config.max_depth}
ğŸ’¾ Ø­ÙØ¸ HTML: {'âœ…' if config.save_html else 'âŒ'}
ğŸ“¦ Ø­ÙØ¸ Ø§Ù„Ø£ØµÙˆÙ„: {'âœ…' if config.save_assets else 'âŒ'}
ğŸ”§ ØªØ­Ù„ÙŠÙ„ CMS: {'âœ…' if config.analyze_cms else 'âŒ'}
ğŸ”— ÙØ­Øµ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…Ø¹Ø·Ù„Ø©: {'âœ…' if config.check_broken_links else 'âŒ'}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ‰ ØªÙ… Ø¥Ù†Ø¬Ø§Ø² Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ù†Ø¬Ø§Ø­! Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù…Ø­ÙÙˆØ¸Ø© ÙÙŠ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©.
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""
        
        text_report_file = os.path.join(config.output_root, "reports", "summary_report.txt")
        async with aiofiles.open(text_report_file, 'w', encoding='utf-8') as f:
            await f.write(text_report)
        
        logger.info(f"ğŸ“Š ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø´Ø§Ù…Ù„: {report_file}")
        logger.info(f"ğŸ“„ ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ù„Ø®Øµ Ø§Ù„Ù†ØµÙŠ: {text_report_file}")
        
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø´Ø§Ù…Ù„: {e}")

# =====================================================
# 10. Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù„ØªØ´ØºÙŠÙ„
# =====================================================

async def main():
    """Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„"""
    
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                Ø£Ø¯Ø§Ø© ØªØ­Ù„ÙŠÙ„ ÙˆÙ„Ù‚Ø·Ø§Øª Ø§Ù„Ø´Ø§Ø´Ø© Ø§Ù„Ø´Ø§Ù…Ù„Ø©               â•‘
â•‘            Enhanced Website Screenshot & Analysis Tool       â•‘
â•‘                                                              â•‘
â•‘  ğŸ¯ ØªÙ‚Ø±Ø£ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù…Ù† Ù…Ù„Ù Ø±ÙˆØ§Ø¨Ø·_AKWAM_ØªØ±ØªÙŠØ¨_Ù‡Ø±Ù…ÙŠ_Ø´Ø§Ù…Ù„.txt      â•‘
â•‘  ğŸ“¸ ØªØ£Ø®Ø° Ù„Ù‚Ø·Ø§Øª Ø´Ø§Ø´Ø© Ù…ØªØ¹Ø¯Ø¯Ø© Ø¨ØµÙŠØºØ© PNG Ø¹Ø§Ù„ÙŠØ© Ø§Ù„Ø¬ÙˆØ¯Ø©           â•‘
â•‘  ğŸ” ØªØ­Ù„Ù„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ ÙˆØ§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙˆØµÙÙŠØ© ÙˆØ£Ù†Ø¸Ù…Ø© Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰      â•‘
â•‘  ğŸ’¾ ØªØ­ÙØ¸ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£ØµÙˆÙ„ ÙˆØ§Ù„Ù…Ù„ÙØ§Øª Ø¨Ø´ÙƒÙ„ Ù…Ù†Ø¸Ù…                    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    screenshot_stats['start_time'] = datetime.now()
    logger.info(f"ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„ Ù„Ù„Ù…ÙˆÙ‚Ø¹: {config.base_url}")
    
    try:
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù…Ù† Ø§Ù„Ù…Ù„Ù
        links_file_path = os.path.join("site_analysis_tools", config.links_file)
        links_to_process = load_links_from_file(links_file_path)
        
        if not links_to_process:
            logger.error("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø±ÙˆØ§Ø¨Ø· Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©!")
            return
        
        logger.info(f"ğŸ“‹ ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(links_to_process)} Ø±Ø§Ø¨Ø· Ù…Ù† Ø§Ù„Ù…Ù„Ù")
        
        # Ø¥Ø·Ù„Ø§Ù‚ Ø§Ù„Ù…ØªØµÙØ­
        async with async_playwright() as playwright:
            browser = await playwright.chromium.launch(
                headless=True,
                args=[
                    '--no-sandbox', 
                    '--disable-dev-shm-usage',
                    '--disable-blink-features=AutomationControlled',
                    '--disable-extensions'
                ]
            )
            
            # Ø¥Ù†Ø´Ø§Ø¡ ØµÙØ­Ø© Ø¬Ø¯ÙŠØ¯Ø©
            page = await browser.new_page()
            
            # ØªØ¹ÙŠÙŠÙ† user agent
            await page.set_extra_http_headers({
                'User-Agent': ua.random
            })
            
            # ØªØ¹ÙŠÙŠÙ† Ø­Ø¬Ù… Ø§Ù„Ø¹Ø±Ø¶
            await page.set_viewport_size({
                'width': config.viewport_width, 
                'height': config.viewport_height
            })
            
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù…Ù† Ø§Ù„Ù…Ù„Ù Ø£ÙˆÙ„Ø§Ù‹
            processed_count = 0
            for link in links_to_process:
                if processed_count >= config.max_pages:
                    logger.info(f"âš ï¸ ØªÙ… Ø§Ù„ÙˆØµÙˆÙ„ Ù„Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù…Ù† Ø§Ù„ØµÙØ­Ø§Øª: {config.max_pages}")
                    break
                
                await crawl(page, link, 0)
                processed_count += 1
                
                # ØªØ£Ø®ÙŠØ± Ù‚ØµÙŠØ± Ø¨ÙŠÙ† Ø§Ù„Ø·Ù„Ø¨Ø§Øª
                if processed_count % 10 == 0:
                    logger.info(f"ğŸ“Š ØªÙ…Øª Ù…Ø¹Ø§Ù„Ø¬Ø© {processed_count} ØµÙØ­Ø© Ù…Ù† Ø£ØµÙ„ {len(links_to_process)}")
                    await asyncio.sleep(2)  # Ø§Ø³ØªØ±Ø§Ø­Ø© Ù‚ØµÙŠØ±Ø© ÙƒÙ„ 10 ØµÙØ­Ø§Øª
            
            # Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ù…ØªØµÙØ­
            await browser.close()
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ÙØ¹Ù„
        if config.generate_sitemap:
            sitemap_path = os.path.join(config.output_root, "sitemap.xml")
            save_sitemap(sitemap_urls, sitemap_path)
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø´Ø§Ù…Ù„
        await generate_comprehensive_report()
        
        # Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
        duration = screenshot_stats['end_time'] - screenshot_stats['start_time']
        
        print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                        ğŸ‰ ØªÙ… Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡ Ù…Ù† Ø§Ù„ØªØ­Ù„ÙŠÙ„!             â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â±ï¸ Ø§Ù„Ù…Ø¯Ø© Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©: {duration.total_seconds():.2f} Ø«Ø§Ù†ÙŠØ© ({duration.total_seconds()/60:.2f} Ø¯Ù‚ÙŠÙ‚Ø©)
ğŸ“Š Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {screenshot_stats['total_pages_processed']}
ğŸ“¸ Ù„Ù‚Ø·Ø§Øª Ø§Ù„Ø´Ø§Ø´Ø© Ø§Ù„Ù†Ø§Ø¬Ø­Ø©: {screenshot_stats['successful_screenshots']}
âŒ Ù„Ù‚Ø·Ø§Øª Ø§Ù„Ø´Ø§Ø´Ø© Ø§Ù„ÙØ§Ø´Ù„Ø©: {screenshot_stats['failed_screenshots']}
ğŸ’¾ Ø§Ù„Ø£ØµÙˆÙ„ Ø§Ù„Ù…Ø­Ù…Ù„Ø©: {screenshot_stats['total_assets_downloaded']}
ğŸŒ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· ÙÙŠ Sitemap: {len(sitemap_urls)}

ğŸ“ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù…Ø­ÙÙˆØ¸Ø© ÙÙŠ: {config.output_root}
ğŸ“¸ Ù„Ù‚Ø·Ø§Øª Ø§Ù„Ø´Ø§Ø´Ø© ÙÙŠ: {config.screenshots_root}
ğŸ“Š Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ± Ø§Ù„Ù…ÙØµÙ„Ø© ÙÙŠ: {os.path.join(config.output_root, 'reports')}
        """)
        
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø£ Ø¹Ø§Ù… ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„: {e}")
        print(f"âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø¹Ø§Ù…: {e}")
        raise

def run_screenshot_tool():
    """ØªØ´ØºÙŠÙ„ Ø§Ù„Ø£Ø¯Ø§Ø© Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø®Ø·Ø§Ø¡"""
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nâš ï¸ ØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…")
        logger.info("ØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…")
    except Exception as e:
        print(f"\nâŒ Ø®Ø·Ø£ ÙÙŠ ØªØ´ØºÙŠÙ„ Ø§Ù„Ø£Ø¯Ø§Ø©: {e}")
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ´ØºÙŠÙ„ Ø§Ù„Ø£Ø¯Ø§Ø©: {e}")

# =====================================================
# 11. ØªØ´ØºÙŠÙ„ Ø§Ù„Ø£Ø¯Ø§Ø©
# =====================================================

if __name__ == "__main__":
    run_screenshot_tool()