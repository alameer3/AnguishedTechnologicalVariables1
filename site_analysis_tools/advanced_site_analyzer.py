#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Ø£Ø¯Ø§Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ØªØ·ÙˆØ±Ø© Ø§Ù„Ø´Ø§Ù…Ù„Ø© Ù„Ù…ÙˆÙ‚Ø¹ AKWAM
ØªØ­Ù„ÙŠÙ„ Ø¹Ù…ÙŠÙ‚ ÙˆÙ…ØªÙ‚Ø¯Ù… Ù„ÙÙ‡Ù… ÙƒÙ„ Ø´ÙŠØ¡ ÙÙŠ Ø§Ù„Ù…ÙˆÙ‚Ø¹
"""

import requests
import re
import json
import time
from datetime import datetime
from bs4 import BeautifulSoup
from urllib.parse import unquote, urljoin, urlparse
from collections import defaultdict, Counter
import trafilatura
import lxml.html
from dataclasses import dataclass
from typing import Dict, List, Any, Optional

@dataclass
class ContentMetadata:
    id: str
    title: str
    url: str
    content_type: str
    language: str
    year: Optional[str] = None
    quality: Optional[str] = None
    genre: Optional[str] = None
    rating: Optional[str] = None
    episode_number: Optional[str] = None
    season_number: Optional[str] = None
    series_name: Optional[str] = None

class AdvancedSiteAnalyzer:
    def __init__(self):
        self.session = self.setup_session()
        self.analysis_results = {
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'site_structure': {},
            'content_catalog': {},
            'technical_analysis': {},
            'user_experience': {},
            'security_analysis': {},
            'performance_metrics': {},
            'content_metadata': [],
            'link_patterns': {},
            'discovery_insights': {}
        }
        self.processed_urls = set()
        self.content_database = []
        
    def setup_session(self):
        """Ø¥Ø¹Ø¯Ø§Ø¯ Ø¬Ù„Ø³Ø© HTTP Ù…Ø­Ø³Ù†Ø©"""
        session = requests.Session()
        session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ar,en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        return session
    
    def load_links_from_file(self, filename='site_links.txt'):
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù…Ù† Ø§Ù„Ù…Ù„Ù"""
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·
            links = re.findall(r'https://ak\.sv[^\s]*', content)
            unique_links = list(set(links))
            
            print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(unique_links)} Ø±Ø§Ø¨Ø· ÙØ±ÙŠØ¯ Ù…Ù† Ø£ØµÙ„ {len(links)} Ø±Ø§Ø¨Ø·")
            return unique_links
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù: {e}")
            return []
    
    def analyze_url_patterns(self, links):
        """ØªØ­Ù„ÙŠÙ„ Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø±ÙˆØ§Ø¨Ø·"""
        patterns = {
            'movies': [],
            'series': [],
            'episodes': [],
            'shows': [],
            'mix': [],
            'admin_pages': [],
            'hash_links': [],
            'main_pages': [],
            'other': []
        }
        
        id_ranges = {
            'movies': {'min': float('inf'), 'max': 0, 'ids': []},
            'series': {'min': float('inf'), 'max': 0, 'ids': []},
            'episodes': {'min': float('inf'), 'max': 0, 'ids': []},
            'shows': {'min': float('inf'), 'max': 0, 'ids': []},
            'mix': {'min': float('inf'), 'max': 0, 'ids': []}
        }
        
        for link in links:
            decoded_link = unquote(link)
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£ÙÙ„Ø§Ù…
            if '/movie/' in link:
                match = re.search(r'/movie/(\d+)/', link)
                if match:
                    movie_id = int(match.group(1))
                    patterns['movies'].append(decoded_link)
                    id_ranges['movies']['ids'].append(movie_id)
                    id_ranges['movies']['min'] = min(id_ranges['movies']['min'], movie_id)
                    id_ranges['movies']['max'] = max(id_ranges['movies']['max'], movie_id)
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø³Ù„Ø³Ù„Ø§Øª
            elif '/series/' in link:
                match = re.search(r'/series/(\d+)/', link)
                if match:
                    series_id = int(match.group(1))
                    patterns['series'].append(decoded_link)
                    id_ranges['series']['ids'].append(series_id)
                    id_ranges['series']['min'] = min(id_ranges['series']['min'], series_id)
                    id_ranges['series']['max'] = max(id_ranges['series']['max'], series_id)
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø­Ù„Ù‚Ø§Øª
            elif '/episode/' in link:
                match = re.search(r'/episode/(\d+)/', link)
                if match:
                    episode_id = int(match.group(1))
                    patterns['episodes'].append(decoded_link)
                    id_ranges['episodes']['ids'].append(episode_id)
                    id_ranges['episodes']['min'] = min(id_ranges['episodes']['min'], episode_id)
                    id_ranges['episodes']['max'] = max(id_ranges['episodes']['max'], episode_id)
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ø±ÙˆØ¶
            elif '/shows/' in link:
                match = re.search(r'/shows/(\d+)/', link)
                if match:
                    show_id = int(match.group(1))
                    patterns['shows'].append(decoded_link)
                    id_ranges['shows']['ids'].append(show_id)
                    id_ranges['shows']['min'] = min(id_ranges['shows']['min'], show_id)
                    id_ranges['shows']['max'] = max(id_ranges['shows']['max'], show_id)
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù†ÙˆØ¹Ø§Øª
            elif '/mix/' in link:
                match = re.search(r'/mix/(\d+)/', link)
                if match:
                    mix_id = int(match.group(1))
                    patterns['mix'].append(decoded_link)
                    id_ranges['mix']['ids'].append(mix_id)
                    id_ranges['mix']['min'] = min(id_ranges['mix']['min'], mix_id)
                    id_ranges['mix']['max'] = max(id_ranges['mix']['max'], mix_id)
            
            # Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„Ø¥Ø¯Ø§Ø±ÙŠØ©
            elif any(admin in link for admin in ['/ad-policy', '/contactus', '/dmca', '/AKWAM-Notifications']):
                patterns['admin_pages'].append(decoded_link)
            
            # Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠØ©
            elif '/#' in link:
                patterns['hash_links'].append(decoded_link)
            
            # Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
            elif link in ['https://ak.sv/', 'https://ak.sv/ones', 'https://ak.sv/movies', 'https://ak.sv/series', 'https://ak.sv/shows', 'https://ak.sv/mix']:
                patterns['main_pages'].append(decoded_link)
            
            else:
                patterns['other'].append(decoded_link)
        
        # ØªÙ†Ø¸ÙŠÙ id_ranges Ù…Ù† Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù„Ø§Ù†Ù‡Ø§Ø¦ÙŠØ©
        for content_type in id_ranges:
            if id_ranges[content_type]['min'] == float('inf'):
                id_ranges[content_type]['min'] = 0
        
        return patterns, id_ranges
    
    def extract_content_metadata(self, link, content_type):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙˆØµÙÙŠØ© Ù„Ù„Ù…Ø­ØªÙˆÙ‰"""
        metadata = None
        
        try:
            if content_type == 'movie':
                match = re.search(r'/movie/(\d+)/([^?]*)', link)
                if match:
                    movie_id, movie_name = match.groups()
                    metadata = ContentMetadata(
                        id=movie_id,
                        title=unquote(movie_name).replace('-', ' '),
                        url=link,
                        content_type='movie',
                        language=self.detect_language(movie_name)
                    )
            
            elif content_type == 'series':
                match = re.search(r'/series/(\d+)/([^?]*)', link)
                if match:
                    series_id, series_name = match.groups()
                    # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ÙˆØ³Ù…
                    season_match = re.search(r'Ø§Ù„Ù…ÙˆØ³Ù…-(\w+)', series_name)
                    season_number = season_match.group(1) if season_match else None
                    
                    metadata = ContentMetadata(
                        id=series_id,
                        title=unquote(series_name).replace('-', ' '),
                        url=link,
                        content_type='series',
                        language=self.detect_language(series_name),
                        season_number=season_number
                    )
            
            elif content_type == 'episode':
                match = re.search(r'/episode/(\d+)/([^/]+)/([^?]*)', link)
                if match:
                    episode_id, series_name, episode_name = match.groups()
                    
                    # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø±Ù‚Ù… Ø§Ù„Ø­Ù„Ù‚Ø©
                    episode_match = re.search(r'Ø§Ù„Ø­Ù„Ù‚Ø©-(\d+)', episode_name)
                    episode_number = episode_match.group(1) if episode_match else None
                    
                    metadata = ContentMetadata(
                        id=episode_id,
                        title=unquote(episode_name).replace('-', ' '),
                        url=link,
                        content_type='episode',
                        language=self.detect_language(series_name),
                        series_name=unquote(series_name).replace('-', ' '),
                        episode_number=episode_number
                    )
            
        except Exception as e:
            print(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙˆØµÙÙŠØ©: {e}")
        
        return metadata
    
    def detect_language(self, text):
        """ÙƒØ´Ù Ø§Ù„Ù„ØºØ© Ù…Ù† Ø§Ù„Ù†Øµ"""
        arabic_chars = len(re.findall(r'[\u0600-\u06FF]', text))
        total_chars = len(re.sub(r'[^a-zA-Z\u0600-\u06FF]', '', text))
        
        if total_chars == 0:
            return 'unknown'
        
        arabic_ratio = arabic_chars / total_chars
        
        if arabic_ratio > 0.7:
            return 'arabic'
        elif arabic_ratio > 0.3:
            return 'mixed'
        else:
            return 'english'
    
    def analyze_site_structure(self, base_url='https://ak.sv'):
        """ØªØ­Ù„ÙŠÙ„ Ø¨Ù†ÙŠØ© Ø§Ù„Ù…ÙˆÙ‚Ø¹"""
        print("ğŸ—ï¸ ØªØ­Ù„ÙŠÙ„ Ø¨Ù†ÙŠØ© Ø§Ù„Ù…ÙˆÙ‚Ø¹...")
        
        try:
            response = self.session.get(base_url, timeout=15)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù‚ÙˆØ§Ø¦Ù… ÙˆØ§Ù„ØªÙ†Ù‚Ù„
                navigation = self.extract_navigation_structure(soup)
                
                # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„ØªÙ‚Ù†ÙŠØ©
                technical_structure = self.analyze_technical_structure(response.text, soup)
                
                # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
                main_content = self.analyze_main_content_structure(soup)
                
                self.analysis_results['site_structure'] = {
                    'navigation': navigation,
                    'technical': technical_structure,
                    'content_layout': main_content,
                    'page_load_time': response.elapsed.total_seconds()
                }
                
                print("âœ… ØªÙ… ØªØ­Ù„ÙŠÙ„ Ø¨Ù†ÙŠØ© Ø§Ù„Ù…ÙˆÙ‚Ø¹")
                
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø¨Ù†ÙŠØ© Ø§Ù„Ù…ÙˆÙ‚Ø¹: {e}")
    
    def extract_navigation_structure(self, soup):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ù†ÙŠØ© Ø§Ù„ØªÙ†Ù‚Ù„"""
        navigation = {
            'main_menu': [],
            'footer_links': [],
            'breadcrumbs': [],
            'category_links': []
        }
        
        # Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
        nav_elements = soup.find_all(['nav', 'ul', 'div'], class_=re.compile(r'nav|menu|header', re.I))
        for nav in nav_elements:
            links = nav.find_all('a')
            for link in links[:20]:
                href = link.get('href', '')
                text = link.get_text(strip=True)
                if href and text:
                    navigation['main_menu'].append({
                        'text': text,
                        'url': href,
                        'is_internal': href.startswith('/') or 'ak.sv' in href
                    })
        
        # Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ØªØ°ÙŠÙŠÙ„
        footer = soup.find('footer') or soup.find('div', class_=re.compile(r'footer', re.I))
        if footer:
            footer_links = footer.find_all('a')
            for link in footer_links:
                href = link.get('href', '')
                text = link.get_text(strip=True)
                if href and text:
                    navigation['footer_links'].append({
                        'text': text,
                        'url': href
                    })
        
        return navigation
    
    def analyze_technical_structure(self, html, soup):
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„ØªÙ‚Ù†ÙŠØ© Ø§Ù„Ø´Ø§Ù…Ù„"""
        technical = {
            'meta_tags': {},
            'scripts': [],
            'stylesheets': [],
            'external_resources': [],
            'encoding': 'utf-8',
            'doctype': 'HTML5' if '<!DOCTYPE html>' in html.upper() else 'Other',
            'html_structure': {},
            'all_tags': {},
            'attributes_analysis': {},
            'text_content': {},
            'hidden_elements': {},
            'interactive_elements': {},
            'multimedia_elements': {},
            'seo_elements': {},
            'security_elements': {}
        }
        
        # ØªØ­Ù„ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¹Ù„Ø§Ù…Ø§Øª ÙÙŠ Ø§Ù„ØµÙØ­Ø©
        all_tags = soup.find_all()
        tag_counter = Counter([tag.name for tag in all_tags])
        technical['all_tags'] = dict(tag_counter)
        
        # Meta tags ØªØ­Ù„ÙŠÙ„ Ù…ÙØµÙ„
        meta_tags = soup.find_all('meta')
        for meta in meta_tags:
            name = meta.get('name') or meta.get('property') or meta.get('http-equiv')
            content = meta.get('content')
            if name and content:
                technical['meta_tags'][name] = content
        
        # Scripts ØªØ­Ù„ÙŠÙ„ Ù…ÙØµÙ„
        scripts = soup.find_all('script')
        for script in scripts:
            script_info = {
                'src': script.get('src', ''),
                'type': script.get('type', 'text/javascript'),
                'content_length': len(script.get_text()) if script.get_text() else 0,
                'has_inline_code': bool(script.get_text().strip()),
                'is_external': bool(script.get('src')),
                'async': script.has_attr('async'),
                'defer': script.has_attr('defer')
            }
            if script_info['src']:
                script_info['is_external_domain'] = not (script_info['src'].startswith('/') or 'ak.sv' in script_info['src'])
            technical['scripts'].append(script_info)
        
        # Stylesheets ØªØ­Ù„ÙŠÙ„ Ù…ÙØµÙ„
        links = soup.find_all('link')
        for link in links:
            if link.get('rel') == ['stylesheet'] or 'stylesheet' in str(link.get('rel', [])):
                style_info = {
                    'href': link.get('href', ''),
                    'media': link.get('media', 'all'),
                    'type': link.get('type', 'text/css'),
                    'is_external': not (link.get('href', '').startswith('/') or 'ak.sv' in link.get('href', ''))
                }
                technical['stylesheets'].append(style_info)
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠØ©
        forms = soup.find_all('form')
        buttons = soup.find_all('button')
        inputs = soup.find_all('input')
        selects = soup.find_all('select')
        textareas = soup.find_all('textarea')
        
        technical['interactive_elements'] = {
            'forms': len(forms),
            'buttons': len(buttons),
            'inputs': len(inputs),
            'selects': len(selects),
            'textareas': len(textareas),
            'form_details': [
                {
                    'method': form.get('method', 'get'),
                    'action': form.get('action', ''),
                    'enctype': form.get('enctype', 'application/x-www-form-urlencoded')
                } for form in forms
            ],
            'input_types': Counter([inp.get('type', 'text') for inp in inputs])
        }
        
        # ØªØ­Ù„ÙŠÙ„ Ø¹Ù†Ø§ØµØ± Ø§Ù„ÙˆØ³Ø§Ø¦Ø· Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©
        images = soup.find_all('img')
        videos = soup.find_all('video')
        audios = soup.find_all('audio')
        iframes = soup.find_all('iframe')
        objects = soup.find_all('object')
        embeds = soup.find_all('embed')
        
        technical['multimedia_elements'] = {
            'images': len(images),
            'videos': len(videos),
            'audios': len(audios),
            'iframes': len(iframes),
            'objects': len(objects),
            'embeds': len(embeds),
            'image_formats': Counter([img.get('src', '').split('.')[-1].lower() for img in images if img.get('src') and '.' in img.get('src')]),
            'lazy_loading': len([img for img in images if img.get('loading') == 'lazy']),
            'alt_texts': len([img for img in images if img.get('alt')])
        }
        
        # ØªØ­Ù„ÙŠÙ„ Ø¹Ù†Ø§ØµØ± SEO
        title = soup.find('title')
        h_tags = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
        canonical = soup.find('link', rel='canonical')
        
        technical['seo_elements'] = {
            'title': title.get_text() if title else '',
            'title_length': len(title.get_text()) if title else 0,
            'heading_tags': Counter([h.name for h in h_tags]),
            'total_headings': len(h_tags),
            'canonical_url': canonical.get('href') if canonical else '',
            'meta_description': technical['meta_tags'].get('description', ''),
            'meta_keywords': technical['meta_tags'].get('keywords', ''),
            'og_tags': {k: v for k, v in technical['meta_tags'].items() if k.startswith('og:')},
            'twitter_tags': {k: v for k, v in technical['meta_tags'].items() if k.startswith('twitter:')},
            'robots': technical['meta_tags'].get('robots', '')
        }
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù…Ø®ÙÙŠØ© ÙˆØ§Ù„Ù…Ø´Ø±ÙˆØ·Ø©
        hidden_elements = soup.find_all(attrs={'style': re.compile(r'display:\s*none|visibility:\s*hidden', re.I)})
        comments = soup.find_all(string=lambda text: isinstance(text, type(soup.new_string(''))) and text.parent.name == '[document]')
        
        technical['hidden_elements'] = {
            'hidden_by_style': len(hidden_elements),
            'html_comments': len([c for c in soup.contents if str(type(c)) == "<class 'bs4.Comment'>"]),
            'conditional_comments': len(re.findall(r'<!--\[if.*?\]>.*?<!\[endif\]-->', html, re.DOTALL)),
            'noscript_tags': len(soup.find_all('noscript'))
        }
        
        # ØªØ­Ù„ÙŠÙ„ Ø¹Ù†Ø§ØµØ± Ø§Ù„Ø£Ù…Ø§Ù†
        technical['security_elements'] = {
            'csrf_tokens': len(soup.find_all('input', {'name': re.compile(r'csrf|token', re.I)})),
            'https_links': len([a for a in soup.find_all('a', href=True) if a['href'].startswith('https://')]),
            'http_links': len([a for a in soup.find_all('a', href=True) if a['href'].startswith('http://')]),
            'external_links': len([a for a in soup.find_all('a', href=True) if not a['href'].startswith('/') and 'ak.sv' not in a['href'] and a['href'].startswith('http')]),
            'email_links': len([a for a in soup.find_all('a', href=True) if a['href'].startswith('mailto:')]),
            'tel_links': len([a for a in soup.find_all('a', href=True) if a['href'].startswith('tel:')])
        }
        
        # ØªØ­Ù„ÙŠÙ„ Ø¨Ù†ÙŠØ© HTML
        html_structure = soup.find('html')
        head = soup.find('head')
        body = soup.find('body')
        
        technical['html_structure'] = {
            'html_lang': html_structure.get('lang', '') if html_structure else '',
            'html_dir': html_structure.get('dir', '') if html_structure else '',
            'head_elements': len(head.find_all()) if head else 0,
            'body_elements': len(body.find_all()) if body else 0,
            'total_elements': len(all_tags),
            'max_nesting_depth': self.calculate_max_depth(soup) if soup else 0
        }
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ ÙˆØ§Ù„Ù…Ø­ØªÙˆÙ‰
        all_text = soup.get_text()
        technical['text_content'] = {
            'total_characters': len(all_text),
            'total_words': len(all_text.split()),
            'paragraphs': len(soup.find_all('p')),
            'lists': len(soup.find_all(['ul', 'ol'])),
            'list_items': len(soup.find_all('li')),
            'tables': len(soup.find_all('table')),
            'table_rows': len(soup.find_all('tr')),
            'table_cells': len(soup.find_all(['td', 'th']))
        }
        
        return technical
    
    def calculate_max_depth(self, element, current_depth=0):
        """Ø­Ø³Ø§Ø¨ Ø£Ù‚ØµÙ‰ Ø¹Ù…Ù‚ Ù„Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù…ØªØ¯Ø§Ø®Ù„Ø©"""
        if not element.children:
            return current_depth
        
        max_child_depth = current_depth
        for child in element.children:
            if hasattr(child, 'children'):
                child_depth = self.calculate_max_depth(child, current_depth + 1)
                max_child_depth = max(max_child_depth, child_depth)
        
        return max_child_depth
    
    def analyze_main_content_structure(self, soup):
        """ØªØ­Ù„ÙŠÙ„ Ø¨Ù†ÙŠØ© Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ"""
        content_structure = {
            'content_sections': [],
            'media_elements': {},
            'interactive_elements': {},
            'layout_type': 'unknown'
        }
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£Ù‚Ø³Ø§Ù… Ø§Ù„Ù…Ø­ØªÙˆÙ‰
        main_content = soup.find('main') or soup.find('div', class_=re.compile(r'main|content|container', re.I))
        
        if main_content:
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ù‚Ø³Ø§Ù…
            sections = main_content.find_all(['section', 'div', 'article'], class_=re.compile(r'section|block|item|card', re.I))
            for section in sections[:10]:
                section_info = {
                    'tag': section.name,
                    'class': section.get('class', []),
                    'content_preview': section.get_text(strip=True)[:200]
                }
                content_structure['content_sections'].append(section_info)
        
        # Ø¹Ù†Ø§ØµØ± Ø§Ù„ÙˆØ³Ø§Ø¦Ø·
        images = soup.find_all('img')
        videos = soup.find_all('video')
        iframes = soup.find_all('iframe')
        
        content_structure['media_elements'] = {
            'images_count': len(images),
            'videos_count': len(videos),
            'iframes_count': len(iframes),
            'has_lazy_loading': any('lazy' in img.get('loading', '') for img in images)
        }
        
        # Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠØ©
        forms = soup.find_all('form')
        buttons = soup.find_all('button')
        inputs = soup.find_all('input')
        
        content_structure['interactive_elements'] = {
            'forms_count': len(forms),
            'buttons_count': len(buttons),
            'inputs_count': len(inputs),
            'has_search': any('search' in input.get('type', '') or 'search' in input.get('class', []) for input in inputs)
        }
        
        return content_structure
    
    def deep_content_analysis(self, links):
        """ØªØ­Ù„ÙŠÙ„ Ø¹Ù…ÙŠÙ‚ Ù„Ù„Ù…Ø­ØªÙˆÙ‰"""
        print("ğŸ” Ø¨Ø¯Ø¡ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù…ÙŠÙ‚ Ù„Ù„Ù…Ø­ØªÙˆÙ‰...")
        
        content_analysis = {
            'movies': {'count': 0, 'samples': [], 'metadata': []},
            'series': {'count': 0, 'samples': [], 'metadata': []},
            'episodes': {'count': 0, 'samples': [], 'metadata': []},
            'shows': {'count': 0, 'samples': [], 'metadata': []},
            'mix': {'count': 0, 'samples': [], 'metadata': []}
        }
        
        # ØªØ­Ù„ÙŠÙ„ Ø¹ÙŠÙ†Ø§Øª Ù…Ù† ÙƒÙ„ Ù†ÙˆØ¹ Ù…Ø­ØªÙˆÙ‰
        for link in links[:50]:  # ØªØ­Ù„ÙŠÙ„ Ø£ÙˆÙ„ 50 Ø±Ø§Ø¨Ø· ÙƒØ¹ÙŠÙ†Ø©
            try:
                if '/movie/' in link:
                    metadata = self.extract_content_metadata(link, 'movie')
                    if metadata:
                        content_analysis['movies']['metadata'].append(metadata)
                        content_analysis['movies']['count'] += 1
                
                elif '/series/' in link:
                    metadata = self.extract_content_metadata(link, 'series')
                    if metadata:
                        content_analysis['series']['metadata'].append(metadata)
                        content_analysis['series']['count'] += 1
                
                elif '/episode/' in link:
                    metadata = self.extract_content_metadata(link, 'episode')
                    if metadata:
                        content_analysis['episodes']['metadata'].append(metadata)
                        content_analysis['episodes']['count'] += 1
                
            except Exception as e:
                print(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø±Ø§Ø¨Ø· {link}: {e}")
        
        self.analysis_results['content_catalog'] = content_analysis
        print(f"âœ… ØªÙ… ØªØ­Ù„ÙŠÙ„ {sum(cat['count'] for cat in content_analysis.values())} Ø¹Ù†ØµØ± Ù…Ø­ØªÙˆÙ‰")
    
    def analyze_content_trends(self, metadata_list):
        """ØªØ­Ù„ÙŠÙ„ Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ù…Ø­ØªÙˆÙ‰"""
        trends = {
            'language_distribution': Counter(),
            'content_type_distribution': Counter(),
            'popular_keywords': Counter(),
            'naming_patterns': []
        }
        
        for metadata in metadata_list:
            if hasattr(metadata, 'language'):
                trends['language_distribution'][metadata.language] += 1
            if hasattr(metadata, 'content_type'):
                trends['content_type_distribution'][metadata.content_type] += 1
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ù…Ù† Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ†
            if hasattr(metadata, 'title'):
                words = re.findall(r'\w+', metadata.title.lower())
                for word in words:
                    if len(word) > 3:  # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù‚ØµÙŠØ±Ø©
                        trends['popular_keywords'][word] += 1
        
        return trends
    
    def perform_comprehensive_analysis(self):
        """ØªÙ†ÙÙŠØ° Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„"""
        print("ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„ Ø§Ù„Ù…ØªØ·ÙˆØ±...")
        
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·
        links = self.load_links_from_file()
        if not links:
            print("âŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø±ÙˆØ§Ø¨Ø· Ù„Ù„ØªØ­Ù„ÙŠÙ„")
            return
        
        # ØªØ­Ù„ÙŠÙ„ Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø±ÙˆØ§Ø¨Ø·
        patterns, id_ranges = self.analyze_url_patterns(links)
        self.analysis_results['link_patterns'] = {
            'patterns': patterns,
            'id_ranges': id_ranges,
            'total_links': len(links),
            'pattern_distribution': {k: len(v) for k, v in patterns.items()}
        }
        
        # ØªØ­Ù„ÙŠÙ„ Ø¨Ù†ÙŠØ© Ø§Ù„Ù…ÙˆÙ‚Ø¹
        self.analyze_site_structure()
        
        # Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù…ÙŠÙ‚ Ù„Ù„Ù…Ø­ØªÙˆÙ‰
        self.deep_content_analysis(links)
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª
        all_metadata = []
        for category in self.analysis_results['content_catalog'].values():
            all_metadata.extend(category.get('metadata', []))
        
        trends = self.analyze_content_trends(all_metadata)
        self.analysis_results['discovery_insights'] = trends
        
        # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        self.save_analysis_results()
        
        print("âœ… ØªÙ… Ø¥ÙƒÙ…Ø§Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„ Ø§Ù„Ù…ØªØ·ÙˆØ±")
    
    def save_analysis_results(self):
        """Ø­ÙØ¸ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ­Ù„ÙŠÙ„"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # Ø­ÙØ¸ JSON Ù…ÙØµÙ„
        json_filename = f'advanced_analysis_{timestamp}.json'
        with open(json_filename, 'w', encoding='utf-8') as f:
            # ØªØ­ÙˆÙŠÙ„ ContentMetadata objects Ø¥Ù„Ù‰ dictionaries
            json_data = self.convert_analysis_to_json()
            json.dump(json_data, f, ensure_ascii=False, indent=2)
        
        # Ø­ÙØ¸ ØªÙ‚Ø±ÙŠØ± Ù†ØµÙŠ Ù…Ù‚Ø±ÙˆØ¡
        report_filename = f'ØªÙ‚Ø±ÙŠØ±_Ø§Ù„ØªØ­Ù„ÙŠÙ„_Ø§Ù„Ù…ØªØ·ÙˆØ±_{timestamp}.md'
        self.generate_readable_report(report_filename)
        
        print(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ:")
        print(f"   - {json_filename}")
        print(f"   - {report_filename}")
    
    def convert_analysis_to_json(self):
        """ØªØ­ÙˆÙŠÙ„ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¥Ù„Ù‰ JSON"""
        json_data = dict(self.analysis_results)
        
        # ØªØ­ÙˆÙŠÙ„ ContentMetadata objects
        for category in json_data.get('content_catalog', {}).values():
            if 'metadata' in category:
                category['metadata'] = [
                    {
                        'id': m.id,
                        'title': m.title,
                        'url': m.url,
                        'content_type': m.content_type,
                        'language': m.language,
                        'year': m.year,
                        'quality': m.quality,
                        'genre': m.genre,
                        'rating': m.rating,
                        'episode_number': m.episode_number,
                        'season_number': m.season_number,
                        'series_name': m.series_name
                    } for m in category['metadata']
                ]
        
        return json_data
    
    def generate_readable_report(self, filename):
        """Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ù†ØµÙŠ Ù…Ù‚Ø±ÙˆØ¡"""
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(f"""# ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ØªØ·ÙˆØ± Ø§Ù„Ø´Ø§Ù…Ù„ Ù„Ù…ÙˆÙ‚Ø¹ AKWAM

**ØªØ§Ø±ÙŠØ® Ø§Ù„ØªØ­Ù„ÙŠÙ„:** {self.analysis_results['timestamp']}

## ğŸ“Š Ù†Ø¸Ø±Ø© Ø¹Ø§Ù…Ø© Ø¹Ù„Ù‰ Ø§Ù„Ù†ØªØ§Ø¦Ø¬

### Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…Ø­Ù„Ù„Ø©
- **Ø§Ù„Ø¹Ø¯Ø¯ Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ:** {self.analysis_results.get('link_patterns', {}).get('total_links', 0):,}
- **Ø§Ù„Ø£ÙÙ„Ø§Ù…:** {len(self.analysis_results.get('link_patterns', {}).get('patterns', {}).get('movies', [])):,}
- **Ø§Ù„Ù…Ø³Ù„Ø³Ù„Ø§Øª:** {len(self.analysis_results.get('link_patterns', {}).get('patterns', {}).get('series', [])):,}
- **Ø§Ù„Ø­Ù„Ù‚Ø§Øª:** {len(self.analysis_results.get('link_patterns', {}).get('patterns', {}).get('episodes', [])):,}
- **Ø§Ù„Ø¹Ø±ÙˆØ¶ Ø§Ù„ØªÙ„ÙØ²ÙŠÙˆÙ†ÙŠØ©:** {len(self.analysis_results.get('link_patterns', {}).get('patterns', {}).get('shows', [])):,}
- **Ø§Ù„Ù…Ù†ÙˆØ¹Ø§Øª:** {len(self.analysis_results.get('link_patterns', {}).get('patterns', {}).get('mix', [])):,}

## ğŸ”¢ ØªØ­Ù„ÙŠÙ„ Ù…Ø¹Ø±ÙØ§Øª Ø§Ù„Ù…Ø­ØªÙˆÙ‰

""")
            
            # ÙƒØªØ§Ø¨Ø© ØªØ­Ù„ÙŠÙ„ Ù†Ø·Ø§Ù‚Ø§Øª Ø§Ù„Ù…Ø¹Ø±ÙØ§Øª
            id_ranges = self.analysis_results.get('link_patterns', {}).get('id_ranges', {})
            for content_type, ranges in id_ranges.items():
                if ranges.get('ids'):
                    f.write(f"""### {content_type.upper()}
- **Ø£ØµØºØ± Ù…Ø¹Ø±Ù:** {ranges['min']:,}
- **Ø£ÙƒØ¨Ø± Ù…Ø¹Ø±Ù:** {ranges['max']:,}
- **Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ø¹Ø±ÙØ§Øª:** {len(ranges['ids']):,}
- **Ø§Ù„Ù†Ø·Ø§Ù‚:** {ranges['max'] - ranges['min']:,}

""")
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù„ØºØ§Øª
            insights = self.analysis_results.get('discovery_insights', {})
            if insights.get('language_distribution'):
                f.write("""## ğŸŒ ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù„ØºØ§Øª

""")
                for lang, count in insights['language_distribution'].most_common():
                    f.write(f"- **{lang}:** {count:,}\n")
                f.write("\n")
            
            # Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©
            if insights.get('popular_keywords'):
                f.write("""## ğŸ”‘ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø§Ù„Ø£ÙƒØ«Ø± Ø´ÙŠÙˆØ¹Ø§Ù‹

""")
                for keyword, count in insights['popular_keywords'].most_common(20):
                    f.write(f"- **{keyword}:** {count:,}\n")
                f.write("\n")
            
            # ØªØ­Ù„ÙŠÙ„ Ø¨Ù†ÙŠØ© Ø§Ù„Ù…ÙˆÙ‚Ø¹
            site_structure = self.analysis_results.get('site_structure', {})
            if site_structure:
                f.write("""## ğŸ—ï¸ ØªØ­Ù„ÙŠÙ„ Ø¨Ù†ÙŠØ© Ø§Ù„Ù…ÙˆÙ‚Ø¹

### Ø§Ù„ØªÙ†Ù‚Ù„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
""")
                navigation = site_structure.get('navigation', {})
                main_menu = navigation.get('main_menu', [])
                for item in main_menu[:10]:
                    f.write(f"- **{item.get('text', 'Ø¨Ø¯ÙˆÙ† Ù†Øµ')}:** {item.get('url', 'Ø¨Ø¯ÙˆÙ† Ø±Ø§Ø¨Ø·')}\n")
                
                f.write(f"""

### Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªÙ‚Ù†ÙŠØ©
- **ÙˆÙ‚Øª ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø©:** {site_structure.get('page_load_time', 0):.2f} Ø«Ø§Ù†ÙŠØ©
- **Ø¹Ø¯Ø¯ Ø§Ù„Ù€ Scripts:** {len(site_structure.get('technical', {}).get('scripts', []))}
- **Ø¹Ø¯Ø¯ Ø£ÙˆØ±Ø§Ù‚ Ø§Ù„Ø£Ù†Ù…Ø§Ø·:** {len(site_structure.get('technical', {}).get('stylesheets', []))}

""")
            
            f.write("""## ğŸ¯ ØªÙˆØµÙŠØ§Øª Ø§Ù„ØªØ­Ø³ÙŠÙ†

### Ù„Ù„Ù…Ø·ÙˆØ±ÙŠÙ†
1. **ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡:** ØªÙ‚Ù„ÙŠÙ„ ÙˆÙ‚Øª ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø©
2. **ØªØ­Ø³ÙŠÙ† SEO:** Ø¥Ø¶Ø§ÙØ© meta tags Ø£ÙƒØ«Ø± ØªÙØµÙŠÙ„Ø§Ù‹
3. **ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªÙ†Ù‚Ù„:** ØªØ¨Ø³ÙŠØ· Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù‚ÙˆØ§Ø¦Ù…

### Ù„Ù…Ø­ØªÙˆÙ‰
1. **ØªÙ†Ø¸ÙŠÙ… Ø£ÙØ¶Ù„:** ØªØµÙ†ÙŠÙ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø­Ø³Ø¨ Ø§Ù„Ù„ØºØ© ÙˆØ§Ù„Ù†ÙˆØ¹
2. **Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØµÙÙŠØ©:** Ø¥Ø¶Ø§ÙØ© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø£ÙƒØ«Ø± Ù„Ù„Ø£ÙÙ„Ø§Ù… ÙˆØ§Ù„Ù…Ø³Ù„Ø³Ù„Ø§Øª
3. **Ø§Ù„Ø¨Ø­Ø«:** ØªØ­Ø³ÙŠÙ† Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© Ø§Ù„Ø¨Ø­Ø«

---

*ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù‡Ø°Ø§ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø¨ÙˆØ§Ø³Ø·Ø© Ø£Ø¯Ø§Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ØªØ·ÙˆØ±Ø© Ø§Ù„Ø´Ø§Ù…Ù„Ø©*
""")

if __name__ == "__main__":
    analyzer = AdvancedSiteAnalyzer()
    analyzer.perform_comprehensive_analysis()