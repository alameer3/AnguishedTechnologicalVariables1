#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ù…Ø­Ù„Ù„ Ø¨Ù†ÙŠØ© Ø§Ù„Ù…ÙˆÙ‚Ø¹ - ØªØ±ØªÙŠØ¨ Ø®Ø§Øµ Ù„ÙÙ‡Ù… Ù‡ÙŠÙƒÙ„ ÙˆØ¨Ù†ÙŠØ© Ù…ÙˆÙ‚Ø¹ AKWAM
"""

import re
import urllib.parse
from collections import defaultdict, OrderedDict
import json

class SiteStructureAnalyzer:
    def __init__(self, filename='site_links.txt'):
        self.filename = filename
        self.all_links = []
        self.structure_analysis = {}
        
    def load_links(self):
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·"""
        with open(self.filename, 'r', encoding='utf-8') as f:
            content = f.read()
        
        lines = content.strip().split('\n')
        for line in lines:
            line = line.strip()
            if line.startswith('https://ak.sv/'):
                self.all_links.append(line)
                
    def analyze_url_patterns(self):
        """ØªØ­Ù„ÙŠÙ„ Ø£Ù†Ù…Ø§Ø· URLs Ù„ÙÙ‡Ù… Ø¨Ù†ÙŠØ© Ø§Ù„Ù…ÙˆÙ‚Ø¹"""
        patterns = {
            'main_sections': defaultdict(list),
            'content_types': defaultdict(list),
            'parameters': defaultdict(list),
            'depth_levels': defaultdict(list),
            'id_ranges': defaultdict(list),
            'language_patterns': defaultdict(list)
        }
        
        for link in self.all_links:
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
            path = link.replace('https://ak.sv/', '')
            parts = path.split('/')
            
            # Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø¹Ù…Ù‚
            depth = len([p for p in parts if p and not p.startswith('#') and '?' not in p])
            patterns['depth_levels'][f'depth_{depth}'].append(link)
            
            # Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
            if '/movie/' in link:
                id_match = re.search(r'/movie/(\d+)/', link)
                if id_match:
                    movie_id = int(id_match.group(1))
                    patterns['content_types']['movies'].append((movie_id, link))
                    patterns['id_ranges']['movie_ids'].append(movie_id)
                    
            elif '/series/' in link:
                id_match = re.search(r'/series/(\d+)/', link)
                if id_match:
                    series_id = int(id_match.group(1))
                    patterns['content_types']['series'].append((series_id, link))
                    patterns['id_ranges']['series_ids'].append(series_id)
                    
            elif '/episode/' in link:
                id_match = re.search(r'/episode/(\d+)/', link)
                if id_match:
                    episode_id = int(id_match.group(1))
                    patterns['content_types']['episodes'].append((episode_id, link))
                    patterns['id_ranges']['episode_ids'].append(episode_id)
                    
            elif '/person/' in link:
                id_match = re.search(r'/person/(\d+)/', link)
                if id_match:
                    person_id = int(id_match.group(1))
                    patterns['content_types']['persons'].append((person_id, link))
                    patterns['id_ranges']['person_ids'].append(person_id)
                    
            elif '/shows/' in link or '/show/' in link:
                patterns['content_types']['shows'].append(link)
                
            elif '/mix/' in link:
                patterns['content_types']['mix'].append(link)
                
            # Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª ÙˆØ§Ù„ÙÙ„Ø§ØªØ±
            if '?' in link:
                query_part = link.split('?')[1]
                params = query_part.split('&')
                for param in params:
                    if '=' in param:
                        key, value = param.split('=', 1)
                        patterns['parameters'][key].append(value)
                        
            # Ø§Ù„Ø£Ù‚Ø³Ø§Ù… Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
            if parts and parts[0]:
                main_section = parts[0].split('?')[0]  # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª
                patterns['main_sections'][main_section].append(link)
                
            # Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù„ØºØ©
            if re.search(r'[\u0600-\u06FF]', link):
                patterns['language_patterns']['arabic'].append(link)
            else:
                patterns['language_patterns']['english'].append(link)
        
        return patterns
    
    def analyze_content_organization(self):
        """ØªØ­Ù„ÙŠÙ„ ØªÙ†Ø¸ÙŠÙ… Ø§Ù„Ù…Ø­ØªÙˆÙ‰"""
        organization = {
            'navigation_structure': {},
            'content_hierarchy': {},
            'naming_conventions': {},
            'content_relationships': {}
        }
        
        # Ø¨Ù†ÙŠØ© Ø§Ù„ØªÙ†Ù‚Ù„
        main_pages = [link for link in self.all_links if link.count('/') <= 4 and '?' not in link]
        organization['navigation_structure'] = {
            'main_pages': len(main_pages),
            'categories': len([link for link in self.all_links if '?category=' in link]),
            'pages': len([link for link in self.all_links if '?page=' in link]),
            'tags': len([link for link in self.all_links if '?tag=' in link])
        }
        
        # Ø§ØµØ·Ù„Ø§Ø­Ø§Øª Ø§Ù„ØªØ³Ù…ÙŠØ©
        arabic_titles = []
        english_titles = []
        
        for link in self.all_links:
            title = link.split('/')[-1]
            if title:
                title = urllib.parse.unquote(title)
                if re.search(r'[\u0600-\u06FF]', title):
                    arabic_titles.append(title)
                else:
                    english_titles.append(title)
        
        organization['naming_conventions'] = {
            'arabic_titles': len(arabic_titles),
            'english_titles': len(english_titles),
            'total_titles': len(arabic_titles) + len(english_titles)
        }
        
        return organization
    
    def create_structure_understanding_file(self):
        """Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù Ù„ÙÙ‡Ù… Ø¨Ù†ÙŠØ© Ø§Ù„Ù…ÙˆÙ‚Ø¹"""
        patterns = self.analyze_url_patterns()
        organization = self.analyze_content_organization()
        
        output_content = []
        
        # Ø§Ù„Ø¹Ù†ÙˆØ§Ù†
        output_content.append("# ğŸ—ï¸ Ø¯Ù„ÙŠÙ„ ÙÙ‡Ù… Ø¨Ù†ÙŠØ© Ù…ÙˆÙ‚Ø¹ AKWAM")
        output_content.append("=" * 50)
        output_content.append(f"ğŸ“Š Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…Ø­Ù„Ù„Ø©: {len(self.all_links):,}")
        output_content.append("ğŸ¯ Ø§Ù„Ù‡Ø¯Ù: ÙÙ‡Ù… ØªÙ†Ø¸ÙŠÙ… ÙˆÙ‡ÙŠÙƒÙ„ Ø§Ù„Ù…ÙˆÙ‚Ø¹")
        output_content.append("")
        
        # 1. Ø§Ù„Ù‡ÙŠÙƒÙ„ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ Ù„Ù„Ù…ÙˆÙ‚Ø¹
        output_content.append("## 1ï¸âƒ£ Ø§Ù„Ù‡ÙŠÙƒÙ„ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ Ù„Ù„Ù…ÙˆÙ‚Ø¹")
        output_content.append("-" * 40)
        output_content.append("### Ø§Ù„Ø£Ù‚Ø³Ø§Ù… Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©:")
        
        main_sections = dict(sorted(patterns['main_sections'].items(), key=lambda x: len(x[1]), reverse=True))
        for section, links in main_sections.items():
            if len(links) >= 5:  # Ø¹Ø±Ø¶ Ø§Ù„Ø£Ù‚Ø³Ø§Ù… Ø§Ù„Ù…Ù‡Ù…Ø© ÙÙ‚Ø·
                output_content.append(f"  ğŸ“‚ /{section}/ â†’ {len(links):,} Ø±Ø§Ø¨Ø·")
                
                # Ø¹Ø±Ø¶ Ù†Ù…Ø§Ø°Ø¬ Ù…Ù† ÙƒÙ„ Ù‚Ø³Ù…
                sample_links = links[:3]
                for sample in sample_links:
                    output_content.append(f"      â†³ {sample}")
                if len(links) > 3:
                    output_content.append(f"      â†³ ... Ùˆ {len(links)-3:,} Ø±Ø§Ø¨Ø· Ø¢Ø®Ø±")
                output_content.append("")
        
        # 2. Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ ÙˆÙ†Ø·Ø§Ù‚Ø§Øª IDs
        output_content.append("## 2ï¸âƒ£ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ ÙˆÙ†Ø·Ø§Ù‚Ø§Øª Ø§Ù„Ø£Ø±Ù‚Ø§Ù…")
        output_content.append("-" * 40)
        
        content_types = ['movies', 'series', 'episodes', 'persons']
        for content_type in content_types:
            if content_type in patterns['content_types']:
                items = patterns['content_types'][content_type]
                if items:
                    ids = [item[0] if isinstance(item, tuple) else 0 for item in items]
                    ids = [id_val for id_val in ids if id_val > 0]
                    
                    if ids:
                        min_id, max_id = min(ids), max(ids)
                        output_content.append(f"### ğŸ“‹ {content_type.upper()}")
                        output_content.append(f"  Ø§Ù„Ø¹Ø¯Ø¯: {len(items):,}")
                        output_content.append(f"  Ù†Ø·Ø§Ù‚ IDs: {min_id:,} - {max_id:,}")
                        output_content.append(f"  Ø§Ù„Ù…Ø¯Ù‰: {max_id - min_id:,}")
                        output_content.append("")
                        
                        # Ø£Ù…Ø«Ù„Ø© Ù…Ù† Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© ÙˆØ§Ù„ÙˆØ³Ø· ÙˆØ§Ù„Ù†Ù‡Ø§ÙŠØ©
                        sorted_items = sorted(items, key=lambda x: x[0] if isinstance(x, tuple) else 0)
                        
                        output_content.append("  ğŸ” Ø£Ù…Ø«Ù„Ø©:")
                        output_content.append("  Ø§Ù„Ø£Ù‚Ø¯Ù…:")
                        for i, item in enumerate(sorted_items[:2]):
                            if isinstance(item, tuple):
                                id_val, link = item
                                title = urllib.parse.unquote(link.split('/')[-1])
                                output_content.append(f"    [{id_val:>5}] {title}")
                        
                        output_content.append("  Ø§Ù„Ø£Ø­Ø¯Ø«:")
                        for i, item in enumerate(sorted_items[-2:]):
                            if isinstance(item, tuple):
                                id_val, link = item
                                title = urllib.parse.unquote(link.split('/')[-1])
                                output_content.append(f"    [{id_val:>5}] {title}")
                        output_content.append("")
        
        # 3. Ù†Ø¸Ø§Ù… Ø§Ù„ØªØµÙØ­ ÙˆØ§Ù„ÙÙ„ØªØ±Ø©
        output_content.append("## 3ï¸âƒ£ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØµÙØ­ ÙˆØ§Ù„ÙÙ„ØªØ±Ø©")
        output_content.append("-" * 40)
        
        # Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª
        if patterns['parameters']:
            output_content.append("### Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„ØªØµÙØ­:")
            param_stats = {}
            for param, values in patterns['parameters'].items():
                unique_values = len(set(values))
                param_stats[param] = {
                    'total': len(values),
                    'unique': unique_values,
                    'samples': list(set(values))[:5]
                }
            
            # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„Ø£Ù‡Ù…ÙŠØ©
            sorted_params = sorted(param_stats.items(), key=lambda x: x[1]['total'], reverse=True)
            
            for param, stats in sorted_params:
                output_content.append(f"  ğŸ”§ {param}:")
                output_content.append(f"    Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…: {stats['total']:,} Ù…Ø±Ø©")
                output_content.append(f"    Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…Ø®ØªÙ„ÙØ©: {stats['unique']:,}")
                output_content.append(f"    Ø£Ù…Ø«Ù„Ø©: {', '.join(map(str, stats['samples']))}")
                output_content.append("")
        
        # 4. Ø§Ù„ØªÙˆØ²ÙŠØ¹ Ø­Ø³Ø¨ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø¹Ù…Ù‚
        output_content.append("## 4ï¸âƒ£ Ø§Ù„ØªÙˆØ²ÙŠØ¹ Ø­Ø³Ø¨ Ø¹Ù…Ù‚ Ø§Ù„Ù…Ø³Ø§Ø±")
        output_content.append("-" * 40)
        
        for depth, links in sorted(patterns['depth_levels'].items()):
            depth_num = depth.split('_')[1]
            percentage = len(links) / len(self.all_links) * 100
            output_content.append(f"ğŸ“Š Ù…Ø³ØªÙˆÙ‰ {depth_num}: {len(links):,} Ø±Ø§Ø¨Ø· ({percentage:.1f}%)")
            
            # Ø£Ù…Ø«Ù„Ø© Ù„ÙƒÙ„ Ù…Ø³ØªÙˆÙ‰
            samples = links[:3]
            for sample in samples:
                output_content.append(f"  â†³ {sample}")
            output_content.append("")
        
        # 5. ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù„ØºØ© ÙˆØ§Ù„Ù…Ø­ØªÙˆÙ‰
        output_content.append("## 5ï¸âƒ£ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù„ØºØ© ÙˆØ§Ù„Ù…Ø­ØªÙˆÙ‰")
        output_content.append("-" * 40)
        
        arabic_links = patterns['language_patterns']['arabic']
        english_links = patterns['language_patterns']['english']
        
        output_content.append(f"ğŸ‡¸ğŸ‡¦ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¹Ø±Ø¨ÙŠ: {len(arabic_links):,} ({len(arabic_links)/len(self.all_links)*100:.1f}%)")
        output_content.append(f"ğŸŒ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ: {len(english_links):,} ({len(english_links)/len(self.all_links)*100:.1f}%)")
        output_content.append("")
        
        # Ø£Ù…Ø«Ù„Ø© Ù…Ù† Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¹Ø±Ø¨ÙŠ
        if arabic_links:
            output_content.append("### Ù†Ù…Ø§Ø°Ø¬ Ù…Ù† Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¹Ø±Ø¨ÙŠ:")
            arabic_samples = arabic_links[:10]
            for sample in arabic_samples:
                title = urllib.parse.unquote(sample.split('/')[-1])
                content_type = "ÙÙŠÙ„Ù…" if "/movie/" in sample else "Ù…Ø³Ù„Ø³Ù„" if "/series/" in sample else "Ø­Ù„Ù‚Ø©" if "/episode/" in sample else "Ù…Ø­ØªÙˆÙ‰"
                output_content.append(f"  â€¢ {content_type}: {title}")
            output_content.append("")
        
        # 6. Ù…Ù„Ø®Øµ Ø§Ù„Ø¨Ù†ÙŠØ© ÙˆØ§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬Ø§Øª
        output_content.append("## 6ï¸âƒ£ Ù…Ù„Ø®Øµ Ø¨Ù†ÙŠØ© Ø§Ù„Ù…ÙˆÙ‚Ø¹ ÙˆØ§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬Ø§Øª")
        output_content.append("-" * 40)
        
        output_content.append("### Ø®ØµØ§Ø¦Øµ Ø¨Ù†ÙŠØ© Ø§Ù„Ù…ÙˆÙ‚Ø¹:")
        output_content.append("âœ… Ù†Ø¸Ø§Ù… ØªØ±Ù‚ÙŠÙ… Ù…Ù†Ø¸Ù… ÙˆÙ…ØªØ³Ù„Ø³Ù„ Ù„Ù„Ù…Ø­ØªÙˆÙ‰")
        output_content.append("âœ… ØªØµÙ†ÙŠÙ ÙˆØ§Ø¶Ø­ Ø­Ø³Ø¨ Ù†ÙˆØ¹ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ (Ø£ÙÙ„Ø§Ù…ØŒ Ù…Ø³Ù„Ø³Ù„Ø§ØªØŒ Ø­Ù„Ù‚Ø§Øª)")
        output_content.append("âœ… Ù†Ø¸Ø§Ù… ÙÙ„ØªØ±Ø© Ù…ØªØ·ÙˆØ± (ÙØ¦Ø§ØªØŒ ØµÙØ­Ø§ØªØŒ Ø¹Ù„Ø§Ù…Ø§Øª)")
        output_content.append("âœ… Ø¯Ø¹Ù… Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ù„ØºØ§Øª (Ø¹Ø±Ø¨ÙŠ/Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ)")
        output_content.append("âœ… Ù‡ÙŠÙƒÙ„ URL Ù…Ù†Ø·Ù‚ÙŠ ÙˆÙ‚Ø§Ø¨Ù„ Ù„Ù„ÙÙ‡Ù…")
        output_content.append("")
        
        output_content.append("### Ù†Ù…Ø· ØªÙ†Ø¸ÙŠÙ… Ø§Ù„Ù…Ø­ØªÙˆÙ‰:")
        output_content.append("ğŸ“‹ Ø§Ù„Ø£ÙÙ„Ø§Ù…: Ù†Ø¸Ø§Ù… ID Ù…ØªØ³Ù„Ø³Ù„ Ù…Ø¹ Ø¹Ù†Ø§ÙˆÙŠÙ† ÙˆØ§Ø¶Ø­Ø©")
        output_content.append("ğŸ“‹ Ø§Ù„Ù…Ø³Ù„Ø³Ù„Ø§Øª: ØªØ±Ù‚ÙŠÙ… Ù…Ù†ÙØµÙ„ Ù…Ø¹ Ø±Ø¨Ø· Ø¨Ø§Ù„Ø­Ù„Ù‚Ø§Øª")
        output_content.append("ğŸ“‹ Ø§Ù„Ø­Ù„Ù‚Ø§Øª: ØªØ±Ù‚ÙŠÙ… ÙØ±ÙŠØ¯ Ù…Ø¹ Ø±Ø¨Ø· Ø¨Ø§Ù„Ù…Ø³Ù„Ø³Ù„ Ø§Ù„Ø£ØµÙ„ÙŠ")
        output_content.append("ğŸ“‹ Ø§Ù„Ø£Ø´Ø®Ø§Øµ: Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø´Ø§Ù…Ù„Ø© Ù„Ù„Ù…Ù…Ø«Ù„ÙŠÙ† ÙˆØ§Ù„Ù…Ø´Ø§Ù‡ÙŠØ±")
        output_content.append("")
        
        output_content.append("### Ù‚Ø§Ø¨Ù„ÙŠØ© Ø§Ù„Ø§Ø³ØªÙØ§Ø¯Ø© Ù„Ù€ YEMEN_FLIX:")
        output_content.append("ğŸ¯ ÙŠÙ…ÙƒÙ† ØªØ·Ø¨ÙŠÙ‚ Ù†ÙØ³ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ±Ù‚ÙŠÙ… Ø§Ù„Ù…Ù†Ø¸Ù…")
        output_content.append("ğŸ¯ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¨Ù†ÙŠØ© URL Ù…Ø´Ø§Ø¨Ù‡Ø© Ù„Ù„Ø³Ù‡ÙˆÙ„Ø©")
        output_content.append("ğŸ¯ ØªØ·Ø¨ÙŠÙ‚ Ù†Ø¸Ø§Ù… ÙÙ„ØªØ±Ø© Ù…ØªØ·ÙˆØ±")
        output_content.append("ğŸ¯ Ø¯Ø¹Ù… Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¹Ø±Ø¨ÙŠ ÙˆØ§Ù„Ø¯ÙˆÙ„ÙŠ")
        output_content.append("ğŸ¯ Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø´Ø§Ù…Ù„Ø© Ù„Ù„Ù…Ù…Ø«Ù„ÙŠÙ†")
        
        output_content.append("")
        output_content.append("=" * 50)
        output_content.append("ğŸ“– ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù‡Ø°Ø§ Ø§Ù„Ø¯Ù„ÙŠÙ„ Ù„ÙÙ‡Ù… Ø¨Ù†ÙŠØ© Ù…ÙˆÙ‚Ø¹ AKWAM Ø¨Ø¹Ù…Ù‚")
        output_content.append("ğŸ¯ Ø§Ù„Ù‡Ø¯Ù: Ø§Ù„Ø§Ø³ØªÙØ§Ø¯Ø© Ù…Ù† Ø§Ù„ØªÙ†Ø¸ÙŠÙ… Ø§Ù„Ù…Ù…ØªØ§Ø² ÙÙŠ ØªØ·ÙˆÙŠØ± YEMEN_FLIX")
        
        # Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù
        with open('Ø¯Ù„ÙŠÙ„_ÙÙ‡Ù…_Ø¨Ù†ÙŠØ©_Ù…ÙˆÙ‚Ø¹_AKWAM.txt', 'w', encoding='utf-8') as f:
            f.write('\n'.join(output_content))
        
        print(f"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø¯Ù„ÙŠÙ„ ÙÙ‡Ù… Ø§Ù„Ø¨Ù†ÙŠØ©: Ø¯Ù„ÙŠÙ„_ÙÙ‡Ù…_Ø¨Ù†ÙŠØ©_Ù…ÙˆÙ‚Ø¹_AKWAM.txt")
        print(f"ğŸ—ï¸ ØªÙ… ØªØ­Ù„ÙŠÙ„ {len(self.all_links):,} Ø±Ø§Ø¨Ø· Ù„ÙÙ‡Ù… Ø¨Ù†ÙŠØ© Ø§Ù„Ù…ÙˆÙ‚Ø¹")
        
        return output_content
    
    def run_structure_analysis(self):
        """ØªØ´ØºÙŠÙ„ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨Ù†ÙŠØ©"""
        print("ğŸ—ï¸ Ø¨Ø¯Ø¡ ØªØ­Ù„ÙŠÙ„ Ø¨Ù†ÙŠØ© Ø§Ù„Ù…ÙˆÙ‚Ø¹...")
        
        self.load_links()
        result = self.create_structure_understanding_file()
        
        print("ğŸ¯ ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø¯Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ù„ÙÙ‡Ù… Ø¨Ù†ÙŠØ© ÙˆØªÙ†Ø¸ÙŠÙ… Ø§Ù„Ù…ÙˆÙ‚Ø¹")
        
        return result

if __name__ == "__main__":
    analyzer = SiteStructureAnalyzer()
    analyzer.run_structure_analysis()