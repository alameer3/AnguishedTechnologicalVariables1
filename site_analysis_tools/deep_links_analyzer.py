#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ù…Ø­Ù„Ù„ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…ØªØ·ÙˆØ± - ØªØ­Ù„ÙŠÙ„ Ø¹Ù…ÙŠÙ‚ Ù„Ù…Ù„Ù site_links.txt
ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ù„Ø¨Ù†ÙŠØ© Ù…ÙˆÙ‚Ø¹ AKWAM ÙˆÙ…Ø­ØªÙˆØ§Ù‡
"""

import re
import json
import urllib.parse
from collections import defaultdict, Counter
from datetime import datetime
import statistics

class DeepLinksAnalyzer:
    def __init__(self, filename='site_links.txt'):
        self.filename = filename
        self.all_links = []
        self.categories = defaultdict(int)
        self.patterns = defaultdict(int)
        self.detailed_analysis = {}
        
    def load_links(self):
        """ØªØ­Ù…ÙŠÙ„ ÙˆØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù…Ù† Ø§Ù„Ù…Ù„Ù"""
        with open(self.filename, 'r', encoding='utf-8') as f:
            content = f.read()
        
        lines = content.strip().split('\n')
        for line in lines:
            line = line.strip()
            if line.startswith('https://ak.sv/'):
                self.all_links.append(line)
                
        print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(self.all_links):,} Ø±Ø§Ø¨Ø· Ø¨Ù†Ø¬Ø§Ø­")
        
    def analyze_structure(self):
        """ØªØ­Ù„ÙŠÙ„ Ù‡ÙŠÙƒÙ„ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· ÙˆØªØµÙ†ÙŠÙÙ‡Ø§"""
        for link in self.all_links:
            # ØªØ­Ù„ÙŠÙ„ Ù†ÙˆØ¹ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
            if '/movie/' in link:
                self.categories['Ø£ÙÙ„Ø§Ù…'] += 1
            elif '/series/' in link:
                self.categories['Ù…Ø³Ù„Ø³Ù„Ø§Øª'] += 1
            elif '/episode/' in link:
                self.categories['Ø­Ù„Ù‚Ø§Øª'] += 1
            elif '/shows/' in link:
                self.categories['Ø¹Ø±ÙˆØ¶ ØªÙ„ÙØ²ÙŠÙˆÙ†ÙŠØ©'] += 1
            elif '/show/' in link:
                self.categories['Ø¹Ø±ÙˆØ¶'] += 1
            elif '/mix/' in link:
                self.categories['Ù…Ù†ÙˆØ¹Ø§Øª'] += 1
            elif '/person/' in link:
                self.categories['Ø£Ø´Ø®Ø§Øµ/Ù…Ù…Ø«Ù„ÙŠÙ†'] += 1
            elif link.count('/') == 3:
                self.categories['ØµÙØ­Ø§Øª Ø±Ø¦ÙŠØ³ÙŠØ©'] += 1
            else:
                self.categories['Ø£Ø®Ø±Ù‰'] += 1
                
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ù†Ù…Ø§Ø·
            path = link.replace('https://ak.sv/', '')
            if '/' in path:
                pattern = path.split('/')[0]
                self.patterns[pattern] += 1
                
    def analyze_movies(self):
        """ØªØ­Ù„ÙŠÙ„ ØªÙØµÙŠÙ„ÙŠ Ù„Ù„Ø£ÙÙ„Ø§Ù…"""
        movies = [link for link in self.all_links if '/movie/' in link]
        movie_analysis = {
            'total_count': len(movies),
            'ids_range': {'min': float('inf'), 'max': 0},
            'languages': defaultdict(int),
            'sample_titles': []
        }
        
        movie_ids = []
        for movie in movies:
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ID
            match = re.search(r'/movie/(\d+)/', movie)
            if match:
                movie_id = int(match.group(1))
                movie_ids.append(movie_id)
                
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù„ØºØ© Ù…Ù† Ø§Ù„Ù†Øµ
            title = movie.split('/')[-1]
            title = urllib.parse.unquote(title)
            if re.search(r'[\u0600-\u06FF]', title):  # Ù†Øµ Ø¹Ø±Ø¨ÙŠ
                movie_analysis['languages']['Ø¹Ø±Ø¨ÙŠ'] += 1
            else:
                movie_analysis['languages']['Ø£Ø¬Ù†Ø¨ÙŠ'] += 1
                
            # Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ†
            if len(movie_analysis['sample_titles']) < 20:
                movie_analysis['sample_titles'].append({
                    'id': movie_id if match else 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯',
                    'title': title,
                    'url': movie
                })
        
        if movie_ids:
            movie_analysis['ids_range']['min'] = min(movie_ids)
            movie_analysis['ids_range']['max'] = max(movie_ids)
            movie_analysis['average_id'] = statistics.mean(movie_ids)
            movie_analysis['median_id'] = statistics.median(movie_ids)
            
        self.detailed_analysis['movies'] = movie_analysis
        
    def analyze_series(self):
        """ØªØ­Ù„ÙŠÙ„ ØªÙØµÙŠÙ„ÙŠ Ù„Ù„Ù…Ø³Ù„Ø³Ù„Ø§Øª"""
        series = [link for link in self.all_links if '/series/' in link]
        series_analysis = {
            'total_count': len(series),
            'ids_range': {'min': float('inf'), 'max': 0},
            'languages': defaultdict(int),
            'seasons_pattern': defaultdict(int),
            'sample_titles': []
        }
        
        series_ids = []
        for serie in series:
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ID
            match = re.search(r'/series/(\d+)/', serie)
            if match:
                series_id = int(match.group(1))
                series_ids.append(series_id)
                
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ÙˆØ³Ù…
            if 'Ø§Ù„Ù…ÙˆØ³Ù…' in serie:
                season_match = re.search(r'Ø§Ù„Ù…ÙˆØ³Ù…-(\d+)', serie)
                if season_match:
                    season = int(season_match.group(1))
                    series_analysis['seasons_pattern'][f'Ø§Ù„Ù…ÙˆØ³Ù… {season}'] += 1
                    
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù„ØºØ©
            title = serie.split('/')[-1]
            title = urllib.parse.unquote(title)
            if re.search(r'[\u0600-\u06FF]', title):
                series_analysis['languages']['Ø¹Ø±Ø¨ÙŠ'] += 1
            else:
                series_analysis['languages']['Ø£Ø¬Ù†Ø¨ÙŠ'] += 1
                
            # Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ†
            if len(series_analysis['sample_titles']) < 20:
                series_analysis['sample_titles'].append({
                    'id': series_id if match else 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯',
                    'title': title,
                    'url': serie
                })
        
        if series_ids:
            series_analysis['ids_range']['min'] = min(series_ids)
            series_analysis['ids_range']['max'] = max(series_ids)
            series_analysis['average_id'] = statistics.mean(series_ids)
            
        self.detailed_analysis['series'] = series_analysis
        
    def analyze_episodes(self):
        """ØªØ­Ù„ÙŠÙ„ ØªÙØµÙŠÙ„ÙŠ Ù„Ù„Ø­Ù„Ù‚Ø§Øª"""
        episodes = [link for link in self.all_links if '/episode/' in link]
        episodes_analysis = {
            'total_count': len(episodes),
            'languages': defaultdict(int),
            'episode_numbers': defaultdict(int),
            'series_distribution': defaultdict(int),
            'sample_episodes': []
        }
        
        for episode in episodes:
            # ØªØ­Ù„ÙŠÙ„ Ø±Ù‚Ù… Ø§Ù„Ø­Ù„Ù‚Ø©
            if 'Ø§Ù„Ø­Ù„Ù‚Ø©' in episode:
                episodes_analysis['languages']['Ø¹Ø±Ø¨ÙŠ'] += 1
                episode_match = re.search(r'Ø§Ù„Ø­Ù„Ù‚Ø©-(\d+)', episode)
                if episode_match:
                    ep_num = int(episode_match.group(1))
                    episodes_analysis['episode_numbers'][ep_num] += 1
            else:
                episodes_analysis['languages']['Ø£Ø¬Ù†Ø¨ÙŠ'] += 1
                
            # ØªØ­Ù„ÙŠÙ„ ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù…Ø³Ù„Ø³Ù„Ø§Øª
            parts = episode.split('/')
            if len(parts) >= 5:
                series_name = parts[4].split('-')[0]  # Ø£ÙˆÙ„ Ø¬Ø²Ø¡ Ù…Ù† Ø§Ø³Ù… Ø§Ù„Ù…Ø³Ù„Ø³Ù„
                episodes_analysis['series_distribution'][series_name] += 1
                
            # Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„Ø­Ù„Ù‚Ø§Øª
            if len(episodes_analysis['sample_episodes']) < 15:
                episodes_analysis['sample_episodes'].append({
                    'series': parts[4] if len(parts) >= 5 else 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯',
                    'episode': parts[-1] if len(parts) > 5 else 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯',
                    'url': episode
                })
        
        self.detailed_analysis['episodes'] = episodes_analysis
        
    def analyze_persons(self):
        """ØªØ­Ù„ÙŠÙ„ ØµÙØ­Ø§Øª Ø§Ù„Ø£Ø´Ø®Ø§Øµ/Ø§Ù„Ù…Ù…Ø«Ù„ÙŠÙ†"""
        persons = [link for link in self.all_links if '/person/' in link]
        persons_analysis = {
            'total_count': len(persons),
            'ids_range': {'min': float('inf'), 'max': 0},
            'name_patterns': defaultdict(int),
            'sample_persons': []
        }
        
        person_ids = []
        for person in persons:
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ID
            match = re.search(r'/person/(\d+)/', person)
            if match:
                person_id = int(match.group(1))
                person_ids.append(person_id)
                
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø³Ù…Ø§Ø¡
            name = person.split('/')[-1]
            name = urllib.parse.unquote(name)
            if re.search(r'[\u0600-\u06FF]', name):
                persons_analysis['name_patterns']['Ø¹Ø±Ø¨ÙŠ'] += 1
            else:
                persons_analysis['name_patterns']['Ø£Ø¬Ù†Ø¨ÙŠ'] += 1
                
            # Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„Ø£Ø´Ø®Ø§Øµ
            if len(persons_analysis['sample_persons']) < 15:
                persons_analysis['sample_persons'].append({
                    'id': person_id if match else 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯',
                    'name': name,
                    'url': person
                })
        
        if person_ids:
            persons_analysis['ids_range']['min'] = min(person_ids)
            persons_analysis['ids_range']['max'] = max(person_ids)
            
        self.detailed_analysis['persons'] = persons_analysis
        
    def generate_comprehensive_report(self):
        """Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ø´Ø§Ù…Ù„ ÙˆÙ…ÙØµÙ„"""
        report = []
        report.append("ğŸ¬ ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„ Ø§Ù„Ø¹Ù…ÙŠÙ‚ Ù„Ù…ÙˆÙ‚Ø¹ AKWAM")
        report.append("=" * 60)
        report.append(f"ğŸ“… ØªØ§Ø±ÙŠØ® Ø§Ù„ØªØ­Ù„ÙŠÙ„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"ğŸ“Š Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…Ø­Ù„Ù„Ø©: {len(self.all_links):,}")
        report.append("")
        
        # Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¹Ø§Ù…Ø©
        report.append("ğŸ“ˆ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¹Ø§Ù…Ø©:")
        report.append("-" * 30)
        for category, count in sorted(self.categories.items(), key=lambda x: x[1], reverse=True):
            percentage = (count / len(self.all_links)) * 100
            report.append(f"â€¢ {category}: {count:,} ({percentage:.1f}%)")
        report.append("")
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£ÙÙ„Ø§Ù…
        if 'movies' in self.detailed_analysis:
            movies = self.detailed_analysis['movies']
            report.append("ğŸ¥ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£ÙÙ„Ø§Ù… Ø§Ù„Ù…ØªØ¹Ù…Ù‚:")
            report.append("-" * 30)
            report.append(f"â€¢ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø£ÙÙ„Ø§Ù…: {movies['total_count']:,}")
            report.append(f"â€¢ Ù†Ø·Ø§Ù‚ IDs: {movies['ids_range']['min']:,} - {movies['ids_range']['max']:,}")
            if 'average_id' in movies:
                report.append(f"â€¢ Ù…ØªÙˆØ³Ø· ID: {movies['average_id']:.0f}")
            report.append(f"â€¢ ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù„ØºØ§Øª:")
            for lang, count in movies['languages'].items():
                percentage = (count / movies['total_count']) * 100
                report.append(f"  - {lang}: {count:,} ({percentage:.1f}%)")
            
            report.append(f"â€¢ Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„Ø£ÙÙ„Ø§Ù…:")
            for movie in movies['sample_titles'][:10]:
                report.append(f"  - ID {movie['id']}: {movie['title']}")
            report.append("")
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø³Ù„Ø³Ù„Ø§Øª
        if 'series' in self.detailed_analysis:
            series = self.detailed_analysis['series']
            report.append("ğŸ“º ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø³Ù„Ø³Ù„Ø§Øª Ø§Ù„Ù…ØªØ¹Ù…Ù‚:")
            report.append("-" * 30)
            report.append(f"â€¢ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ø³Ù„Ø³Ù„Ø§Øª: {series['total_count']:,}")
            report.append(f"â€¢ Ù†Ø·Ø§Ù‚ IDs: {series['ids_range']['min']:,} - {series['ids_range']['max']:,}")
            report.append(f"â€¢ ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù„ØºØ§Øª:")
            for lang, count in series['languages'].items():
                percentage = (count / series['total_count']) * 100
                report.append(f"  - {lang}: {count:,} ({percentage:.1f}%)")
            
            if series['seasons_pattern']:
                report.append(f"â€¢ Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…ÙˆØ§Ø³Ù… Ø§Ù„Ø£ÙƒØ«Ø± Ø´ÙŠÙˆØ¹Ø§Ù‹:")
                for season, count in sorted(series['seasons_pattern'].items(), key=lambda x: x[1], reverse=True)[:5]:
                    report.append(f"  - {season}: {count:,}")
            
            report.append(f"â€¢ Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„Ù…Ø³Ù„Ø³Ù„Ø§Øª:")
            for serie in series['sample_titles'][:10]:
                report.append(f"  - ID {serie['id']}: {serie['title']}")
            report.append("")
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø­Ù„Ù‚Ø§Øª
        if 'episodes' in self.detailed_analysis:
            episodes = self.detailed_analysis['episodes']
            report.append("ğŸ¬ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø­Ù„Ù‚Ø§Øª Ø§Ù„Ù…ØªØ¹Ù…Ù‚:")
            report.append("-" * 30)
            report.append(f"â€¢ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø­Ù„Ù‚Ø§Øª: {episodes['total_count']:,}")
            report.append(f"â€¢ ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù„ØºØ§Øª:")
            for lang, count in episodes['languages'].items():
                percentage = (count / episodes['total_count']) * 100
                report.append(f"  - {lang}: {count:,} ({percentage:.1f}%)")
            
            # Ø£ÙƒØ«Ø± Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ø­Ù„Ù‚Ø§Øª Ø´ÙŠÙˆØ¹Ø§Ù‹
            if episodes['episode_numbers']:
                top_episodes = sorted(episodes['episode_numbers'].items(), key=lambda x: x[1], reverse=True)[:10]
                report.append(f"â€¢ Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ø­Ù„Ù‚Ø§Øª Ø§Ù„Ø£ÙƒØ«Ø± Ø´ÙŠÙˆØ¹Ø§Ù‹:")
                for ep_num, count in top_episodes:
                    report.append(f"  - Ø§Ù„Ø­Ù„Ù‚Ø© {ep_num}: {count:,} Ù…Ø±Ø©")
            
            # ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù…Ø³Ù„Ø³Ù„Ø§Øª
            top_series = sorted(episodes['series_distribution'].items(), key=lambda x: x[1], reverse=True)[:10]
            report.append(f"â€¢ Ø§Ù„Ù…Ø³Ù„Ø³Ù„Ø§Øª Ø§Ù„Ø£ÙƒØ«Ø± Ø­Ù„Ù‚Ø§Øª:")
            for series_name, count in top_series:
                report.append(f"  - {series_name}: {count:,} Ø­Ù„Ù‚Ø©")
            report.append("")
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø´Ø®Ø§Øµ
        if 'persons' in self.detailed_analysis:
            persons = self.detailed_analysis['persons']
            report.append("ğŸ‘¥ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø´Ø®Ø§Øµ/Ø§Ù„Ù…Ù…Ø«Ù„ÙŠÙ†:")
            report.append("-" * 30)
            report.append(f"â€¢ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø£Ø´Ø®Ø§Øµ: {persons['total_count']:,}")
            report.append(f"â€¢ Ù†Ø·Ø§Ù‚ IDs: {persons['ids_range']['min']:,} - {persons['ids_range']['max']:,}")
            report.append(f"â€¢ ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø£Ø³Ù…Ø§Ø¡:")
            for pattern, count in persons['name_patterns'].items():
                percentage = (count / persons['total_count']) * 100
                report.append(f"  - {pattern}: {count:,} ({percentage:.1f}%)")
            
            report.append(f"â€¢ Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„Ø£Ø´Ø®Ø§Øµ:")
            for person in persons['sample_persons'][:10]:
                report.append(f"  - ID {person['id']}: {person['name']}")
            report.append("")
        
        # Ø§Ù„Ø®Ù„Ø§ØµØ© ÙˆØ§Ù„ØªÙˆØµÙŠØ§Øª
        report.append("ğŸ” Ø§Ù„Ø®Ù„Ø§ØµØ© ÙˆØ§Ù„ØªÙˆØµÙŠØ§Øª:")
        report.append("-" * 30)
        report.append("â€¢ Ø§Ù„Ù…ÙˆÙ‚Ø¹ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù…ÙƒØªØ¨Ø© Ø¶Ø®Ù…Ø© ÙˆÙ…ØªÙ†ÙˆØ¹Ø© Ù…Ù† Ø§Ù„Ù…Ø­ØªÙˆÙ‰")
        report.append("â€¢ Ù†Ø¸Ø§Ù… ØªØ±Ù‚ÙŠÙ… Ù…Ù†Ø¸Ù… Ù„Ù„Ù…Ø­ØªÙˆÙ‰ Ù…Ø¹ IDs ÙØ±ÙŠØ¯Ø©")
        report.append("â€¢ Ø¯Ø¹Ù… Ù‚ÙˆÙŠ Ù„Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¹Ø±Ø¨ÙŠ ÙˆØ§Ù„Ø£Ø¬Ù†Ø¨ÙŠ")
        report.append("â€¢ Ø¨Ù†ÙŠØ© URL ÙˆØ§Ø¶Ø­Ø© ÙˆÙ…Ù†Ø·Ù‚ÙŠØ©")
        report.append("â€¢ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‡Ø°Ø§ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ù„Ø¨Ù†Ø§Ø¡ Ù†Ø¸Ø§Ù… ÙÙ‡Ø±Ø³Ø© Ù…ØªØ·ÙˆØ±")
        report.append("")
        
        return "\n".join(report)
    
    def run_complete_analysis(self):
        """ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙƒØ§Ù…Ù„"""
        print("ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„ Ø§Ù„Ø¹Ù…ÙŠÙ‚...")
        
        self.load_links()
        self.analyze_structure()
        self.analyze_movies()
        self.analyze_series() 
        self.analyze_episodes()
        self.analyze_persons()
        
        report = self.generate_comprehensive_report()
        
        # Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
        output_file = f"ØªØ­Ù„ÙŠÙ„_Ø§Ù„Ø±ÙˆØ§Ø¨Ø·_Ø§Ù„Ø´Ø§Ù…Ù„_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(report)
            
        print(f"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø´Ø§Ù…Ù„: {output_file}")
        return report

if __name__ == "__main__":
    analyzer = DeepLinksAnalyzer()
    report = analyzer.run_complete_analysis()
    print("\n" + "="*60)
    print(report)